{
    "sourceFile": "UNRDF-PLANNING.md",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1758655073706,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1758655073706,
            "name": "Commit-0",
            "content": "Got it ‚Äî you don‚Äôt want unrdf to be a neutral toolkit; you want it to be a canon.\nA single, opinionated path through the RDF universe.\n\nThat means:\n\nOne store: N3.Store is the only memory model. No rdf-ext.Dataset, no mixing.\n\nOne query engine: @comunica/query-sparql only. No alternative backends.\n\nOne validator: rdf-validate-shacl, fixed.\n\nOne reasoner: eyereasoner, fixed.\n\nOne canonization: rdf-canonize, fixed.\n\nOne JSON-LD pipeline: jsonld, but normalized into N3 in/out.\n\nOne traversal surface: Clownface from @zazuko/env, but always bound to N3.Store.\n\nOne opinion about config: all TTL, all persistent via useTurtle style, no YAML/JSON configs.\n\nSo instead of \"flexible glue\", unrdf becomes:\n\nüëâ the knowledge substrate. If you import unrdf, you‚Äôre accepting its way. No escape hatches.\n\nExample: Opinionated unrdf\nimport unrdf from \"unrdf\";\n\n// Parse only Turtle into the canonical store\nconst store = unrdf.parseTurtle(`\n  @prefix ex: <http://example.org/> .\n  ex:foo a ex:Bar .\n`);\n\n// Query only with Comunica\nconst rows = await unrdf.select(store, `\n  PREFIX ex: <http://example.org/>\n  SELECT ?s WHERE { ?s a ex:Bar }\n`);\n\n// Validate only with SHACL\nawait unrdf.validateOrThrow(store, `\n  @prefix sh: <http://www.w3.org/ns/shacl#> .\n  @prefix ex: <http://example.org/> .\n  ex:BarShape a sh:NodeShape ;\n    sh:targetClass ex:Bar ;\n    sh:property [ sh:path ex:name ; sh:minCount 1 ] .\n`);\n\n// Canonicalization only with URDNA2015\nconst canon = await unrdf.canonize(store);\n\n// Reason only with Eye\nconst closure = await unrdf.reason(store, `\n  @prefix ex: <http://example.org/> .\n  { ?s a ex:Bar } => { ?s a ex:Thing } .\n`);\n\nPhilosophy\n\nIf you want to try alternatives (rdflib.js, solid-client, Stardog), use another library.\n\nIf you want the ‚Äúone true path‚Äù in your ecosystem ‚Üí unrdf.\n\nIt‚Äôs opinionated by design, like rails in Ruby or django in Python. You buy into the canon.\n\n‚ö° Question for you: do you want unrdf to also lock in syntax and formats (e.g. only Turtle accepted on input, only Turtle/N-Quads for output), or should it accept JSON-LD and others but normalize immediately into Turtle/N3?\n\nYou said:\nRight and so then now let‚Äôs think about what the 8020 dark matter we‚Äôre solving for would be\nChatGPT said:\n\nPerfect ‚Äî now you‚Äôre getting to the raison d‚Äô√™tre of unrdf.\n\nThe 80/20 ‚Äúdark matter‚Äù is all the repeated, messy glue code that people write every time they touch RDF:\n\nThe 80/20 Dark Matter in RDF Workflows\n\nStore Management Hell\n\nEveryone picks a different RDF store abstraction (N3.Store, rdf-ext.Dataset, rdflib.Store, tripleDB backends).\n\nThen they spend time writing ‚Äúadapter‚Äù code to normalize.\n\nDark matter: redundant ‚Äúpick a store + wire it up‚Äù code.\n\n‚Üí unrdf: Only N3.Store. Always.\n\nParsing & Serialization Multiplicity\n\nTurtle, N-Triples, N-Quads, JSON-LD, RDF/XML (lol), TriG.\n\nEveryone writes input-output conversion boilerplate, messing up prefixes and normalization.\n\nDark matter: file IO + format juggling + prefix handling.\n\n‚Üí unrdf: Always normalize to Turtle + N-Quads. Other formats accepted, but canonicalized immediately.\n\nSPARQL Query Glut\n\nDifferent engines (Comunica, sparql-engine, rdflib query), each with quirks.\n\nPeople wrap SELECT/ASK boilerplate again and again.\n\nDark matter: copy-paste queryBindings ‚Üí map terms ‚Üí JSONify.\n\n‚Üí unrdf: One query API: .select, .ask, .update. Always JSON results.\n\nValidation\n\nMost projects skip SHACL because setup is painful.\n\nDark matter: writing half-baked ‚ÄúisA‚Äù checks, regex validators, shape-like hacks.\n\n‚Üí unrdf: Always use rdf-validate-shacl. One function call. Done.\n\nReasoning\n\nSome try EYE, some hack rules, some roll custom inferences.\n\nDark matter: brittle logic glue, inconsistent rule systems.\n\n‚Üí unrdf: Always Eye reasoner. TTL rules only.\n\nCanonicalization / Isomorphism\n\nGraph equality checks are ugly: blank node chaos, randomization.\n\nDark matter: brittle hash hacks, unstable tests.\n\n‚Üí unrdf: Always rdf-canonize (URDNA2015). Deterministic, testable.\n\nGraph Traversal\n\nEveryone rewrites helper functions: getAll, getOne, traverse list.\n\nDark matter: trivial but duplicated utilities.\n\n‚Üí unrdf: Clownface pointer is the one traversal surface.\n\nSummary of Dark Matter Eliminated\n\nChoosing a store\n\nInput format juggling\n\nQuery boilerplate\n\nValidation glue\n\nReasoning hacks\n\nGraph equality\n\nTraversal utilities\n\nSo the 80/20 is: 80% of RDF engineering time is spent not on ‚Äúknowledge modeling‚Äù but on glue code.\n\nüëâ unrdf vaporizes that glue by making it opinionated, canonical, and single-path.\n\nDo you want me to draft the manifesto-style README intro that states this dark-matter‚Äìelimination mission in bold, post-cyberpunk terms?\n\nYou said:\nWell, let‚Äôs think about how to best utilize the Composable pattern across the RDF ecosystem\nChatGPT said:\n\nYes ‚Äî now we‚Äôre designing the ergonomics, not just the substrate. If unrdf is opinionated, then the Composable pattern is the best way to expose it. Think: Vue/React composables, but for RDF.\n\nWhy Composables for RDF?\n\nRDF is a universe of concerns (parsing, querying, validating, reasoning).\n\nMost libraries bundle too much or fragment too much.\n\nComposables give one narrow surface per concern that can be stitched together freely.\n\nYou can \"pull in\" only what you need while still following the one-true-path.\n\nCore unrdf Composables\n1. useStore\n\nAlways N3.Store.\n\nProvides canonicalized RDF store.\n\nMethods: addQuads, stats, serialize.\n\nFoundation for all other composables.\n\n2. useTurtle\n\nFile IO + parsing/serialization of .ttl.\n\nAlways canonicalize into N3.Store.\n\nProvides loadGraph, saveGraph, listGraphFiles.\n\n3. useGraph\n\nCore high-level RDF API.\n\nWraps a store with common graph ops.\n\nMethods: .select, .ask, .update, .isomorphic, .union, .intersection, .difference.\n\nEntry point for 80% of use cases.\n\n4. useReasoner\n\nEYE-backed reasoning composable.\n\nAlways TTL rules.\n\nMethod: reason(dataStore, rulesStore).\n\n5. useValidator\n\nSHACL composable.\n\n.validate ‚Üí report with conforms + results.\n\n.validateOrThrow for hard-mode.\n\n6. useCanon\n\nCanonicalization and isomorphism.\n\n.canonicalize, .isomorphic.\n\nFor tests + stable pipelines.\n\n7. useJsonLd\n\nBridges JSON-LD <-> Turtle.\n\n.toJSONLD, .fromJSONLD.\n\nAlways compacts with provided context.\n\n8. usePointer\n\nGraph traversal composable.\n\nAlways clownface pointer.\n\nEncourages chainable API:\n\nconst ptr = usePointer(store)\nptr.node('ex:Alice').out('foaf:knows').value\n\nWhy This Works\n\nOrthogonality: Each composable is laser-focused. No overlaps.\n\nSingle-path opinionation: If you need parsing, it‚Äôs always useTurtle. If you need reasoning, it‚Äôs always useReasoner.\n\nComposable ergonomics: You can stitch them as needed:\n\nconst { store } = await useTurtle({ path: './graph' })\nconst graph = useGraph(store)\nconst valid = await useValidator().validate(shapes)\nconst enriched = await useReasoner().reason(store, rules)\n\n\nThis turns RDF into a ‚Äúcomposable knowledge fabric‚Äù:\n\nno adapters,\n\nno choices,\n\njust one way per concern.\n\nDo you want me to draw you a PlantUML C4 diagram showing the composables as building blocks (like puzzle pieces) in the unified unrdf ecosystem?\n\nYou said:\nAll right, I really want you to think about Composable surface to make sure we have everything covered\nChatGPT said:\nThought for 36s\n\nPerfect‚Äîlet‚Äôs lock the opinionated composable surface for unrdf (single path, N3-only, no overlap). Each composable owns exactly one concern; you stitch them as needed.\n\nCore (always N3.Store)\n\nuseStore() ‚Üí { store, add(quads), remove(quads), clear(), stats(), serialize({format}) }\n\nuseTerms() ‚Üí { iri(str), lit(val, dt|lang), bnode(id?), quad(s,p,o,g?) }\n\nusePrefixes() ‚Üí { register(map), expand(curie), shrink(iri), list() }\n\nuseIRIs() (graph:// resolver) ‚Üí { resolve(uri), map(prefix, path), maps() }\n\nuseLists(store) ‚Üí { read(head), write(items):head }\n\nIO (parsing/serialization only)\n\nuseTurtle() ‚Üí { parse(ttl):Store, write(store,{prefixes}):string }\n\nuseTurtleFS(dir) ‚Üí { loadAll():Store, save(name,store), list():string[], load(name):Store }\n\nuseNQuads() ‚Üí { parse(nq):Store, write(store):string }\n\nuseJsonLd() ‚Üí { toJSONLD(store,{context,frame}), fromJSONLD(doc):Store }\n\nQuery & Traversal\n\nuseGraph(store) ‚Üí { select(q), ask(q), update(q), construct(q):Store, difference(b), union(...g), intersection(b) }\n\nusePointer(store) (clownface) ‚Üí { node(iri|bnode), out(pred), in(pred), value(), add(), remove() }\n\nQuality & Inference\n\nuseValidator() (SHACL) ‚Üí { validate(data,shapes):{conforms,results}, validateOrThrow(...) }\n\nuseReasoner() (EYE/N3 rules) ‚Üí { reason(data,rules):Store }\n\nuseCanon() (URDNA2015) ‚Üí { canonicalize(store):nquads, isomorphic(a,b):boolean }\n\nChange, Provenance, Safety\n\nuseDelta() ‚Üí { diff(a,b):{added,removed}, patch(base,{added,removed}):Store }\n\nuseProvenance(store) (named graphs + prov) ‚Üí { stamp(activity,agent,at), withGraph(name):Store }\n\nuseTx(store) (atomic batch) ‚Üí { run(mutator):{ok, error?} }\n\nMutator gets a shadow Store; commit or rollback.\n\nPersistence & Snapshots\n\nuseSnapshot(dir) ‚Üí { save(store,tag):cid, load(cid|tag):Store, list():[] }\n(content-addressed; deterministic writer)\n\nOps & Observability\n\nuseMetrics() ‚Üí { wrap(label,fn), last(), timeline() }\n\nuseCache() (query result cache) ‚Üí { get(key), set(key,val,ttl), withGraphCache(graph) }\n\nCoverage Matrix (80/20 dark-matter)\n\nParse/Serialize: Turtle, N-Quads, JSON-LD ‚úÖ\n\nSingle store: N3 only, everywhere ‚úÖ\n\nQuery: SELECT/ASK/CONSTRUCT/UPDATE via Comunica on in-mem store ‚úÖ\n\nTraversal: clownface pointer ‚úÖ\n\nValidation: SHACL ‚úÖ\n\nReasoning: N3 rules (EYE) ‚úÖ\n\nCanon/Equivalence: URDNA2015 + isomorphic ‚úÖ\n\nLists: rdf:List read/write ‚úÖ\n\nNamespaces/IRIs: prefix registry + graph:// resolver ‚úÖ\n\nDiff/Patch: dataset delta primitives ‚úÖ\n\nProvenance: named graph stamping + prov helpers ‚úÖ\n\nTransactions: safe batch mutations ‚úÖ\n\nSnapshots: deterministic, content-addressed ‚úÖ\n\nMetrics/Cache: observable, cacheable ops ‚úÖ\n\nFS: opinionated dir loader/saver ‚úÖ\n\nExample (single, enforced path)\nimport { useTurtleFS, useGraph, useValidator, useReasoner, useDelta, useSnapshot } from 'unrdf';\n\nconst fs = useTurtleFS('./graph');\nconst base = await fs.load('default');\nconst graph = useGraph(base);\n\nconst rules = await fs.load('rules');         // N3 rules\nconst inferred = await useReasoner().reason(base, rules);\n\nconst shapes = await fs.load('shapes');\nawait useValidator().validateOrThrow(inferred, shapes);\n\nconst { added, removed } = useDelta().diff(base, inferred);\nconst next = useDelta().patch(base, { added, removed });\n\nawait fs.save('default', next);\nconst cid = useSnapshot('./snap').save(next, 'post-reasoned');\n\n\nThis surface is complete, non-overlapping, and opinionated: if it‚Äôs RDF in JS, you do it this one way‚Äîor you use another library.\n\nYou said:\nNow write a a RXIV paper with no latex\nChatGPT said:\n\nGot it. Here‚Äôs a draft in plain text that reads like an arXiv preprint, but without LaTeX markup. It positions unrdf as the opinionated substrate for RDF/knowledge computing.\n\nunrdf: An Opinionated Composable Framework for RDF Knowledge Operations\nAbstract\n\nThe RDF ecosystem has matured into a diverse set of libraries for parsing, querying, validating, and reasoning over linked data. However, this diversity has also created fragmentation: multiple overlapping APIs, inconsistent data models, and incompatible store abstractions. This paper introduces unrdf, a unification layer that enforces a single composable surface over the RDF universe. By adopting a strict opinionated stance ‚Äî one store, one query engine, one path ‚Äî unrdf removes ambiguity, eliminates redundant abstractions, and offers deterministic, reproducible graph operations. This design aims to provide the 80/20 ‚Äúdark matter‚Äù of RDF workflows: parsing, serialization, querying, validation, reasoning, canonicalization, and provenance, all under one coherent interface.\n\nIntroduction\n\nRDF and related standards (Turtle, SHACL, SPARQL, JSON-LD) form the backbone of the knowledge web. Yet, developers face friction when assembling applications due to the abundance of libraries with overlapping responsibilities. A typical project may mix N3 for parsing, Comunica for SPARQL, rdf-ext for datasets, rdf-validate-shacl for constraints, and eyereasoner for inference. Each library is useful in isolation, but together they form a patchwork of styles, APIs, and stores. This complexity hinders adoption outside semantic specialists.\n\nunrdf addresses this by enforcing a single opinionated path. The framework selects a canonical implementation for each layer ‚Äî N3 store as the universal substrate, Comunica as the SPARQL engine, EYE rules for reasoning, URDNA2015 for canonicalization, and SHACL for validation. These are wrapped in a composable API pattern, exposing cohesive surfaces such as useGraph, useValidator, useReasoner, and useDelta. The result is not a new ontology language or reasoner but a reduction of cognitive overhead for practitioners.\n\nDesign Principles\n\nOne Store Rule: All data resides in a single substrate ‚Äî the N3 Store. No competing dataset abstractions are permitted.\n\nComposable Surfaces: Each concern is expressed as a composable, returning a focused API object. Examples include useTurtle, useJsonLd, useGraph, useCanon.\n\nStrict Opinionation: If multiple options exist in the ecosystem, unrdf selects one and excludes the others. For example, N3 for parsing, not rdflib.\n\nDeterminism by Default: Canonical serialization, canonical URDNA2015, and reproducible query results ensure reproducibility and testability.\n\nDark Matter Coverage: The framework covers the core 80% of RDF workflows that almost every project requires, leaving exotic needs to external tools.\n\nAPI Overview\n\nCore: useStore, useTerms, usePrefixes, useLists\n\nI/O: useTurtle, useNQuads, useJsonLd, useTurtleFS\n\nQuery & Traversal: useGraph (SPARQL), usePointer (clownface traversal)\n\nValidation & Reasoning: useValidator (SHACL), useReasoner (EYE/N3 rules)\n\nEquivalence: useCanon (URDNA2015 canonicalization, isomorphism checks)\n\nChange Management: useDelta (diff/patch), useTx (transaction batch)\n\nPersistence: useSnapshot (content-addressed storage)\n\nOps: useMetrics, useCache\n\nThis API surface eliminates overlap: there is exactly one way to perform each operation.\n\nExample\nimport { useTurtleFS, useGraph, useValidator, useReasoner } from 'unrdf';\n\nconst fs = useTurtleFS('./graph');\nconst base = await fs.load('default');\n\nconst graph = useGraph(base);\nconst inferred = await useReasoner().reason(base, await fs.load('rules'));\nawait useValidator().validateOrThrow(inferred, await fs.load('shapes'));\n\nawait fs.save('default', inferred);\n\nDiscussion\n\nBy design, unrdf is not neutral. It does not attempt to represent every possible RDF style; it enforces one. The trade-off is intentional: a narrower surface, but one that guarantees interoperability, determinism, and simplicity. In effect, unrdf provides a substrate for knowledge operations akin to how Git provides a substrate for version control. Its composables embody the philosophy that knowledge software should be boring, predictable, and resistant to fragmentation.\n\nConclusion\n\nunrdf demonstrates that an opinionated unification of RDF tooling is not only possible but desirable. By constraining choice, it amplifies usability and composability, unlocking the latent power of RDF without exposing its internal complexity. We position unrdf as a substrate for the coming wave of knowledge-native systems, where the emphasis shifts from building bespoke frameworks to composing knowledge operations within a coherent runtime.\n\nDo you want me to extend this into a 15‚Äì20 page style preprint (with sections like Related Work, Threat Model, Future Work), or keep it as a short position paper like above?\n\nYou said:\nNow write a hypothetical impact report for 2027\nChatGPT said:\n\nAlright ‚Äî here‚Äôs a hypothetical 2027 impact report for unrdf, written like something a foundation or consortium would publish to show adoption and outcomes.\n\nunrdf: 2027 Impact Report\n\nStatus: Public Release\nDate: September 2027\n\nExecutive Summary\n\nSince its public release in early 2025, unrdf has evolved into the de facto substrate for RDF knowledge operations. By enforcing a strict, opinionated composable surface, unrdf transformed what was once a fragmented ecosystem of incompatible libraries into a unified, deterministic workflow.\nIn 2027, unrdf powers 43% of production RDF workloads in enterprise, academia, and public sector deployments. It has been adopted by major infrastructure providers, is bundled in multiple cloud-native platforms, and is now a baseline dependency for numerous open-source and industrial knowledge frameworks.\n\nKey Outcomes\n1. Ecosystem Unification\n\nOne Store Rule (N3 Store) became the de facto standard substrate for RDF data in JavaScript/TypeScript ecosystems. Competing abstractions (rdflib.js, rdf-ext) now interoperate via unrdf adapters rather than being used directly.\n\nLibraries that previously competed (Comunica, rdf-validate-shacl, eyereasoner, rdf-canonize, jsonld) now align under unrdf‚Äôs composable APIs.\n\n2. Developer Productivity\n\nReduction in API complexity by 70%: developers report building knowledge applications with ~‚Öì of the code compared to pre-unrdf stacks.\n\nDeterministic defaults (URDNA2015 canonicalization, ordered SPARQL results) made reproducible testing standard practice.\n\nComposables as lingua franca: useGraph, useValidator, useReasoner, useCanon are now widely recognized as the ‚Äúfour pillars‚Äù of RDF knowledge ops.\n\n3. Adoption Metrics\n\nEnterprise: 600+ companies, including top-10 banks and major telecom operators, report production use.\n\nAcademia: 40+ universities teach RDF workflows using unrdf in knowledge engineering courses.\n\nOpen Data: National statistical agencies in EU and LATAM adopted unrdf for publishing linked data at scale.\n\nAI Integration: 9 major LLM toolkits (including 3 open-source ones) embed unrdf for graph grounding and canonicalization.\n\n4. Standards Influence\n\nW3C‚Äôs KnowledgeOps Community Group cites unrdf as a reference implementation for composable RDF pipelines.\n\nThe URDNA2015 canonicalization process became a default expectation in most linked data platforms due to unrdf adoption.\n\nA proposed ‚ÄúComposable Graph Ops‚Äù draft spec (C-GOPS) emerged directly from unrdf‚Äôs surface design.\n\nCase Studies\n\nGlobal Bank\nMigrated 20 years of compliance data (18B triples) to unrdf pipelines. Reduced validation runtime from 7 days to 36 hours, achieving regulatory reproducibility guarantees.\n\nTelecom Operator (6G Core)\nUsed unrdf validation (SHACL + EYE) to enforce deterministic routing policies across distributed graph stores. Achieved sub-¬µs policy evaluation under high load.\n\nOpen Science Consortium\nStandardized reproducible simulation metadata via unrdf canonicalization. 50% reduction in ‚Äúirreproducible result‚Äù incidents across published research datasets.\n\nUnexpected Impacts\n\nComposable ‚Äúdark matter‚Äù surfaced:\nThe simple APIs (useGraph, useCanon, useReasoner) became metaphors beyond RDF, inspiring composable standards in other ecosystems (geospatial, time-series, even blockchain).\n\nunrdf ‚Üí LLM alignment:\nMultiple research labs demonstrated that unrdf‚Äôs deterministic canonical forms make RDF an ideal bridge for grounding LLM outputs in structured knowledge.\n\nCultural shift:\nRDF went from being considered ‚Äúacademic and messy‚Äù to boring, predictable infrastructure, similar to Git. This made RDF invisible yet indispensable.\n\nChallenges\n\nOpinionation backlash:\nSome communities resisted unrdf‚Äôs ‚Äúone store only‚Äù policy. However, by 2026, most objections diminished as interoperability adapters proved effective.\n\nPerformance plateau:\nScaling beyond 100B triples in-memory remains a challenge. Work is ongoing to combine unrdf‚Äôs deterministic semantics with GPU acceleration.\n\nGovernance tension:\nThe question of who stewards unrdf (OSS community vs foundation vs vendor consortium) remains unresolved.\n\nFuture Outlook (2028‚Äì2030)\n\nHardware Acceleration: Integration with domain-specific accelerators for graph workloads.\n\nCross-Domain Ops: Expansion of unrdf composables into temporal, geospatial, and event-based data.\n\nLLM-Native KnowledgeOps: Direct pipelines where unrdf canonicalization and reasoning feed into grounding layers for next-gen foundation models.\n\nCompositional Standards: Formalization of ‚ÄúGraphOps Composables‚Äù into W3C standards track.\n\nConclusion\n\nIn 2025, unrdf began as an experiment in radical simplification: one store, one query engine, one way. By 2027, it has reshaped the RDF landscape into something boring, predictable, and industrial-strength. The impact is not that RDF became more powerful, but that it became usable and reproducible at scale. The story of unrdf is the story of how knowledge operations became infrastructure.\n\nDo you want me to make a short press-style version of this (e.g., 2-page executive briefing with charts and adoption numbers), or keep this long-form technical impact report style?\n\nYou said:\nWe will also want to add zod support \nChatGPT said:\n\nGot it ‚Äî adding Zod support to unrdf is a very natural extension, because it closes the loop between knowledge representation (RDF) and runtime type safety / validation (Zod). Here‚Äôs how it fits into the opinionated Composable surface and the 80/20 dark matter unrdf is solving for:\n\nWhy Zod in unrdf?\n\nSchema enforcement at runtime\n\nSHACL is expressive but heavy; Zod is lightweight, fast, and developer-friendly.\n\nZod gives instant developer feedback when working with RDF-derived JSON.\n\nUse case: SPARQL SELECT results can be automatically parsed and validated into Zod-typed objects.\n\nBridging RDF ‚Üî application layer\n\nRDF is graph-first, Zod is object-first.\n\nWith Zod, unrdf can enforce contracts when mapping RDF graphs to JS/TS data structures.\n\nThis eliminates the ‚Äúuncertain JSON blob‚Äù problem when querying graphs.\n\nComposable synergy\n\nuseValidator composable already exists for SHACL.\n\nAdding useZod (or extending useValidator) lets devs apply lightweight validation without pulling in full SHACL reasoning.\n\nOpinionated rule: Zod validation is for app-facing data surfaces (SPARQL results, JSON-LD), while SHACL remains for RDF-native graph validation.\n\nExample Composable: useZod\nimport { z } from \"zod\";\nimport { useGraph } from \"unrdf\";\n\n// Define Zod schema for Person\nconst PersonSchema = z.object({\n  id: z.string().url(),\n  name: z.string(),\n  age: z.number().int().min(0),\n});\n\nasync function example() {\n  const graph = await useGraph(\"./graph\"); // load RDF store\n  \n  // SPARQL query\n  const results = await graph.select(`\n    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n    SELECT ?id ?name ?age WHERE {\n      ?id a foaf:Person ;\n          foaf:name ?name ;\n          foaf:age ?age .\n    }\n  `);\n\n  // Zod validation\n  const validated = results.map(r => PersonSchema.parse(r));\n  console.log(validated);\n}\n\n\nOpinionated behavior:\n\nSPARQL bindings are always converted into plain JS objects ({id, name, age}) before hitting Zod.\n\nZod ensures the mapping conforms to developer expectations, not just graph constraints.\n\n80/20 Dark Matter Solved with Zod\n\n80% developer friction today: translating RDF/SPARQL into application-safe types.\n\n20% of effort ‚Üí 80% of safety: by enforcing Zod schemas at boundaries, unrdf makes RDF data as ergonomic as JSON APIs.\n\nZod acts as the ‚Äúdark matter illuminator‚Äù for the hidden boundary between graph knowledge and application logic.\n\nHypothetical 2027 Impact Addendum\n\nIf unrdf integrates Zod in 2025‚Äì2026:\n\nBy 2027, 90% of unrdf developers use Zod validation on SPARQL results.\n\nEnterprise adoption accelerates, because type-safe graph access reduces onboarding friction for non-RDF engineers.\n\nZod integration becomes part of the ‚Äúboring infrastructure guarantee‚Äù ‚Äî RDF graphs behave like typed APIs, eliminating runtime surprises.\n\n‚ö° So the rule of thumb could be:\n\nInside the graph ‚Üí SHACL, reasoning, canonicalization.\n\nAt the app boundary ‚Üí Zod, runtime type safety, developer ergonomics.\n\nDo you want me to sketch a unified surface for how useValidator and useZod sit together in unrdf (like a mini-API proposal), or keep Zod as a separate composable (useZod) that only works on SPARQL/JSON-LD outputs?\n\nYou said:\nWorking backwards press release\nChatGPT said:\n\nPerfect. Here‚Äôs a working backwards press release (Amazon-style) for unrdf with Zod integration. It‚Äôs written as if the project is already launched and successful in 2027:\n\nPress Release\n\nFOR IMMEDIATE RELEASE\nDate: March 17, 2027\n\nIntroducing unrdf v3.0: The Opinionated Framework for the Entire RDF Universe ‚Äî Now with Native Zod Validation\n\nSan Francisco, CA ‚Äî Today the GitVan team announced the release of unrdf v3.0, the first opinionated JavaScript/TypeScript framework that unifies the entire RDF ecosystem into a single composable interface. With this release, unrdf introduces native Zod integration, allowing developers to validate SPARQL results, JSON-LD documents, and RDF-mapped objects with the same ease and confidence they already enjoy with JSON APIs.\n\nFor decades, RDF has promised a universal knowledge representation layer. Yet adoption lagged due to fragmented tooling, steep learning curves, and a lack of developer-friendly ergonomics. unrdf changes that:\n\nOne Store: N3.js as the canonical RDF store.\n\nOne Query Engine: Comunica as the single SPARQL execution layer.\n\nOne Validator: SHACL for graph-level validation, Zod for application-level type safety.\n\nOne API: A composable surface that makes graphs feel as natural as arrays and objects.\n\nCustomer Quotes\n\n‚ÄúWe reduced onboarding time for new engineers by 80%. They no longer need to ‚Äòlearn RDF‚Äô ‚Äî they just use Zod schemas like they would with any REST or GraphQL API.‚Äù\n‚Äî Head of Engineering, Fortune 100 Bank\n\n‚ÄúOur compliance team sleeps at night knowing SHACL guards the graphs, and our app team ships faster with Zod validating the boundaries. unrdf gave us both.‚Äù\n‚Äî CTO, Autonomous Robotics Startup\n\nKey Features\n\nZod Everywhere: Validate SPARQL SELECT results, JSON-LD payloads, or mapped RDF resources directly into Zod-typed objects.\n\nComposable by Design: Load graphs (useGraph), validate (useValidator, useZod), reason (useReasoner), serialize (useSerializer), all from a consistent surface.\n\nOpinionated Defaults: One RDF engine (N3.js + Comunica), one canonicalization algorithm (URDNA2015), one reasoning layer (EYE). No more choice paralysis.\n\nDeterministic by Default: Canonical quads, stable results, and predictable graph operations.\n\nDeveloper Ergonomics: Works seamlessly with modern TypeScript ‚Äî no glue code, no wrappers, just composables.\n\nBusiness Impact\n\nBy 2027, companies using unrdf have reported:\n\n50x reduction in RDF onboarding time for new developers.\n\n70% fewer runtime errors from unexpected RDF payloads.\n\n80% faster integration between RDF knowledge graphs and production systems.\n\nBreakthrough adoption: over 2,000 organizations, from fintech to healthcare, standardizing on unrdf as the de facto RDF framework for JavaScript/TypeScript.\n\nAvailability\n\nunrdf v3.0 is available today on npm\n.\nDocumentation, guides, and examples are live at unrdf.dev\n.\nThe project is open-source under the Apache 2.0 license.\n\nAbout GitVan\n\nGitVan is building the next generation of developer infrastructure, blending AI-powered automation with opinionated frameworks that eliminate complexity. From deterministic computation (ByteActor) to universal knowledge interfaces (unrdf), GitVan‚Äôs mission is to make the future of computing boring, reliable, and accessible.\n\nüî• That‚Äôs the press release.\n\nDo you also want me to write the FAQ (the second half of working backwards) ‚Äî where we anticipate the tough questions devs, CTOs, and skeptics will ask about unrdf and Zod?\n\nYou said:\nThis framework is mjs and jsdoc only period type script is a terrible idea that never works\nChatGPT said:\n\nGot it ‚Äî loud and clear. ‚úÖ\n\nThis means:\n\n.mjs only ‚Äî every module in the framework uses ECMAScript modules.\n\nJSDoc for type safety ‚Äî no .ts, no .d.ts, no TypeScript compiler in the toolchain.\n\nRuntime determinism first ‚Äî all contracts are enforced with Zod + SHACL + JSDoc, not TypeScript‚Äôs compile-time illusions.\n\nZero transpile tax ‚Äî what you write is what runs. Faster builds, simpler CI/CD, less tooling bleed.\n\nOpinionated developer ergonomics ‚Äî the ‚Äúonly way‚Äù is Zod + JSDoc + composables. If you want TypeScript, the answer is: use a different library.\n\nThis reinforces the philosophy you‚Äôve been outlining: opinionated simplicity. Instead of bending to the fragmented preferences of the developer ecosystem, unrdf declares a single correct way to handle RDF knowledge in JavaScript.\n\nHere‚Äôs how I‚Äôd capture it in the positioning statement for unrdf:\n\nüß≠ Positioning Statement\n\nunrdf is the opinionated RDF framework for JavaScript. It makes the RDF universe accessible through a single composable surface ‚Äî powered by N3.js, Comunica, SHACL, and Zod.\n\nNo TypeScript. Ever. TypeScript is an illusion of safety that collapses at runtime. unrdf guarantees correctness at the only level that matters: execution.\n\nJSDoc is the source of truth. Documentation, type hints, and developer experience are delivered directly via JSDoc, keeping the codebase minimal and expressive.\n\nZod is the contract. Runtime validation ensures that what you think your data is, and what it actually is, are always in sync.\n\nComposables everywhere. Every aspect of RDF ‚Äî graphs, queries, validation, reasoning, serialization ‚Äî is accessible through consistent composable functions.\n\nDo you want me to rewrite the working backwards press release to explicitly emphasize ‚ÄúJSDoc + Zod only, no TypeScript‚Äù so that it‚Äôs baked into the announcement DNA?\n\nYou said:\nRight and so let‚Äôs also make some utility functions that cover the dark matter 8020 that doesn‚Äôt fit between the Composable\nChatGPT said:\n\nPerfect ‚Äî this is the right instinct.\n\nThe composable surface covers the mainline use cases (load graphs, query with SPARQL, validate with SHACL, serialize, reason, convert to/from JSON-LD, etc).\n\nBut the dark matter 80/20 are the tiny, annoying, often-ignored utilities that everyone ends up rewriting in projects, and they don‚Äôt belong inside a high-level composable ‚Äî they belong in a utilities layer.\n\nHere‚Äôs a first set of unrdf/utils.mjs candidates that target that ‚Äúhidden 20%‚Äù:\n\n/**\n * @fileoverview unrdf utils ‚Äî opinionated dark-matter helpers\n * JSDoc + Zod, no TypeScript\n */\n\nimport { z } from \"zod\";\nimport { DataFactory } from \"n3\";\n\nconst { namedNode, literal, blankNode, quad } = DataFactory;\n\n/* ------------------------------ TERMS ------------------------------ */\n\n/** Normalize any input into a NamedNode */\nexport function asNamedNode(iri) {\n  if (iri?.termType === \"NamedNode\") return iri;\n  return namedNode(String(iri));\n}\n\n/** Normalize a value into a Literal */\nexport function asLiteral(value, datatype = \"http://www.w3.org/2001/XMLSchema#string\") {\n  return literal(String(value), datatype);\n}\n\n/** Convert a quad to a JSON object */\nexport function quadToJSON(q) {\n  return {\n    subject: q.subject.value,\n    predicate: q.predicate.value,\n    object: q.object.value,\n    graph: q.graph.value || null,\n  };\n}\n\n/** Convert JSON back to a quad */\nexport function jsonToQuad(obj) {\n  return quad(\n    asNamedNode(obj.subject),\n    asNamedNode(obj.predicate),\n    obj.object.startsWith(\"http\")\n      ? asNamedNode(obj.object)\n      : asLiteral(obj.object),\n    obj.graph ? asNamedNode(obj.graph) : undefined\n  );\n}\n\n/* --------------------------- VALIDATION ---------------------------- */\n\n/** Zod schema for a quad JSON representation */\nexport const QuadSchema = z.object({\n  subject: z.string().url(),\n  predicate: z.string().url(),\n  object: z.string(),\n  graph: z.string().url().nullable().optional(),\n});\n\n/** Validate a quad JSON */\nexport function validateQuadJSON(obj) {\n  return QuadSchema.parse(obj);\n}\n\n/* -------------------------- GRAPH HELPERS -------------------------- */\n\n/** Get all triples matching predicate across any subject */\nexport function pluck(store, predicateIRI) {\n  return store.getQuads(null, asNamedNode(predicateIRI), null, null);\n}\n\n/** Get a map of subject -> array of objects for a predicate */\nexport function indexByPredicate(store, predicateIRI) {\n  const quads = pluck(store, predicateIRI);\n  const out = new Map();\n  for (const q of quads) {\n    const s = q.subject.value;\n    if (!out.has(s)) out.set(s, []);\n    out.get(s).push(q.object.value);\n  }\n  return out;\n}\n\n/* -------------------------- SERIALIZATION -------------------------- */\n\n/** Quick Turtle dump with minimal prefixes */\nexport async function debugTurtle(store, engine) {\n  return engine.serializeTurtle(store, { prefixes: { ex: \"http://example.org/\" } });\n}\n\n/* -------------------------- IDENTIFIERS ---------------------------- */\n\n/** Create a stable blank node generator */\nexport function makeBNodeGenerator(prefix = \"bn\") {\n  let i = 0;\n  return () => blankNode(`${prefix}${i++}`);\n}\n\nWhy these utilities are dark matter 80/20\n\nasNamedNode / asLiteral ‚Äî every RDF project has dozens of ad-hoc namedNode() or literal() wrappers. We enforce one canonical way.\n\nquadToJSON / jsonToQuad ‚Äî bridging RDF with app logic is always needed, and it‚Äôs always re-implemented. This is the glue layer.\n\nZod validation ‚Äî 95% of RDF libs ignore validation. Here, it‚Äôs opinionated: you must validate before using.\n\npluck / indexByPredicate ‚Äî real-world RDF often devolves into ‚Äújust get me all the labels‚Äù ‚Äî these helpers kill boilerplate.\n\ndebugTurtle ‚Äî for logging/inspection, every dev hacks this. It‚Äôs now built in.\n\nmakeBNodeGenerator ‚Äî reproducible blank node IDs without collisions.\n\nWould you like me to extend this set into a full utils/ directory (e.g. term-utils.mjs, graph-utils.mjs, validation-utils.mjs, etc.) so that we cover all of the likely dark matter surfaces? That way the core composables/ stay clean, and utils/ is the sanctioned catch-all."
        }
    ]
}