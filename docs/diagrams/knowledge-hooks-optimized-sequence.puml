@startuml Knowledge Hook Engine - Optimized Execution Flow
participant "App" as app
participant "KnowledgeHook\nEngine" as khe
participant "HookBatch\nExecutor" as hbe
participant "ConditionCache" as cc
participant "StoreCache" as sc
participant "FileResolver\n(preloaded)" as fr
participant "Oxigraph\nStore" as ox

app -> khe: execute(store, delta)
activate khe
note right: ✅ Decoupled from Transaction Manager\nDirect hook execution, no inheritance

khe -> khe: Warm file cache\n(first execution only)
note right: ✅ FIX #2: Pre-load all files\nCompute hashes ONCE at startup

== Phase 1: Parallel Condition Evaluation (ONCE) ==

khe -> hbe: evaluateConditions(hooks, store)
activate hbe
note right: ✅ Conditions evaluated ONCE per transaction\n(not per hook like before)

par For each hook (parallel)
  hbe -> cc: get(hookId, storeVersion)
  note right: ✅ FIX #4: Condition cache\nWith TTL + store version tracking

  alt Cache hit
    cc --> hbe: Cached boolean result
    note right: 40-50% latency reduction\nfrom cached conditions
  else Cache miss
    hbe -> sc: getOrCreate(store)
    activate sc

    note right: ✅ FIX #1: Store cache\nVersion-based LRU cache

    sc -> sc: Check if store version\nchanged
    alt Store version unchanged
      sc --> hbe: Cached Oxigraph store
      note right: Instant hit!\nNo conversion needed
    else Store version changed
      sc -> ox: createStore(quads)
      note right: ✅ FIX #1: Convert ONCE\nper transaction version\n(not per query)\n\n50-70% latency reduction
      ox --> sc: OxigraphStore
      sc --> hbe: Cached instance
    end
    deactivate sc

    alt Condition type: SPARQL
      hbe -> ox: query(sparql)
      ox --> hbe: SPARQL results
      note right: Oxigraph already loaded!\nNo store conversion overhead
    else Condition type: FILE
      hbe -> fr: resolveFile(path)
      fr --> hbe: { content, hash }
      note right: ✅ FIX #2: Pre-loaded at startup\nNo file I/O in hot path\n20-30% latency reduction
    end

    hbe -> cc: set(hookId, storeVersion, result)
    note right: Cache result for TTL window
  end
end

hbe --> khe: Condition results[N]
deactivate hbe

== Phase 2: Parallel Hook Execution (with Batching) ==

khe -> hbe: executeBatches(satisfiedHooks, store, delta)
activate hbe
note right: ✅ FIX #3: Default batching by dependency\n30-50% latency reduction from parallelization

par Batch 1: Independent hooks (parallel)
  hbe -> hbe: for each hook in batch1:\nhook.run(event)
  note right: All run simultaneously\nNo sequential bottleneck
end

par Batch 2: Depends on Batch 1 (parallel)
  hbe -> hbe: for each hook in batch2:\nhook.run(event)
end

par Batch N: Final batch
  hbe -> hbe: for each hook in batchN:\nhook.run(event)
end

hbe --> khe: Execution results[N]
deactivate hbe

== Phase 3: Lightweight Receipt Generation ==

khe -> khe: generateReceipt(results, delta)
note right: ✅ Decoupled from TransactionManager\nOptional receipt generation\n(can skip if not needed)

khe --> app: Execution results + receipt
deactivate khe

note over app,ox
  ⏱️ LATENCY IMPROVEMENTS:

  Current:  ~1000ms (baseline)
  ✅ FIX #1 (Store cache): 70% reduction → 300ms
  ✅ FIX #2 (File pre-load): 82% reduction → 180ms
  ✅ FIX #3 (Batching): 86% reduction → 140ms
  ✅ FIX #4 (Condition cache): 91% reduction → 90ms
  ✅ FIX #5 (OTEL batching): 92% reduction → 80ms

  Combined Impact: 80-92% latency reduction
  Expected: 80ms from 1000ms baseline
end note

note right of khe
  ✅ KEY IMPROVEMENTS:

  1. Decoupled architecture (no TransactionManager inheritance)
  2. Single store conversion per transaction (not per query)
  3. File I/O happens at startup (not in hot path)
  4. Condition evaluation cached (not repeated)
  5. Hooks execute in dependency-ordered batches
  6. OTEL spans batched (not per hook)

  Result: Maximum parallelization with minimal overhead
end note
@enduml
