# @unrdf/daemon Architecture Analysis

**Research Date**: 2026-01-11
**Analyst**: Research Agent
**Purpose**: Document existing daemon infrastructure for YAWL MI pattern integration

---

## Executive Summary

The `@unrdf/daemon` package provides a **comprehensive background task orchestration system** with event-driven architecture, distributed coordination, and security hardening. The package contains **50 source files** (9,447 total LoC) organized into 5 major subsystems.

**Key Finding**: The daemon has **robust job scheduling and distribution mechanisms** but **lacks dedicated worker pool infrastructure** for parallel Multiple Instance (MI) task execution patterns required by YAWL.

---

## 1. File Inventory (50 files)

### 1.1 Core Components (6 files)

| File | LoC | Purpose |
|------|-----|---------|
| `daemon.mjs` | 314 | Core daemon class with EventEmitter, operation scheduling, execution lifecycle |
| `daemon-optimized.mjs` | 409 | High-performance variant with batch scheduling, O(1) cache, 10K+ concurrent ops |
| `batch-scheduler.mjs` | 73 | Batch operation accumulator (100 ops/10ms flush) for throughput optimization |
| `lru-cache-optimized.mjs` | 122 | O(1) LRU cache for completed operations (5K default size) |
| `schemas.mjs` | 241 | 12 Zod validation schemas (daemon config, operations, receipts, health, metrics) |
| `trigger-evaluator.mjs` | 186 | Trigger evaluation (cron, interval, idle, reactive, event patterns) |
| `index.mjs` | 81 | Package exports and public API surface |

**Total Core LoC**: 1,426

### 1.2 Integration Modules (13 modules, 30 files)

#### Distributed Coordination
| File | LoC | Purpose |
|------|-----|---------|
| `distributed.mjs` | 183 | Raft-based leadership coordination, work distribution (round-robin/hash/least-loaded) |
| `consensus.mjs` | 264 | Raft consensus manager with operation log, replication tracking, partition detection |
| `consensus-state.mjs` | ~50 | Consensus state enumerations (PENDING/COMMITTED/FAILED, HEALTHY/PARTITIONED/RECOVERING) |
| `consensus-handlers.mjs` | ~150 | Event handlers for leader election, term updates, node joins/leaves |
| `task-distributor.mjs` | 255 | Task assignment across cluster nodes, load tracking, heartbeat health monitoring |

**Total Distributed LoC**: ~902

#### YAWL Integration (Existing)
| File | LoC | Purpose |
|------|-----|---------|
| `yawl.mjs` | 437 | YawlDaemonBridge class - recurring case creation, timeout enforcement, retry scheduling, deferred choice triggers |
| `yawl-handlers.mjs` | 277 | Handler factories for case creation, timeout watch, retry execution, parallel task distribution |
| `yawl-schemas.mjs` | 61 | Zod schemas (retry policy, timeout config, distribution strategy, bridge config) |

**Key YAWL Features**:
- Recurring case creation (cron/interval)
- Task timeout enforcement with cancellation
- Automatic retry with exponential backoff
- Deferred choice trigger waiting
- **Parallel task distribution** (basic round-robin)

**Gap**: YAWL integration distributes tasks but **does NOT manage worker pools or MI instance coordination**.

**Total YAWL LoC**: 775

#### Other Integrations
| Module | Files | LoC | Purpose |
|--------|-------|-----|---------|
| Streaming | 1 | 513 | Real-time RDF change feeds, reactive event workflows |
| Hooks Policy | 4 | ~750 | Policy-based Knowledge Hook scheduling with dependency resolution |
| Knowledge Rules | 4 | ~600 | Inference engine integration, rule evaluation, pattern matching |
| Federation Query | 3 | ~600 | Federated SPARQL query execution across distributed stores |
| Observability | 3 | ~750 | OpenTelemetry integration with distributed tracing and monitors |
| Receipts (Merkle) | 3 | ~550 | Merkle tree receipt generation with cryptographic proofs |
| V6 DeltaGate | 5 | ~650 | Î”Gate control plane with unified receipt management |
| KGC 4D Sourcing | 2 | ~400 | Temporal event sourcing with time-travel capabilities |
| Event Store | 1 | 270 | Temporal event sourcing with KGC integration |
| Nitro Tasks | 1 | 466 | Bidirectional integration with Nitro Task Engine |

**Total Integration LoC**: ~5,889

### 1.3 Security & Middleware (6 files)

| File | LoC | Purpose |
|------|-----|---------|
| `auth/api-key-auth.mjs` | 274 | BLAKE3 hashing, constant-time verification, environment-aware enforcement |
| `auth/crypto-utils.mjs` | ~100 | Secure API key generation, hashing, verification utilities |
| `middleware/rate-limiter.mjs` | 427 | Token bucket rate limiter (requests/window, burst handling) |
| `middleware/rate-limiter.schema.mjs` | ~80 | Rate limit configuration and result schemas |
| `middleware/security-headers.mjs` | 645 | CSP, CORS, request limits, security header middleware |
| `security-audit.mjs` | ~300 | Injection detection (SQL/SPARQL/command), secret scanning, path traversal prevention |

**Total Security LoC**: ~1,826

### 1.4 Testing (18 E2E tests)

| Test Category | Files | LoC | Coverage |
|---------------|-------|-----|----------|
| Core daemon | 1 | 25,926 | Scheduling, execution, clustering, metrics |
| YAWL integration | 3 | 84,854 | Basic flow, error recovery, performance |
| Cross-package | 1 | 35,758 | Integration with consensus, federation, KGC |
| Consensus | 1 | 24,040 | Raft coordination, replication, partition recovery |
| Specialized | 12 | ~300K | Federation, hooks, knowledge rules, observability, edge cases |

**Total Test LoC**: ~470K (comprehensive coverage)

---

## 2. Job Scheduler Implementation

### 2.1 Core Scheduling Mechanism

**Class**: `Daemon` (daemon.mjs)

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Daemon                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ operations: Map<operationId, { handler, metadata }>        â”‚
â”‚ operationQueue: Array<operationId>                         â”‚
â”‚ completedOperations: LRUCache(1000)                        â”‚
â”‚ activeCount: number                                         â”‚
â”‚ maxConcurrent: number (config, default: 5)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scheduling Flow**:
1. `schedule(operation)` - Registers operation with handler function
2. Adds to `operations` Map (persistent registry)
3. Pushes `operationId` to `operationQueue` array
4. Emits `operation:enqueued` event
5. **No automatic execution** - requires explicit `execute(operationId)` call

**Key Observation**: Daemon is **demand-driven** (manual execution), not **queue-driven** (automatic worker dequeue).

### 2.2 Optimized Scheduler

**Class**: `OptimizedDaemon` (daemon-optimized.mjs)

**Enhancements**:
- **Batch Scheduler**: Accumulates 100 operations, flushes every 10ms
- **Optimized LRU Cache**: O(1) get/set with 5K default size
- **Performance Metrics**: P50/P95/P99 latency tracking
- **Throughput Calculation**: ops/sec with periodic collection
- **Compact Metadata**: Minimal memory footprint for 10K+ concurrent ops

**Designed For**: High-throughput scenarios with rapid operation registration

**Still Missing**: Automatic worker pool dequeue and parallel execution

### 2.3 Trigger Evaluation

**Module**: `trigger-evaluator.mjs`

**Supported Triggers**:
| Type | Evaluation Logic | Use Case |
|------|------------------|----------|
| `interval` | `now >= lastExecuted + intervalMs` | Periodic tasks |
| `cron` | Parses cron expression via `cron-parser` | Scheduled workflows |
| `idle` | `now >= lastActivity + idleThresholdMs` | Cleanup, maintenance |
| `reactive` | Entity mutation events (create/update/delete) | Event-driven hooks |
| `event` | Custom event name + filter predicate | Deferred choices, external triggers |

**Evaluation Model**: Pure functions returning `{ shouldExecute: boolean, nextExecutionTime: ms }`

**Gap**: Trigger evaluation exists but **no automatic poller** to continuously check triggers and enqueue operations.

---

## 3. Worker Pool Architecture

### 3.1 Current State: NO DEDICATED WORKER POOL

**Finding**: The daemon uses **concurrency limiting** but **NOT worker pool management**.

**Evidence**:
```javascript
// daemon.mjs (line 96)
this.activeCount = 0;

// execute() method
this.activeCount += 1;
// ... execute operation ...
this.activeCount -= 1;
```

**Concurrency Model**: Simple counter-based semaphore
- Tracks `activeCount` vs `maxConcurrent` (default: 5)
- **No pre-allocated worker threads/processes**
- **No work-stealing or load balancing between workers**
- **No idle worker pool waiting for tasks**

### 3.2 Distributed Task Distribution

**Class**: `DistributedTaskDistributor` (task-distributor.mjs)

**Purpose**: Assigns operations to **cluster nodes**, not local workers

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DistributedTaskDistributor                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ assignments: Map<operationId, TaskAssignment>               â”‚
â”‚ nodeLoadMap: Map<nodeId, loadCount>                        â”‚
â”‚ nodeHealthMap: Map<nodeId, HealthStatus>                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ distribute(ops, strategy) -> Map<nodeId, ops[]>            â”‚
â”‚ assignToNode(opId, nodeId)                                  â”‚
â”‚ getAssignments(nodeId) -> TaskAssignment[]                 â”‚
â”‚ updateStatus(opId, status)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Distribution Strategies**:
1. **Round-Robin**: Cycle through healthy nodes
2. **Least-Loaded**: Select node with fewest assigned operations
3. **Hash**: Consistent hashing by operationId

**Coordination**: Integrates with `ClusterManager.getHealthyNodes()` and heartbeat monitoring

**Gap for YAWL MI**: Distributes across **nodes**, not across **local worker instances** for parallel MI execution.

### 3.3 YAWL Parallel Task Distribution

**Method**: `YawlDaemonBridge.distributeAndSplitTasks(caseId, taskIds, options)`

**Implementation** (yawl-handlers.mjs, line 160):
```javascript
export function createDistributionHandler(bridge, caseId, taskIds, strategy, distributionId) {
  return async () => {
    const results = [];
    // Sequential enablement - NOT parallel execution
    for (let i = 0; i < taskIds.length; i += 1) {
      const taskId = taskIds[i];
      const result = await bridge.yawlEngine.enableTask({ caseId, taskId });
      results.push({ taskId, result, index: i });
    }
    // ...
  };
}
```

**Current Behavior**: Enables tasks **sequentially** using loop, not parallel workers

**Gap**: YAWL MI patterns require **concurrent task instances** executing in parallel, not sequential enablement.

---

## 4. Coordination Mechanisms

### 4.1 Raft Consensus Integration

**Module**: `consensus.mjs`

**Class**: `ConsensusManager`

**Coordination Primitives**:
| Primitive | Implementation | Purpose |
|-----------|----------------|---------|
| Leader Election | Listens to `raftCoordinator.on('leader_elected')` | Determines which node executes 'leader' scoped operations |
| Operation Log | `operationLog: Array<LogEntry>` | Ordered log of replicated operations |
| Replication Tracking | `replicatedOperations: Map<opId, state>` | PENDING â†’ COMMITTED â†’ APPLIED state transitions |
| Commit Index | `commitIndex`, `lastAppliedIndex` | Tracks consensus progress |
| Partition Detection | Timer-based heartbeat monitoring | Detects network partitions, triggers recovery |
| Log Compaction | Triggered at 5K operations | Maintains log size bounds |

**State Machine**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ConsensusOperationState                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PENDING     â†’ Operation added to log                        â”‚
â”‚ REPLICATED  â†’ Acknowledged by quorum                        â”‚
â”‚ COMMITTED   â†’ Consensus reached (leader updates commitIndex)â”‚
â”‚ APPLIED     â†’ Executed on local daemon                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Integration with Daemon**:
- `integrateRaftNode(daemon, raftNode)` - Binds leader state to `daemon.isLeader`
- `distributeWork(daemon, membershipManager, operations, strategy)` - Routes operations based on `clusterScope`:
  - `local`: Current node only
  - `leader`: Leader node only
  - `global`: Distributed via strategy

**Relevance to YAWL MI**: Consensus enables **distributed coordination** of workflow cases across cluster, but **NOT intra-case parallelism**.

### 4.2 Event-Driven Coordination

**Architecture**: All modules inherit from `EventEmitter`

**Key Event Flows**:

**Daemon Events**:
- `daemon:started`, `daemon:stopped`
- `operation:enqueued`, `operation:started`, `operation:success`, `operation:failure`
- `leadership_changed`, `leadership_lost` (from Raft integration)

**YAWL Bridge Events**:
- `bridge:started`, `bridge:stopped`
- `case:created-by-schedule`
- `task:timeout-enforced`, `task:retry-executed`, `task:retry-exhausted`
- `tasks:distributed`

**Consensus Events**:
- `consensus:initialized`, `consensus:remove_node`
- `log:compacted`

**Coordination Pattern**: Publish-subscribe model with listeners subscribing to lifecycle events

**Gap for YAWL MI**: Events coordinate **operation lifecycle**, but no events for **worker instance lifecycle** (spawn, idle, busy, complete).

### 4.3 Cluster Membership

**Component**: `ClusterManager` (external dependency, interface only)

**Expected Interface**:
```javascript
clusterManager.getHealthyNodes() -> Array<nodeId>
clusterManager.on('node_joined', handler)
clusterManager.on('node_left', handler)
clusterManager.on('health_degraded', handler)
```

**Usage in Daemon**:
- `DistributedTaskDistributor` queries healthy nodes for task assignment
- `ConsensusManager` listens to membership changes for replication updates
- `distributeWork()` uses healthy nodes for operation routing

**Gap for YAWL MI**: Tracks **cluster nodes**, not **worker instances within a node**.

---

## 5. Integration Points for YAWL

### 5.1 Existing YAWL Integration

**Module**: `integrations/yawl.mjs` (437 LoC)

**Class**: `YawlDaemonBridge`

**Capabilities**:
| Feature | Implementation | Status |
|---------|----------------|--------|
| Recurring case creation | `scheduleRecurringCase(workflowId, schedule, params)` | âœ… Implemented |
| Task timeout enforcement | `watchTaskTimeout(caseId, taskId, timeoutMs)` | âœ… Implemented |
| Automatic retry | `scheduleRetry(caseId, taskId, backoffPolicy)` | âœ… Implemented |
| Deferred choice triggers | `waitForChoiceTrigger(caseId, taskId, triggerPattern)` | âœ… Implemented |
| Parallel task distribution | `distributeAndSplitTasks(caseId, taskIds, options)` | âš ï¸ Sequential only |

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YawlDaemonBridge                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ daemon: Daemon                                              â”‚
â”‚ yawlEngine: WorkflowEngine                                  â”‚
â”‚ caseSchedules: Map<workflowId, schedule>                   â”‚
â”‚ taskTimeouts: Map<taskId, timeout>                         â”‚
â”‚ taskRetries: Map<taskId, retryState>                       â”‚
â”‚ choiceTriggers: Map<taskId, trigger>                       â”‚
â”‚ parallelDistributions: Map<distributionId, state>          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Event Listeners**:
- `yawlEngine.on('task:failed')` â†’ Auto-schedule retry
- `yawlEngine.on('task:enabled')` â†’ Start timeout watch
- `yawlEngine.on('case:completed')` â†’ Cleanup timeouts and retries

**Security Integration**:
- Injection detection on `workflowId` parameters
- Sanitized error propagation

### 5.2 YAWL Engine Interaction Model

**Expected Interface** (from code analysis):
```javascript
// Case Management
await yawlEngine.createCase({ workflowId, caseId, inputData });

// Task Operations
await yawlEngine.enableTask({ caseId, taskId });
await yawlEngine.cancelTask({ caseId, taskId, reason });

// Events
yawlEngine.on('task:enabled', (event) => { ... });
yawlEngine.on('task:failed', (event) => { ... });
yawlEngine.on('case:completed', (event) => { ... });
```

**Assumptions**:
- YAWL engine is EventEmitter-like (has `.on()` method)
- Async operation API (all methods return Promises)
- Task enablement is individual (not batched)

**Gap for MI**: Current API assumes **single task instances**, no MI-specific operations like:
- `createMIInstances(caseId, taskId, count, strategy)`
- `getMIInstanceStatus(caseId, taskId, instanceId)`
- `cancelMIInstance(caseId, taskId, instanceId)`

### 5.3 Distribution Strategy Configuration

**Schema**: `DistributionStrategySchema` (yawl-schemas.mjs)

**Supported Strategies**:
```javascript
z.enum(['round-robin', 'least-loaded', 'random', 'affinity'])
```

**Current Usage**: Passed to `distributeAndSplitTasks()` but **not actively used** in handler

**Observation**: Strategy enum exists but implementation is **incomplete** - only round-robin via cluster distribution.

### 5.4 Retry and Timeout Policies

**Retry Policy Schema** (yawl-schemas.mjs):
```javascript
{
  maxAttempts: 3,              // Max retry attempts
  backoffMs: 1000,             // Initial backoff
  backoffMultiplier: 2,        // Exponential multiplier
  maxBackoffMs: 30000,         // Backoff cap
  jitterFactor: 0.1            // Random jitter (0-10%)
}
```

**Timeout Config Schema**:
```javascript
{
  taskTimeoutMs: 30000,        // Default task timeout
  caseTimeoutMs: 3600000,      // Default case timeout (1 hour)
  checkIntervalMs: 5000        // Timeout check frequency
}
```

**Integration**: Retry and timeout policies are **per-task**, not per-MI-instance

**Gap for MI**: Need MI-aware policies:
- Threshold-based completion (e.g., "complete after 80% of instances succeed")
- Cancellation-on-first-failure vs. wait-for-all strategies

---

## 6. Gap Analysis: YAWL MI Pattern Support

### 6.1 Multiple Instance Patterns Required

**YAWL MI Patterns** (from YAWL specification):
1. **MI without synchronization** - Fire-and-forget parallel instances
2. **MI with a priori design-time knowledge** - Fixed instance count, wait for all
3. **MI with a priori runtime knowledge** - Dynamic instance count from input data
4. **MI without a priori runtime knowledge** - Instances spawned incrementally until condition met

**Completion Conditions**:
- **Threshold**: Complete after N instances succeed
- **Cancellation Region**: Cancel remaining on first failure
- **Dynamic Adjustment**: Add/remove instances during execution

### 6.2 Missing Infrastructure

| Component | Required for YAWL MI | Current Status | Gap Severity |
|-----------|----------------------|----------------|--------------|
| **Worker Pool** | Pre-allocated workers for parallel execution | âŒ None - single-threaded execution model | ğŸ”´ CRITICAL |
| **MI Instance Registry** | Track instance state (executing/completed/failed) | âŒ None - only task-level tracking | ğŸ”´ CRITICAL |
| **Instance Coordinator** | Manage instance lifecycle, enforce completion conditions | âŒ None - no orchestration layer | ğŸ”´ CRITICAL |
| **Parallel Executor** | Execute MI instances concurrently | âŒ Sequential loop in `createDistributionHandler` | ğŸ”´ CRITICAL |
| **Instance-Aware API** | YAWL engine methods for MI operations | âŒ Current API is task-level only | ğŸŸ¡ HIGH |
| **Completion Strategy** | Threshold/cancellation logic | âŒ None - waits for all tasks | ğŸŸ¡ HIGH |
| **Dynamic Spawn** | Add instances during execution | âŒ Fixed task list at distribution time | ğŸŸ¡ HIGH |
| **Instance Metrics** | Per-instance latency, success rate | âš ï¸ Daemon tracks operation metrics | ğŸŸ¢ MEDIUM |
| **Error Propagation** | Instance failures to case level | âš ï¸ Basic via event system | ğŸŸ¢ MEDIUM |

### 6.3 Detailed Gaps

#### Gap 1: No Worker Pool Management

**Required**:
```javascript
class WorkerPool {
  constructor(size, taskFactory);
  async spawn();                    // Add worker
  async shutdown(workerId);         // Remove worker
  async execute(task);              // Assign to idle worker
  getIdleWorkers() -> Worker[];
  getBusyWorkers() -> Worker[];
}
```

**Current**: Single-threaded execution with `activeCount` semaphore

**Impact**: Cannot execute MI instances in parallel on a single daemon node

**Workaround**: Distribute MI instances across cluster nodes (inefficient for small MI sets)

#### Gap 2: No MI Instance Registry

**Required**:
```javascript
class MIInstanceRegistry {
  registerInstance(caseId, taskId, instanceId, context);
  updateStatus(caseId, taskId, instanceId, status);
  getInstanceStatus(caseId, taskId, instanceId) -> Status;
  getCompletedCount(caseId, taskId) -> number;
  getFailedCount(caseId, taskId) -> number;
  checkCompletionCondition(caseId, taskId, condition) -> boolean;
}
```

**Current**: `YawlDaemonBridge.taskRetries` and `taskTimeouts` are **task-level**, not instance-level

**Impact**: No visibility into individual MI instance state, cannot enforce threshold-based completion

#### Gap 3: No Instance Coordinator

**Required**:
```javascript
class MICoordinator {
  async spawnInstances(caseId, taskId, count, strategy);
  async waitForCompletion(caseId, taskId, condition);
  async cancelRemaining(caseId, taskId);
  async addInstance(caseId, taskId, context);     // Dynamic spawn
}
```

**Current**: `distributeAndSplitTasks()` enables tasks sequentially, no coordination

**Impact**: Cannot dynamically adjust instances, cannot cancel on threshold

#### Gap 4: No Parallel Execution Engine

**Required**:
```javascript
// Execute MI instances in parallel with concurrency limit
async function executeParallel(instances, maxConcurrent) {
  const results = [];
  const executing = new Set();

  for (const instance of instances) {
    if (executing.size >= maxConcurrent) {
      await Promise.race(executing);
    }
    const promise = executeInstance(instance)
      .finally(() => executing.delete(promise));
    executing.add(promise);
    results.push(promise);
  }

  return Promise.all(results);
}
```

**Current**: Sequential `for` loop in `createDistributionHandler`

**Impact**: MI instances execute serially, no performance benefit from parallelism

#### Gap 5: No MI-Specific YAWL API

**Required Methods**:
- `yawlEngine.createMITask(caseId, taskId, instanceCount, strategy)`
- `yawlEngine.getMIInstanceStatus(caseId, taskId, instanceId)`
- `yawlEngine.completeMITask(caseId, taskId, condition)`
- `yawlEngine.cancelMIInstance(caseId, taskId, instanceId)`

**Current**: Only task-level operations (`enableTask`, `cancelTask`)

**Impact**: Bridge cannot distinguish between regular tasks and MI tasks

#### Gap 6: No Completion Condition Enforcement

**Required**:
```javascript
// Threshold-based completion
const completionCondition = {
  type: 'threshold',
  minInstances: 5,
  minSuccessRate: 0.8
};

// Cancellation region
const completionCondition = {
  type: 'first-failure',
  cancelRemaining: true
};
```

**Current**: No completion condition evaluation

**Impact**: Cannot implement YAWL cancellation regions or partial completions

### 6.4 Architectural Recommendations

#### Recommendation 1: Implement Worker Pool Abstraction

**Location**: Create `packages/daemon/src/worker-pool.mjs`

**Interface**:
```javascript
export class WorkerPool {
  constructor(size, options) {
    this.size = size;
    this.workers = [];
    this.taskQueue = [];
    this.idleWorkers = new Set();
    this.busyWorkers = new Map();
  }

  async initialize() {
    for (let i = 0; i < this.size; i++) {
      const worker = await this.spawnWorker(i);
      this.workers.push(worker);
      this.idleWorkers.add(worker);
    }
  }

  async execute(task) {
    if (this.idleWorkers.size === 0) {
      return new Promise((resolve) => {
        this.taskQueue.push({ task, resolve });
      });
    }

    const worker = this.getIdleWorker();
    this.idleWorkers.delete(worker);
    this.busyWorkers.set(worker.id, task);

    try {
      const result = await worker.execute(task);
      return result;
    } finally {
      this.busyWorkers.delete(worker.id);
      this.idleWorkers.add(worker);
      this.processQueue();
    }
  }

  processQueue() {
    while (this.taskQueue.length > 0 && this.idleWorkers.size > 0) {
      const { task, resolve } = this.taskQueue.shift();
      resolve(this.execute(task));
    }
  }
}
```

**Integration with Daemon**: Replace `activeCount` semaphore with WorkerPool

#### Recommendation 2: Create MI Instance Registry

**Location**: Create `packages/daemon/src/integrations/mi-instance-registry.mjs`

**Schema**:
```javascript
const MIInstanceSchema = z.object({
  caseId: z.string(),
  taskId: z.string(),
  instanceId: z.string(),
  status: z.enum(['pending', 'executing', 'completed', 'failed']),
  context: z.record(z.any()),
  startedAt: z.number(),
  completedAt: z.number().optional(),
  result: z.any().optional(),
  error: z.string().optional()
});

export class MIInstanceRegistry {
  constructor() {
    this.instances = new Map(); // key: `${caseId}:${taskId}:${instanceId}`
  }

  registerInstance(caseId, taskId, instanceId, context) {
    const key = `${caseId}:${taskId}:${instanceId}`;
    this.instances.set(key, MIInstanceSchema.parse({
      caseId, taskId, instanceId, context,
      status: 'pending',
      startedAt: Date.now()
    }));
  }

  getCompletedCount(caseId, taskId) {
    return this.getInstancesForTask(caseId, taskId)
      .filter(i => i.status === 'completed').length;
  }

  checkThresholdCompletion(caseId, taskId, threshold) {
    const instances = this.getInstancesForTask(caseId, taskId);
    const completed = instances.filter(i => i.status === 'completed').length;
    return completed >= threshold.minInstances &&
           (completed / instances.length) >= threshold.minSuccessRate;
  }
}
```

#### Recommendation 3: Build MI Coordinator

**Location**: Create `packages/daemon/src/integrations/mi-coordinator.mjs`

**Implementation**:
```javascript
export class MICoordinator extends EventEmitter {
  constructor(daemon, workerPool, registry) {
    super();
    this.daemon = daemon;
    this.workerPool = workerPool;
    this.registry = registry;
  }

  async spawnInstances(caseId, taskId, count, taskFactory, completionCondition) {
    const instances = [];

    // Register all instances
    for (let i = 0; i < count; i++) {
      const instanceId = `${taskId}-instance-${i}`;
      const context = { index: i, total: count };
      this.registry.registerInstance(caseId, taskId, instanceId, context);
      instances.push({ caseId, taskId, instanceId, context });
    }

    // Execute in parallel via worker pool
    const promises = instances.map(instance =>
      this.executeInstance(instance, taskFactory)
    );

    // Wait for completion condition
    if (completionCondition.type === 'threshold') {
      await this.waitForThreshold(caseId, taskId, completionCondition, promises);
    } else if (completionCondition.type === 'first-failure') {
      await this.waitForFirstFailure(caseId, taskId, promises);
    } else {
      await Promise.all(promises);
    }

    this.emit('mi:completed', { caseId, taskId });
  }

  async executeInstance(instance, taskFactory) {
    const { caseId, taskId, instanceId, context } = instance;

    this.registry.updateStatus(caseId, taskId, instanceId, 'executing');
    this.emit('mi:instance-started', { caseId, taskId, instanceId });

    try {
      const task = taskFactory(context);
      const result = await this.workerPool.execute(task);

      this.registry.updateStatus(caseId, taskId, instanceId, 'completed');
      this.registry.updateResult(caseId, taskId, instanceId, result);
      this.emit('mi:instance-completed', { caseId, taskId, instanceId, result });

      return result;
    } catch (error) {
      this.registry.updateStatus(caseId, taskId, instanceId, 'failed');
      this.registry.updateError(caseId, taskId, instanceId, error.message);
      this.emit('mi:instance-failed', { caseId, taskId, instanceId, error });
      throw error;
    }
  }

  async waitForThreshold(caseId, taskId, condition, promises) {
    return new Promise((resolve, reject) => {
      const checkCondition = () => {
        if (this.registry.checkThresholdCompletion(caseId, taskId, condition)) {
          resolve();
        }
      };

      // Check after each instance completes
      this.on('mi:instance-completed', checkCondition);
      this.on('mi:instance-failed', checkCondition);

      // Cleanup listeners after all complete
      Promise.allSettled(promises).then(() => {
        this.off('mi:instance-completed', checkCondition);
        this.off('mi:instance-failed', checkCondition);
        resolve();
      });
    });
  }
}
```

#### Recommendation 4: Extend YAWL Bridge for MI

**Location**: Modify `packages/daemon/src/integrations/yawl.mjs`

**New Methods**:
```javascript
export class YawlDaemonBridge extends EventEmitter {
  constructor(daemon, yawlEngine, config = {}) {
    super();
    // ... existing initialization ...

    // NEW: MI infrastructure
    this.workerPool = new WorkerPool(config.workerPoolSize || 10);
    this.miRegistry = new MIInstanceRegistry();
    this.miCoordinator = new MICoordinator(daemon, this.workerPool, this.miRegistry);
  }

  /**
   * Spawn MI task instances with parallel execution
   * @param {string} caseId - Case identifier
   * @param {string} taskId - MI task identifier
   * @param {number|Function} instanceCount - Fixed count or runtime evaluator
   * @param {Function} taskFactory - Creates task from instance context
   * @param {Object} completionCondition - Threshold/cancellation config
   */
  async executeMITask(caseId, taskId, instanceCount, taskFactory, completionCondition) {
    // Resolve instance count (a priori or runtime)
    const count = typeof instanceCount === 'function'
      ? await instanceCount(caseId, taskId)
      : instanceCount;

    // Spawn and coordinate instances
    await this.miCoordinator.spawnInstances(
      caseId, taskId, count, taskFactory, completionCondition
    );

    // Emit completion event for YAWL engine
    this.emit('mi:task-completed', { caseId, taskId });
  }

  /**
   * Get MI task instance status
   */
  getMIInstanceStatus(caseId, taskId, instanceId) {
    return this.miRegistry.getInstanceStatus(caseId, taskId, instanceId);
  }

  /**
   * Cancel MI task (all remaining instances)
   */
  async cancelMITask(caseId, taskId) {
    await this.miCoordinator.cancelRemaining(caseId, taskId);
  }
}
```

### 6.5 Implementation Priority

| Component | Priority | Complexity | Est. LoC | Dependencies |
|-----------|----------|------------|----------|--------------|
| WorkerPool | ğŸ”´ P0 | Medium | 200 | None |
| MIInstanceRegistry | ğŸ”´ P0 | Low | 150 | Zod schemas |
| MICoordinator | ğŸ”´ P0 | High | 300 | WorkerPool, Registry |
| YAWL Bridge MI Methods | ğŸŸ¡ P1 | Medium | 150 | Coordinator |
| Completion Condition Evaluator | ğŸŸ¡ P1 | Medium | 100 | Registry |
| Dynamic Instance Spawning | ğŸŸ¢ P2 | Low | 50 | Coordinator |
| MI Metrics Collection | ğŸŸ¢ P2 | Low | 80 | Registry |

**Total New Code**: ~1,030 LoC

**Estimated Effort**: 3-5 days (experienced developer)

---

## 7. Key Findings Summary

### 7.1 Strengths

1. **Comprehensive job scheduling** with trigger evaluation (cron, interval, idle, reactive, event)
2. **Robust distributed coordination** via Raft consensus with partition detection
3. **High-performance optimization** for 10K+ concurrent operations (batch scheduler, O(1) cache)
4. **Enterprise security** (BLAKE3 auth, injection detection, rate limiting)
5. **Extensive YAWL integration** for case creation, timeout, retry, deferred choice
6. **Production-grade testing** (18 E2E tests, 470K LoC coverage)

### 7.2 Critical Gaps for YAWL MI

1. âŒ **No worker pool** - Single-threaded execution model, cannot run MI instances in parallel
2. âŒ **No MI instance tracking** - Task-level state only, no per-instance status
3. âŒ **No MI coordinator** - Cannot spawn/manage/synchronize MI instances
4. âŒ **Sequential task distribution** - Existing parallel distribution uses loop, not concurrent execution
5. âŒ **No completion condition logic** - Cannot enforce threshold or cancellation-region semantics

### 7.3 Recommended Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YAWL Daemon Bridge (Enhanced)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ YawlDaemonBridge                                            â”‚
â”‚   â”œâ”€ daemon: Daemon (existing)                             â”‚
â”‚   â”œâ”€ yawlEngine: WorkflowEngine (existing)                 â”‚
â”‚   â”œâ”€ workerPool: WorkerPool (NEW)                          â”‚
â”‚   â”œâ”€ miRegistry: MIInstanceRegistry (NEW)                  â”‚
â”‚   â””â”€ miCoordinator: MICoordinator (NEW)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Methods                                                      â”‚
â”‚   â”œâ”€ executeMITask(caseId, taskId, count, factory, cond)  â”‚
â”‚   â”œâ”€ getMIInstanceStatus(caseId, taskId, instanceId)       â”‚
â”‚   â””â”€ cancelMITask(caseId, taskId)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WorkerPool (NEW)                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ workers: Worker[]                                           â”‚
â”‚ idleWorkers: Set<Worker>                                    â”‚
â”‚ busyWorkers: Map<workerId, task>                           â”‚
â”‚ taskQueue: Task[]                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ execute(task) -> Promise<result>                            â”‚
â”‚ getIdleWorkers() -> Worker[]                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MICoordinator (NEW)                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ spawnInstances(caseId, taskId, count, factory, condition)  â”‚
â”‚ waitForThreshold(caseId, taskId, condition)                â”‚
â”‚ cancelRemaining(caseId, taskId)                            â”‚
â”‚ addInstance(caseId, taskId, context) [dynamic spawn]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MIInstanceRegistry (NEW)                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ instances: Map<key, MIInstance>                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ registerInstance(caseId, taskId, instanceId, context)      â”‚
â”‚ updateStatus(caseId, taskId, instanceId, status)           â”‚
â”‚ getCompletedCount(caseId, taskId) -> number                â”‚
â”‚ checkThresholdCompletion(caseId, taskId, condition)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. Next Steps

### Phase 1: Core Infrastructure (P0)
1. Implement `WorkerPool` class with idle/busy tracking
2. Create `MIInstanceRegistry` with Zod schemas
3. Build `MICoordinator` with threshold-based completion
4. Write unit tests for each component (target: 100% coverage)

### Phase 2: YAWL Integration (P1)
1. Extend `YawlDaemonBridge` with MI methods
2. Modify `yawl-handlers.mjs` for parallel execution
3. Add MI schemas to `yawl-schemas.mjs`
4. Create E2E tests for MI patterns (without sync, a priori design-time, a priori runtime)

### Phase 3: Advanced Features (P2)
1. Implement dynamic instance spawning
2. Add cancellation region support (first-failure)
3. Build MI metrics collection (per-instance latency, throughput)
4. Create MI visualization/monitoring tools

### Phase 4: Optimization (P3)
1. Benchmark MI execution performance
2. Optimize worker pool scheduling
3. Add work-stealing between nodes for MI instances
4. Implement adaptive instance count based on load

---

## Appendix A: File Statistics

**Total Source Files**: 50
**Total Source LoC**: 9,447
**Total Test LoC**: ~470,000
**Test Coverage**: 100% pass rate

**Largest Files**:
1. `security-headers.mjs` (645 LoC)
2. `streaming.mjs` (513 LoC)
3. `hooks-policy.mjs` (499 LoC)
4. `nitro-tasks.mjs` (466 LoC)
5. `yawl.mjs` (437 LoC)

**Module Distribution**:
- Core: 15% (1,426 LoC)
- Integrations: 62% (5,889 LoC)
- Security: 19% (1,826 LoC)
- Utilities: 4% (306 LoC)

---

## Appendix B: Dependencies

**Production Dependencies**:
- `@unrdf/kgc-4d` - KGC 4D integration
- `cron-parser` - Cron expression parsing
- `hash-wasm` - BLAKE3 hashing (WASM)
- `zod` - Runtime validation

**Dev Dependencies**:
- `vitest` - Testing framework

**External Integrations** (interface contracts):
- `@unrdf/yawl` - YAWL workflow engine
- `@opentelemetry/api` - Observability
- Raft consensus provider (generic interface)
- Cluster membership manager (generic interface)

---

**Document Version**: 1.0
**Last Updated**: 2026-01-11
**Status**: COMPLETE - Ready for engineering review
