#!/usr/bin/env node
/**
 * @file monitoring-setup-test.mjs
 * @description Production validation of MONITORING.md - Tests ALL examples and instructions
 * @methodology Big Bang 80/20 - Evidence-based validation, no assumptions
 *
 * CRITICAL: This is NOT a trust exercise. Every claim must be proven.
 */

import { readFileSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';

const __dirname = dirname(fileURLToPath(import.meta.url));
const projectRoot = join(__dirname, '..');

/**
 * Test results tracker
 * @type {Array<{category: string, test: string, status: 'PASS'|'FAIL'|'SKIP', evidence: string, details?: string}>}
 */
const results = [];

/**
 * Add test result with evidence
 */
function recordTest(category, test, status, evidence, details = '') {
  results.push({ category, test, status, evidence, details });
  const icon = status === 'PASS' ? '‚úÖ' : status === 'FAIL' ? '‚ùå' : '‚ö†Ô∏è';
  console.log(`${icon} [${category}] ${test}`);
  if (details) console.log(`   ${details}`);
}

/**
 * SECTION 1: Grafana Dashboard Validation
 * Tests: JSON syntax, query validity, panel configuration
 */
async function validateGrafanaDashboard() {
  console.log('\n=== VALIDATING GRAFANA DASHBOARD ===\n');

  try {
    // 1. Read and parse dashboard JSON
    const dashboardPath = join(projectRoot, 'monitoring/dashboards/unrdf-overview.json');
    const rawContent = readFileSync(dashboardPath, 'utf-8');

    recordTest(
      'Dashboard',
      'JSON file exists',
      'PASS',
      `File found at: ${dashboardPath}`
    );

    // 2. Validate JSON syntax
    let dashboard;
    try {
      dashboard = JSON.parse(rawContent);
      recordTest(
        'Dashboard',
        'JSON syntax valid',
        'PASS',
        'JSON.parse() succeeded without errors'
      );
    } catch (error) {
      recordTest(
        'Dashboard',
        'JSON syntax valid',
        'FAIL',
        `Parse error: ${error.message}`,
        error.stack
      );
      return;
    }

    // 3. Validate dashboard structure
    const requiredFields = ['dashboard'];
    const missingFields = requiredFields.filter(f => !dashboard[f]);

    if (missingFields.length === 0) {
      recordTest(
        'Dashboard',
        'Required fields present',
        'PASS',
        `All required fields found: ${requiredFields.join(', ')}`
      );
    } else {
      recordTest(
        'Dashboard',
        'Required fields present',
        'FAIL',
        `Missing fields: ${missingFields.join(', ')}`
      );
    }

    // 4. Validate panels exist
    const panels = dashboard.dashboard?.panels || [];
    if (panels.length > 0) {
      recordTest(
        'Dashboard',
        'Panels configured',
        'PASS',
        `Found ${panels.length} panels`,
        `Panel IDs: ${panels.map(p => p.id).join(', ')}`
      );
    } else {
      recordTest(
        'Dashboard',
        'Panels configured',
        'FAIL',
        'No panels found in dashboard'
      );
    }

    // 5. Validate each panel has required fields
    let panelErrors = 0;
    const requiredPanelFields = ['id', 'title', 'type', 'targets'];

    panels.forEach((panel, idx) => {
      const missing = requiredPanelFields.filter(f => !panel[f]);
      if (missing.length > 0) {
        panelErrors++;
        recordTest(
          'Dashboard',
          `Panel ${panel.id || idx} structure`,
          'FAIL',
          `Missing fields: ${missing.join(', ')}`,
          `Panel title: ${panel.title || 'unknown'}`
        );
      }
    });

    if (panelErrors === 0) {
      recordTest(
        'Dashboard',
        'All panels valid structure',
        'PASS',
        `${panels.length} panels have all required fields`
      );
    }

    // 6. Validate Prometheus queries
    const queries = panels.flatMap(p =>
      (p.targets || []).map(t => ({
        panel: p.title,
        expr: t.expr,
        refId: t.refId
      }))
    );

    recordTest(
      'Dashboard',
      'Prometheus queries configured',
      'PASS',
      `Found ${queries.length} queries across ${panels.length} panels`,
      `Sample queries:\n${queries.slice(0, 3).map(q => `  - ${q.panel}: ${q.expr}`).join('\n')}`
    );

    // 7. Check for required metrics in queries
    const requiredMetrics = [
      'unrdf_service_info',
      'unrdf_otel_validation_score',
      'unrdf_requests_total',
      'unrdf_errors_total',
      'unrdf_request_duration_seconds_bucket',
      'unrdf_memory_heap_used_bytes'
    ];

    const foundMetrics = new Set();
    queries.forEach(q => {
      requiredMetrics.forEach(metric => {
        if (q.expr?.includes(metric)) {
          foundMetrics.add(metric);
        }
      });
    });

    const missingMetrics = requiredMetrics.filter(m => !foundMetrics.has(m));
    if (missingMetrics.length === 0) {
      recordTest(
        'Dashboard',
        'All required metrics referenced',
        'PASS',
        `All ${requiredMetrics.length} required metrics found in queries`
      );
    } else {
      recordTest(
        'Dashboard',
        'All required metrics referenced',
        'FAIL',
        `Missing metrics: ${missingMetrics.join(', ')}`
      );
    }

    // 8. Validate datasource configuration
    const templating = dashboard.dashboard?.templating?.list || [];
    const datasourceVar = templating.find(t => t.name === 'datasource');

    if (datasourceVar && datasourceVar.query === 'prometheus') {
      recordTest(
        'Dashboard',
        'Prometheus datasource configured',
        'PASS',
        'Datasource variable correctly set to prometheus'
      );
    } else {
      recordTest(
        'Dashboard',
        'Prometheus datasource configured',
        'FAIL',
        'Datasource variable not configured or wrong type'
      );
    }

    // 9. Validate annotations
    const annotations = dashboard.dashboard?.annotations?.list || [];
    const expectedAnnotations = ['Deployments', 'Alerts'];
    const foundAnnotations = annotations.map(a => a.name);

    const missingAnnotations = expectedAnnotations.filter(a => !foundAnnotations.includes(a));
    if (missingAnnotations.length === 0) {
      recordTest(
        'Dashboard',
        'Annotations configured',
        'PASS',
        `All expected annotations present: ${expectedAnnotations.join(', ')}`
      );
    } else {
      recordTest(
        'Dashboard',
        'Annotations configured',
        'FAIL',
        `Missing annotations: ${missingAnnotations.join(', ')}`
      );
    }

  } catch (error) {
    recordTest(
      'Dashboard',
      'Overall validation',
      'FAIL',
      `Unexpected error: ${error.message}`,
      error.stack
    );
  }
}

/**
 * SECTION 2: Health Check API Validation
 * Tests: API design, example code compilation
 */
async function validateHealthCheckAPI() {
  console.log('\n=== VALIDATING HEALTH CHECK API ===\n');

  // Check if module exists
  try {
    // Try to import the module (will fail if doesn't exist)
    await import('@unrdf/core/health').catch(() => {
      throw new Error('Module not found');
    });

    recordTest(
      'Health',
      '@unrdf/core/health module exists',
      'PASS',
      'Module can be imported'
    );
  } catch (error) {
    recordTest(
      'Health',
      '@unrdf/core/health module exists',
      'FAIL',
      'Module does not exist - cannot test examples',
      'MONITORING.md references non-existent module'
    );

    // Test API design with stub implementation
    try {
      // Simulate the API from MONITORING.md
      const mockHealth = {
        liveness: async () => ({ status: 'healthy', uptime: 12345 }),
        readiness: async () => ({ status: 'healthy', dependencies: {} }),
        metrics: async () => ({ requests: {}, memory: {}, cpu: {} }),
        prometheus: async () => '# Prometheus format'
      };

      // Test that API surface makes sense
      const liveness = await mockHealth.liveness();
      if (liveness.status && typeof liveness.uptime === 'number') {
        recordTest(
          'Health',
          'Liveness API design valid',
          'PASS',
          'API returns expected structure: { status, uptime }',
          'Tested with mock implementation'
        );
      }

      const readiness = await mockHealth.readiness();
      if (readiness.status && readiness.dependencies !== undefined) {
        recordTest(
          'Health',
          'Readiness API design valid',
          'PASS',
          'API returns expected structure: { status, dependencies }',
          'Tested with mock implementation'
        );
      }

      const metrics = await mockHealth.metrics();
      if (metrics.requests !== undefined && metrics.memory !== undefined) {
        recordTest(
          'Health',
          'Metrics API design valid',
          'PASS',
          'API returns expected structure: { requests, memory, cpu }',
          'Tested with mock implementation'
        );
      }

      const prometheus = await mockHealth.prometheus();
      if (typeof prometheus === 'string') {
        recordTest(
          'Health',
          'Prometheus export API design valid',
          'PASS',
          'API returns string (Prometheus format)',
          'Tested with mock implementation'
        );
      }

    } catch (error) {
      recordTest(
        'Health',
        'API design validation',
        'FAIL',
        `Mock implementation failed: ${error.message}`
      );
    }
  }
}

/**
 * SECTION 3: Logger API Validation
 * Tests: API design, performance timer, log levels
 */
async function validateLoggerAPI() {
  console.log('\n=== VALIDATING LOGGER API ===\n');

  try {
    await import('@unrdf/core/logger').catch(() => {
      throw new Error('Module not found');
    });

    recordTest(
      'Logger',
      '@unrdf/core/logger module exists',
      'PASS',
      'Module can be imported'
    );
  } catch (error) {
    recordTest(
      'Logger',
      '@unrdf/core/logger module exists',
      'FAIL',
      'Module does not exist - testing API design with stubs',
      'MONITORING.md references non-existent module'
    );

    // Test API design
    try {
      const mockLogger = {
        info: (msg, context) => ({ level: 'info', message: msg, ...context }),
        error: (msg, context, error) => ({ level: 'error', message: msg, ...context, error }),
        performance: (msg, timing) => ({ level: 'info', message: msg, duration: timing.duration }),
        slowQuery: (query, duration, threshold) => {
          if (duration > threshold) {
            return { level: 'warn', message: `Slow query: ${query}`, duration, threshold };
          }
        },
        child: (context) => ({ ...mockLogger, context })
      };

      const mockPerformanceTimer = () => {
        const start = performance.now();
        return {
          end: () => ({ duration: performance.now() - start })
        };
      };

      // Test info logging
      const infoLog = mockLogger.info('Test message', { userId: '123' });
      if (infoLog.level === 'info' && infoLog.message === 'Test message') {
        recordTest(
          'Logger',
          'info() method design valid',
          'PASS',
          'Method accepts (message, context) and returns structured log',
          'Tested with mock implementation'
        );
      }

      // Test error logging
      const errorLog = mockLogger.error('Error occurred', { query: 'SELECT *' }, new Error('Test'));
      if (errorLog.level === 'error' && errorLog.error) {
        recordTest(
          'Logger',
          'error() method design valid',
          'PASS',
          'Method accepts (message, context, error) and returns structured log',
          'Tested with mock implementation'
        );
      }

      // Test performance timer
      const timer = mockPerformanceTimer();
      await new Promise(resolve => setTimeout(resolve, 10));
      const timing = timer.end();

      if (timing.duration !== undefined && timing.duration >= 10) {
        recordTest(
          'Logger',
          'performanceTimer() works correctly',
          'PASS',
          `Timer measured ${timing.duration.toFixed(2)}ms (expected ‚â•10ms)`,
          'Tested with mock implementation'
        );
      } else {
        recordTest(
          'Logger',
          'performanceTimer() works correctly',
          'FAIL',
          `Timer returned invalid duration: ${timing.duration}`
        );
      }

      // Test performance logging
      const perfLog = mockLogger.performance('Query execution', timing);
      if (perfLog.duration !== undefined) {
        recordTest(
          'Logger',
          'performance() method design valid',
          'PASS',
          'Method accepts (message, timing) and includes duration',
          'Tested with mock implementation'
        );
      }

      // Test slow query detection
      const slowLog = mockLogger.slowQuery('complex-query', 150, 100);
      if (slowLog && slowLog.level === 'warn' && slowLog.duration === 150) {
        recordTest(
          'Logger',
          'slowQuery() detection works',
          'PASS',
          'Detects queries exceeding threshold (150ms > 100ms)',
          'Tested with mock implementation'
        );
      }

      // Test child logger
      const childLogger = mockLogger.child({ requestId: '456' });
      if (childLogger.context && childLogger.context.requestId === '456') {
        recordTest(
          'Logger',
          'child() method design valid',
          'PASS',
          'Method creates child logger with inherited context',
          'Tested with mock implementation'
        );
      }

    } catch (error) {
      recordTest(
        'Logger',
        'API design validation',
        'FAIL',
        `Mock implementation failed: ${error.message}`,
        error.stack
      );
    }
  }
}

/**
 * SECTION 4: Metrics API Validation
 * Tests: Counter, gauge, histogram, Prometheus export
 */
async function validateMetricsAPI() {
  console.log('\n=== VALIDATING METRICS API ===\n');

  try {
    await import('@unrdf/core/metrics').catch(() => {
      throw new Error('Module not found');
    });

    recordTest(
      'Metrics',
      '@unrdf/core/metrics module exists',
      'PASS',
      'Module can be imported'
    );
  } catch (error) {
    recordTest(
      'Metrics',
      '@unrdf/core/metrics module exists',
      'FAIL',
      'Module does not exist - testing API design with stubs',
      'MONITORING.md references non-existent module'
    );

    // Test API design
    try {
      const mockMetrics = {
        counters: new Map(),
        gauges: new Map(),
        histograms: new Map(),

        incrementCounter: (name, labels = {}) => {
          const key = `${name}${JSON.stringify(labels)}`;
          const current = mockMetrics.counters.get(key) || 0;
          mockMetrics.counters.set(key, current + 1);
          return current + 1;
        },

        recordGauge: (name, value, labels = {}) => {
          const key = `${name}${JSON.stringify(labels)}`;
          mockMetrics.gauges.set(key, value);
          return value;
        },

        startTimer: () => {
          const start = performance.now();
          return { start, elapsed: () => performance.now() - start };
        },

        recordDuration: (name, timer, labels = {}) => {
          const duration = timer.elapsed();
          const key = `${name}${JSON.stringify(labels)}`;
          if (!mockMetrics.histograms.has(key)) {
            mockMetrics.histograms.set(key, []);
          }
          mockMetrics.histograms.get(key).push(duration);
          return duration;
        },

        recordSummary: (name, value, labels = {}) => {
          const key = `${name}${JSON.stringify(labels)}`;
          if (!mockMetrics.histograms.has(key)) {
            mockMetrics.histograms.set(key, []);
          }
          mockMetrics.histograms.get(key).push(value);
          return value;
        },

        toPrometheus: () => {
          let output = '';
          mockMetrics.counters.forEach((value, key) => {
            output += `# COUNTER ${key} ${value}\n`;
          });
          mockMetrics.gauges.forEach((value, key) => {
            output += `# GAUGE ${key} ${value}\n`;
          });
          return output;
        },

        toJSON: () => ({
          counters: Object.fromEntries(mockMetrics.counters),
          gauges: Object.fromEntries(mockMetrics.gauges),
          histograms: Object.fromEntries(mockMetrics.histograms)
        })
      };

      // Test counter increment
      const count1 = mockMetrics.incrementCounter('requests_total', { method: 'GET', status: 200 });
      const count2 = mockMetrics.incrementCounter('requests_total', { method: 'GET', status: 200 });

      if (count2 === count1 + 1) {
        recordTest(
          'Metrics',
          'incrementCounter() works correctly',
          'PASS',
          `Counter incremented: ${count1} ‚Üí ${count2}`,
          'Tested with mock implementation'
        );
      } else {
        recordTest(
          'Metrics',
          'incrementCounter() works correctly',
          'FAIL',
          `Counter did not increment correctly: ${count1} ‚Üí ${count2}`
        );
      }

      // Test gauge recording
      const gaugeValue = mockMetrics.recordGauge('active_connections', 42);
      if (gaugeValue === 42) {
        recordTest(
          'Metrics',
          'recordGauge() works correctly',
          'PASS',
          'Gauge set to value: 42',
          'Tested with mock implementation'
        );
      }

      // Test timer and duration recording
      const timer = mockMetrics.startTimer();
      await new Promise(resolve => setTimeout(resolve, 10));
      const duration = mockMetrics.recordDuration('request_duration', timer, { endpoint: '/api/query' });

      if (duration >= 10) {
        recordTest(
          'Metrics',
          'startTimer() and recordDuration() work correctly',
          'PASS',
          `Duration recorded: ${duration.toFixed(2)}ms (expected ‚â•10ms)`,
          'Tested with mock implementation'
        );
      } else {
        recordTest(
          'Metrics',
          'startTimer() and recordDuration() work correctly',
          'FAIL',
          `Duration too short: ${duration}ms`
        );
      }

      // Test summary recording
      mockMetrics.recordSummary('query_duration', 0.042);
      mockMetrics.recordSummary('query_duration', 0.058);
      mockMetrics.recordSummary('query_duration', 0.031);

      const summaryData = mockMetrics.histograms.get('query_duration{}');
      if (summaryData && summaryData.length === 3) {
        recordTest(
          'Metrics',
          'recordSummary() works correctly',
          'PASS',
          `Recorded 3 values: ${summaryData.join(', ')}`,
          'Tested with mock implementation'
        );
      }

      // Test Prometheus export
      const prometheusFormat = mockMetrics.toPrometheus();
      if (typeof prometheusFormat === 'string' && prometheusFormat.includes('COUNTER')) {
        recordTest(
          'Metrics',
          'toPrometheus() export format valid',
          'PASS',
          'Returns string with metric data',
          `Sample output:\n${prometheusFormat.split('\n').slice(0, 3).join('\n')}`
        );
      } else {
        recordTest(
          'Metrics',
          'toPrometheus() export format valid',
          'FAIL',
          'Output format invalid or empty'
        );
      }

      // Test JSON export
      const jsonFormat = mockMetrics.toJSON();
      if (jsonFormat.counters && jsonFormat.gauges && jsonFormat.histograms) {
        recordTest(
          'Metrics',
          'toJSON() export format valid',
          'PASS',
          'Returns object with counters, gauges, histograms',
          'Tested with mock implementation'
        );
      } else {
        recordTest(
          'Metrics',
          'toJSON() export format valid',
          'FAIL',
          'JSON structure incomplete'
        );
      }

    } catch (error) {
      recordTest(
        'Metrics',
        'API design validation',
        'FAIL',
        `Mock implementation failed: ${error.message}`,
        error.stack
      );
    }
  }
}

/**
 * SECTION 5: OTEL Validation Test
 * Tests: Can run OTEL validation as documented
 */
async function validateOTELSetup() {
  console.log('\n=== VALIDATING OTEL SETUP ===\n');

  try {
    const runAllPath = join(projectRoot, 'validation/run-all.mjs');
    const runAllExists = readFileSync(runAllPath, 'utf-8');

    recordTest(
      'OTEL',
      'validation/run-all.mjs exists',
      'PASS',
      `File found at: ${runAllPath}`
    );

    // Check if it's executable
    // Note: Can't check actual execution without running, which might timeout
    recordTest(
      'OTEL',
      'OTEL validation command documented',
      'PASS',
      'Command: timeout 5s node validation/run-all.mjs comprehensive',
      'Cannot execute in validation test (would require full OTEL setup)'
    );

  } catch (error) {
    recordTest(
      'OTEL',
      'OTEL validation setup',
      'FAIL',
      `validation/run-all.mjs not found: ${error.message}`
    );
  }
}

/**
 * SECTION 6: Express.js Integration Example Validation
 * Tests: Code compiles, imports valid
 */
async function validateExpressIntegration() {
  console.log('\n=== VALIDATING EXPRESS.JS INTEGRATION EXAMPLE ===\n');

  // The example code from MONITORING.md
  const exampleCode = `
import express from 'express';
import { createHealthMiddleware } from '@unrdf/core/health';
import { requestLogger } from '@unrdf/core/logger';
import { metrics } from '@unrdf/core/metrics';

const app = express();
app.use(requestLogger({ logBody: false }));

const health = createHealthMiddleware({
  serviceName: 'unrdf-api',
  version: '5.0.1',
  dependencies: {
    database: async () => ({ status: 'connected' })
  }
});

app.get('/health', health.liveness);
app.get('/health/ready', health.readiness);
app.get('/health/metrics', health.metrics);
app.get('/metrics', health.prometheus);

// Test that example would work
console.log('Express integration example syntax valid');
`;

  try {
    // Check if imports would be available
    const hasExpress = await import('express').then(() => true).catch(() => false);

    if (hasExpress) {
      recordTest(
        'Integration',
        'Express.js dependency available',
        'PASS',
        'express package can be imported'
      );
    } else {
      recordTest(
        'Integration',
        'Express.js dependency available',
        'FAIL',
        'express package not installed (example would fail)'
      );
    }

    // Note about missing modules
    recordTest(
      'Integration',
      'Express.js integration example completeness',
      'FAIL',
      'Example references non-existent modules',
      'Cannot test until @unrdf/core/health, /logger, /metrics are implemented'
    );

  } catch (error) {
    recordTest(
      'Integration',
      'Express.js integration validation',
      'FAIL',
      `Error: ${error.message}`
    );
  }
}

/**
 * SECTION 7: Documentation Completeness Check
 * Tests: All referenced files exist
 */
async function validateDocumentationReferences() {
  console.log('\n=== VALIDATING DOCUMENTATION REFERENCES ===\n');

  const referencedFiles = [
    'monitoring/alerts.yml',
    'monitoring/RUNBOOK.md',
    'monitoring/dashboards/unrdf-overview.json',
    'validation/otel-provider.mjs',
    'docs/bb80-20-methodology.md'
  ];

  referencedFiles.forEach(file => {
    const fullPath = join(projectRoot, file);
    try {
      readFileSync(fullPath, 'utf-8');
      recordTest(
        'Documentation',
        `Referenced file: ${file}`,
        'PASS',
        `File exists at: ${fullPath}`
      );
    } catch (error) {
      recordTest(
        'Documentation',
        `Referenced file: ${file}`,
        'FAIL',
        `File not found: ${fullPath}`,
        'MONITORING.md references non-existent file'
      );
    }
  });
}

/**
 * Generate summary report
 */
function generateSummary() {
  console.log('\n' + '='.repeat(80));
  console.log('MONITORING VALIDATION SUMMARY');
  console.log('='.repeat(80) + '\n');

  const byCategory = {};
  results.forEach(r => {
    if (!byCategory[r.category]) {
      byCategory[r.category] = { PASS: 0, FAIL: 0, SKIP: 0 };
    }
    byCategory[r.category][r.status]++;
  });

  console.log('Results by Category:');
  console.log('-'.repeat(80));
  Object.entries(byCategory).forEach(([category, counts]) => {
    const total = counts.PASS + counts.FAIL + counts.SKIP;
    const passRate = ((counts.PASS / total) * 100).toFixed(1);
    console.log(`${category.padEnd(20)} | PASS: ${counts.PASS.toString().padStart(2)} | FAIL: ${counts.FAIL.toString().padStart(2)} | SKIP: ${counts.SKIP.toString().padStart(2)} | Rate: ${passRate}%`);
  });

  const totals = {
    PASS: results.filter(r => r.status === 'PASS').length,
    FAIL: results.filter(r => r.status === 'FAIL').length,
    SKIP: results.filter(r => r.status === 'SKIP').length
  };

  const totalTests = totals.PASS + totals.FAIL + totals.SKIP;
  const overallPassRate = ((totals.PASS / totalTests) * 100).toFixed(1);

  console.log('-'.repeat(80));
  console.log(`${'TOTAL'.padEnd(20)} | PASS: ${totals.PASS.toString().padStart(2)} | FAIL: ${totals.FAIL.toString().padStart(2)} | SKIP: ${totals.SKIP.toString().padStart(2)} | Rate: ${overallPassRate}%`);
  console.log('='.repeat(80) + '\n');

  // Critical failures
  const criticalFailures = results.filter(r =>
    r.status === 'FAIL' &&
    (r.test.includes('module exists') || r.test.includes('JSON syntax'))
  );

  if (criticalFailures.length > 0) {
    console.log('\nüö® CRITICAL FAILURES (BLOCKER):');
    criticalFailures.forEach(f => {
      console.log(`   ‚ùå [${f.category}] ${f.test}`);
      console.log(`      ${f.evidence}`);
    });
    console.log('');
  }

  return {
    results,
    summary: {
      total: totalTests,
      passed: totals.PASS,
      failed: totals.FAIL,
      skipped: totals.SKIP,
      passRate: parseFloat(overallPassRate),
      criticalFailures: criticalFailures.length
    }
  };
}

/**
 * Main execution
 */
async function main() {
  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë           MONITORING.md PRODUCTION VALIDATION TEST                         ‚ïë');
  console.log('‚ïë                                                                            ‚ïë');
  console.log('‚ïë  Methodology: Big Bang 80/20 - Evidence-Based Validation                  ‚ïë');
  console.log('‚ïë  Principle: Trust nothing, verify everything                              ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  const startTime = performance.now();

  // Run all validation sections
  await validateGrafanaDashboard();
  await validateHealthCheckAPI();
  await validateLoggerAPI();
  await validateMetricsAPI();
  await validateOTELSetup();
  await validateExpressIntegration();
  await validateDocumentationReferences();

  // Generate summary
  const report = generateSummary();

  const duration = ((performance.now() - startTime) / 1000).toFixed(2);
  console.log(`\nValidation completed in ${duration}s\n`);

  // Exit with appropriate code
  if (report.summary.criticalFailures > 0 || report.summary.failed > report.summary.passed) {
    console.log('‚ùå VALIDATION FAILED - See critical failures above\n');
    process.exit(1);
  } else if (report.summary.failed > 0) {
    console.log('‚ö†Ô∏è  VALIDATION PASSED WITH WARNINGS - Some tests failed\n');
    process.exit(0);
  } else {
    console.log('‚úÖ VALIDATION PASSED - All tests successful\n');
    process.exit(0);
  }
}

// Run if executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  main().catch(error => {
    console.error('\n‚ùå FATAL ERROR:', error.message);
    console.error(error.stack);
    process.exit(1);
  });
}

export { validateGrafanaDashboard, validateHealthCheckAPI, validateLoggerAPI, validateMetricsAPI };
