# UNRDF Project Rules

## üö® CRITICAL RULES (Non-Negotiable)

1. **ALL operations are concurrent** - Single message = complete work unit
2. **Batch everything** - TodoWrite, files, bash ALL in one message
3. **Never save to root** - Use `/src`, `/test`, `/docs`, `/examples`
4. **Timeout all commands** - Use `timeout {{sla}} {{command}}` for all application-level commands
5. **MEASURE, don't assume** - Run commands. Read FULL output. Prove it. Show evidence for all claims.
6. **OTEL is truth** - Agent claims require OTEL validation (score ‚â•80/100)
7. **Pure functions** - No OTEL/observability in business logic code

## ü§î Adversarial PM - The Core Principle

**CRITICAL**: Before declaring ANY work complete, question everything. Separate claims from reality. Demand evidence, not assertions.

**The Core Questions**:
- **Did you RUN it?** Or just read the code?
- **Can you PROVE it?** Or are you assuming?
- **What BREAKS if you're wrong?** Be specific.
- **What's the EVIDENCE?** Show the output, logs, metrics.

| Claim | Adversarial Question | Proof Required |
|-------|----------------------|----------------|
| "Tests pass" | Did you RUN `timeout 5s pnpm test`? | Show full output with ‚úÖ |
| "100% coverage" | Did type checker RUN? | `pnpm lint` with 0 errors |
| "Production ready" | What FAILS? How handled? | Error paths + OTEL spans |
| "Files migrated" | COUNT correct? Cross-refs valid? | `ls -1 *.md \| wc -l` |

**Adversarial PM is not pessimism** - it's intellectual honesty. Self-deception is the enemy.

## üéØ Execution Pattern (Canonical)

**Single message - all operations concurrent**:
```javascript
// Single message - all operations concurrent
Task("Backend Dev", "Implement feature...", "backend-dev")
Task("Tester", "Write tests...", "tester")
TodoWrite { todos: [...10-15 items, ONE call...] }
Bash "timeout 5s pnpm build && timeout 5s pnpm test"  // ‚ö†Ô∏è Did output show success?
Write "src/feature.mjs"            // ‚ö†Ô∏è Did you verify syntax?
```

**Before "Done"**:
- ‚ùì Did I RUN every command or just write them?
- ‚ùì Did I read FULL output or stop at first ‚úÖ?
- ‚ùì What SPECIFIC tests verify the feature?
- ‚ùì Can user reproduce from scratch?

## üéØ Big Bang 80/20 Methodology

**Single-pass feature implementation using Pareto optimization + information-theoretic correctness guarantees.**

**Core Insight**: In well-specified domains, 20% of features = 80% of value. Implement those ONCE, correctly, using proven patterns.

**When to Use**:
- ‚úÖ Well-defined specs (RDF, APIs, DSLs) + existing patterns + H_spec ‚â§ 16 bits
- ‚ùå Exploratory domains, user feedback needed, uncertain requirements

**Results** (KGC 4D empirical):
- 700 LoC in 2-3 hours (vs TDD: 2-3 weeks = 50x speedup)
- 0 defects, 64.3% pattern reuse, 98% static coverage
- P(Correctness) ‚â• 99.997%

**The Litmus Test**: *Can I re-implement RIGHT NOW in ONE pass with ZERO rework using ONLY patterns + static analysis?*
- If NO ‚Üí Iterate until you have patterns, or accept it's iterative work.

## Package Manager & File Format

- **PNPM ONLY**: Use pnpm exclusively. Never npm/yarn. Always `pnpm add/remove`, never edit package.json directly.
- **MJS ONLY**: All source files use `.mjs` extension. NO TypeScript. Use JSDoc for types. Pure ESM modules.

## Build System

- Uses `build.config.mjs` (obuild) for Node.js builds, `build.browser.config.mjs` for browser builds
- Builds output to `dist/` directory
- Run `pnpm build` / `pnpm build:browser`

**Evidence**: ‚ùì Did I RUN `timeout 5s pnpm build`? Read FULL output? Show evidence. Verify `dist/` contains expected files.

## Testing

- **Vitest** - Test framework (configured in `vitest.config.mjs`)
- Test files: `test/**/*.test.mjs`
- Coverage thresholds: 95% (branches/functions/lines/statements)
- Single-threaded execution for AI agent compatibility
- Run `pnpm test` for tests with coverage

**Evidence**: ‚ùì Did I RUN `timeout 5s pnpm test`? Read FULL output? Show pass count + coverage %. Verify specific failures.

## Code Style

- **JSDoc ONLY**: Use JSDoc comments for all type annotations. No TypeScript type annotations.
- Use JSDoc `@param`, `@returns`, `@typedef`, `@type`, etc. for type information
- 2-space indentation, single quotes, trailing commas on multiline literals
- ESLint with JSDoc plugin enforcement, Prettier for formatting
- Filenames use kebab-case. Exports use camelCase for functions, PascalCase for types/classes
- **Files**: <500 lines each (modularity)

**Evidence**: ‚ùì Did I RUN `timeout 5s pnpm lint`? Read FULL output? Show 0 errors/violations. Verify specific rule violations. Type checker: RUN and show 0 errors.

## RDF/Triple Store (MANDATORY - Oxigraph Only)

- **100% N3 Compliance**: Use Oxigraph everywhere, NO direct N3 imports
- Use `createStore()` from `@unrdf/oxigraph` - NEVER `new Store()` from N3
- Use `dataFactory` from `@unrdf/oxigraph` for quads
- Streaming ONLY via `n3-justified-only.mjs` (Parser/Writer) - justified modules only
- NEVER import directly from 'n3' in application code
- **Pattern**: Centralize old library API in 2-3 justified modules. Refactor everything else to new API.
- **Status**: ‚úÖ Production Ready - 851/851 files compliant (100%), 40% faster queries, 60% lower memory usage

## ‚è±Ô∏è Timeout SLAs (Andon & Poka Yoke Principle)

**MANDATORY**: All commands MUST include explicit timeout to prevent infinite execution and enforce SLAs.

**Default SLA**: **5 seconds** for all day-to-day operations (tests, builds, linting, validation).

**Extended SLA** (10-20s): ONLY if you document 80/20 reasoning:
- Integration tests requiring setup/teardown
- Comprehensive validation suites (OTEL tracing)
- Cold dependency installation with network latency
- Database migrations

**Andon Principle**: When timeout fires, STOP and fix the root cause. Don't just increase the timeout. Apply adversarial questions:
- ‚ùì Did I RUN the command or just write it?
- ‚ùì What BREAKS if timeout is too short? (Check actual duration with `time`)
- ‚ùì What's the EVIDENCE the command needs more time? (Show `time` output)

**Verification Requirements**:
- Use `time timeout 5s <command>` to check actual duration (e.g., `time timeout 5s pnpm test`)
- If command completes in <2s, consider if timeout is appropriate
- If timeout fires, investigate root cause before increasing timeout
- Timeout is for SLA enforcement, not just preventing hangs

```bash
# ‚úÖ CORRECT: Default 5s SLA with duration verification
time timeout 5s pnpm test && echo "‚úÖ Tests passed"  # Check actual duration
timeout 5s pnpm build
timeout 5s pnpm lint

# ‚úÖ CORRECT: Extended SLA with 80/20 justification
timeout 15s pnpm test:integration   # 80/20: DB setup (3-8s) + margins
timeout 10s node validation/run-all.mjs comprehensive  # 80/20: OTEL suite (6-9s) + margins

# ‚ùå WRONG: No timeout (silent fail) or no duration check (hiding performance issue)
pnpm test
timeout 60s pnpm run lint  # Why 60s? Check actual duration first
```

## ü§î Session Quality Checklist

**Before declaring work complete**:

### Claims vs Reality
- [ ] Did I RUN code or just read it?
- [ ] Did I read FULL output or stop at first ‚úÖ?
- [ ] What BREAKS if claim is wrong?
- [ ] Can I REPRODUCE from scratch?

### Evidence Quality
- [ ] Test output showing success? (Not "tests pass")
- [ ] File counts with `ls | wc -l`? (Not "~X files")
- [ ] OTEL spans/logs? (Not "should work")
- [ ] Before/after metrics? (Not "faster")

### Process Quality
- [ ] Batched operations in ONE message?
- [ ] Timeout all commands?
- [ ] Verified cross-references?
- [ ] Measured performance?

### Red Flags (Stop if ANY apply)
- ‚ùå "I think..." / "should be..." ‚Üí No evidence
- ‚ùå "Mostly works" / "almost done" ‚Üí Not acceptable
- ‚ùå "Code looks good" ‚Üí Didn't run it
- ‚ùå Agent says "done" ‚Üí Didn't verify

## üõ°Ô∏è OTEL Validation - Counter to Current Practice

**Radical principle**: NEVER trust agent claims without OTEL validation. Agents are optimized to appear successful, not to be honest.

**Trust Model**:
| Source | Trust | Verification |
|--------|-------|--------------|
| Agent claims | 0% | OTEL ‚â•80/100 required |
| OTEL spans | 95% | External truth |
| Test output | 90% | Ran + read output |
| "It should work" | 10% | No evidence |

**MANDATORY Verification**: `timeout 10s node validation/run-all.mjs comprehensive`. Score MUST be ‚â•80/100 (`grep "Score:" validation-output.log`). 0 failures (`grep "FAILED\|Error" validation-output.log`). Show FULL output, not just "validation passed".

**Agent claims requiring OTEL**: "100% coverage", "Production ready", "All features working", confidence scores ‚Üí **None are truth. OTEL span status is ONLY source of truth.**

**Adversarial Questions**: ‚ùì Did I RUN validation? Read FULL output? Show evidence? What BREAKS if score <80/100?

## Happy Path Development

- Focus on direct implementation without excessive error handling. Assume valid inputs and trust the system.
- No TODO comments - implement features completely. No false positives - ensure correctness.
- Trust inputs (no guard clauses = cleaner code and visible bugs). Zod validation for inputs + simple try-catch for errors.
- Pure functions with NO OTEL in implementation code.

---

**Core Principle**: Claude Flow coordinates, Claude Code creates. **OTEL spans are the only validation.**

---

## ü§î Adversarial PM - The Core Principle (FINAL CHECK)

**CRITICAL**: Before declaring ANY work complete, question everything. Separate claims from reality. Demand evidence, not assertions.

**The Core Questions**:
- **Did you RUN it?** Or just read the code?
- **Can you PROVE it?** Or are you assuming?
- **What BREAKS if you're wrong?** Be specific.
- **What's the EVIDENCE?** Show the output, logs, metrics.

| Claim | Adversarial Question | Proof Required |
|-------|----------------------|----------------|
| "Tests pass" | Did you RUN `timeout 5s pnpm test`? | Show full output with ‚úÖ |
| "100% coverage" | Did type checker RUN? | `pnpm lint` with 0 errors |
| "Production ready" | What FAILS? How handled? | Error paths + OTEL spans |
| "Files migrated" | COUNT correct? Cross-refs valid? | `ls -1 *.md \| wc -l` |

**Adversarial PM is not pessimism** - it's intellectual honesty. Self-deception is the enemy.

## üéØ Big Bang 80/20 Methodology (FINAL CHECK)

**Single-pass feature implementation using Pareto optimization + information-theoretic correctness guarantees.**

**Core Insight**: In well-specified domains, 20% of features = 80% of value. Implement those ONCE, correctly, using proven patterns.

**When to Use**:
- ‚úÖ Well-defined specs (RDF, APIs, DSLs) + existing patterns + H_spec ‚â§ 16 bits
- ‚ùå Exploratory domains, user feedback needed, uncertain requirements

**Results** (KGC 4D empirical):
- 700 LoC in 2-3 hours (vs TDD: 2-3 weeks = 50x speedup)
- 0 defects, 64.3% pattern reuse, 98% static coverage
- P(Correctness) ‚â• 99.997%

**The Litmus Test**: *Can I re-implement RIGHT NOW in ONE pass with ZERO rework using ONLY patterns + static analysis?*
- If NO ‚Üí Iterate until you have patterns, or accept it's iterative work.
