# Claude Code Configuration - UNRDF v6.0.0

> **UNRDF**: RDF Knowledge Graph Substrate Platform
> **Version**: 6.0.0-rc.1
> **Packages**: 67 packages in pnpm monorepo
> **Language**: JavaScript ESM (.mjs) + JSDoc + Zod
> **Status**: Research prototype - architecturally complete, not production-validated

---

## ğŸ¤” Adversarial PM - The Core Principle

**CRITICAL**: Before declaring ANY work complete, question everything. Separate claims from reality. Demand evidence, not assertions.

**The Core Questions**:
- **Did you RUN it?** Or just read the code?
- **Can you PROVE it?** Or are you assuming?
- **What BREAKS if you're wrong?** Be specific.
- **What's the EVIDENCE?** Show the output, logs, metrics.

| Claim | Adversarial Question | Proof Required |
|-------|----------------------|----------------|
| "Tests pass" | Did you RUN `timeout 5s npm test`? | Show full output with pass count |
| "100% coverage" | Did type checker RUN? | `npm run lint` with 0 errors |
| "Production ready" | What FAILS? How handled? | Error paths + OTEL spans |
| "Files migrated" | COUNT correct? Cross-refs valid? | `ls -1 *.md \| wc -l` |

**Adversarial PM is not pessimism** - it's intellectual honesty. Self-deception is the enemy.

---

## ğŸ—ï¸ Architecture Overview

### 5-Layer Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 5: APPLICATION                                        â”‚
â”‚   CLI (@unrdf/cli), APIs, React/Vue integrations           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 4: KNOWLEDGE SUBSTRATE                                â”‚
â”‚   Hooks, Federation, Streaming, Knowledge Engine            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 3: KGC (Knowledge Graph Governance)                   â”‚
â”‚   Temporal event sourcing, Receipts, Cryptographic proofs   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 2: RDF CORE                                           â”‚
â”‚   SPARQL, SHACL validation, Parsers (pure functions)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 1: INFRASTRUCTURE                                     â”‚
â”‚   Oxigraph (Rust/WASM), Raft consensus, OpenTelemetry       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Package Tiers (67 Total)

**Essential Tier** (7 packages - always needed):
- `@unrdf/core` - RDF Graph Operations, SPARQL, Foundational Substrate
- `@unrdf/oxigraph` - Oxigraph SPARQL engine binding (10-100x faster than N3)
- `@unrdf/kgc-4d` - KGC 4D Datum & Universe Freeze Engine
- `@unrdf/yawl` - Core YAWL workflow engine
- `@unrdf/hooks` - Policy Definition and Execution Framework
- `@unrdf/streaming` - Change Feeds and Real-time Synchronization
- `@unrdf/v6-core` - Î”Gate control plane, unified receipts, delta contracts

**Extended Tier** (8 packages - common use cases):
- `@unrdf/federation` - Distributed RDF Query with RAFT Consensus
- `@unrdf/knowledge-engine` - Rule Engine, Inference, Pattern Matching
- `@unrdf/cli` - Command-line Tools for Graph Operations
- `@unrdf/kgc-runtime` - KGC governance runtime with Zod schemas
- `@unrdf/kgc-substrate` - Deterministic KnowledgeStore
- `@unrdf/receipts` - Batch receipt generation with Merkle trees
- `@unrdf/consensus` - Production-grade Raft consensus
- `@unrdf/v6-compat` - V5 to V6 migration bridge

**Optional Tier** (30+ packages - specialized use cases):
- `@unrdf/yawl-*` - YAWL workflow extensions (8 packages: ai, api, durable, kafka, langchain, observability, queue, realtime, viz)
- `@unrdf/kgc-*` - KGC governance tools (9 packages: claude, cli, docs, multiverse, probe, swarm, tools)
- `@unrdf/ml-*` - Machine learning (2 packages: inference, versioning)
- `@unrdf/atomvm` - Erlang VM integration
- `@unrdf/blockchain` - Blockchain integration
- `@unrdf/zkp` - Zero-knowledge proofs
- `@unrdf/geosparql` - Geospatial queries
- `@unrdf/semantic-search` - Semantic search
- `@unrdf/graph-analytics` - Graph algorithms
- `@unrdf/privacy` - Privacy-preserving operations
- `@unrdf/serverless` - Serverless deployment
- Additional packages: ai-ml-innovations, caching, codegen, collab, composables, daemon, dark-matter, decision-fabric, diataxis-kit, domain, engine-gateway, event-automation, fusion, nextra, observability, project-engine, rdf-graphql, react, self-healing-workflows, spatial-kg, temporal-discovery

**Internal Tier** (8 packages):
- `@unrdf/docs` - Documentation site
- `@unrdf/kgn` - Knowledge graph notebook
- `@unrdf/test-utils` - Testing utilities
- `@unrdf/validation` - Validation tools
- `@unrdf/integration-tests` - Integration test suite
- `@unrdf/diataxis-kit` - Documentation framework

---

## ğŸ¯ Execution Pattern (Canonical)

```javascript
// Single message - all operations concurrent
Task("Backend Dev", "Implement feature...", "backend-dev")
Task("Tester", "Write tests...", "tester")
TodoWrite { todos: [...10-15 items, ONE call...] }
Bash "timeout 5s npm run build && timeout 5s npm test"
Write "src/feature.mjs"
```

**Before "Done"**:
- â“ Did I RUN every command or just write them?
- â“ Did I read output or assume success?
- â“ What SPECIFIC tests verify the feature?
- â“ Can user reproduce from scratch?

---

## ğŸ¯ Big Bang 80/20 Methodology

**Single-pass feature implementation using Pareto optimization + information-theoretic correctness guarantees.**

**Core Insight**: In well-specified domains, 20% of features = 80% of value. Implement those ONCE, correctly, using proven patterns.

**When to Use**:
- âœ… Well-defined specs (RDF, APIs, DSLs) + existing patterns + H_spec â‰¤ 16 bits
- âŒ Exploratory domains, user feedback needed, uncertain requirements

**Results** (Git-verified empirical):
- KGC-4D: 6,327 LoC developed over 20+ days
- 99.8% test pass rate (443/444), OTEL validation 100/100
- Pattern reuse ~64%, 98% static coverage
- P(Correctness): 99.8% measured, 99.997% theoretical bound

**The Litmus Test**: *Can I re-implement RIGHT NOW in ONE pass with ZERO rework using ONLY patterns + static analysis?*

---

## ğŸš¨ CRITICAL RULES

1. **ALL operations concurrent** - Single message = complete work unit
2. **Batch everything** - TodoWrite, files, bash ALL in one message
3. **Timeout all commands** - `timeout 5s npm test` (default), 10-20s only if justified
4. **MEASURE, don't assume** - Run commands. Read output. Prove it.
5. **Pattern reuse** - Copy exactly, don't improve
6. **OTEL is truth** - Agent claims require validation â‰¥80/100

---

## â±ï¸ Timeout SLAs (Andon & Poka Yoke)

**Default**: **5 seconds** for all operations. If exceeded â†’ investigate root cause.

```bash
# âœ… CORRECT
timeout 5s npm test && echo "Tests passed"
timeout 5s pnpm -r test:fast

# âœ… Extended (MUST justify)
timeout 15s npm run test:integration  # DB setup 3-8s + margin
timeout 60s pnpm install              # Initial install

# âŒ WRONG
npm test                  # No timeout = silent hang risk
timeout 60s npm run lint  # Why 60s? Hiding performance issue?
```

**Andon Principle**: When timeout fires, STOP and fix root cause. Don't just increase timeout.

---

## ğŸ›¡ï¸ OTEL Validation

**NEVER trust agent claims without OTEL validation.** Agents optimized to appear successful, not be honest.

```bash
node validation/run-all.mjs comprehensive
grep "Score:" validation-output.log  # MUST be â‰¥80/100
grep "FAILED\|Error" validation-output.log  # MUST be 0 results
```

**Trust Model**:
| Source | Trust | Verification |
|--------|-------|--------------|
| Agent claims | 0% | OTEL â‰¥80/100 required |
| OTEL spans | 95% | External truth |
| Test output | 90% | Ran + read output |
| "It should work" | 10% | No evidence |

---

## ğŸš€ Development Agents & Tools

### Task Tool - Specialized Agents
Use the Task tool with appropriate subagent_type for complex operations:

**Primary Agents** (Use These First):
- `Explore` - Fast codebase exploration (quick/medium/thorough levels)
- `Plan` - Software architecture and implementation planning
- `production-validator` - Production readiness validation
- `coder` - Implementation specialist for clean, efficient code
- `reviewer` - Code review and quality assurance
- `tester` - Comprehensive testing and test writing
- `researcher` - Deep research and information gathering

**Specialized Agents**:
- `planner` - Strategic planning and task orchestration
- `Bash` - Command execution (git, npm, docker operations)
- `general-purpose` - Multi-step tasks requiring multiple tools

**Agent Usage Rules**:
- âœ… Launch multiple agents concurrently in ONE message (up to 10 max)
- âœ… Use `Explore` for codebase questions (not direct Grep/Glob)
- âœ… Verify agent output with commands (NEVER trust claims)
- âœ… Match agent expertise to task requirements
- âŒ Do NOT use deprecated `analyst` agent (use `code-analyzer` tools instead)

---

## ğŸ“¦ Package Structure

### Monorepo Layout
```
/home/user/unrdf/
â”œâ”€â”€ packages/                  # 56 packages
â”‚   â”œâ”€â”€ core/                 # @unrdf/core - Foundation
â”‚   â”œâ”€â”€ oxigraph/             # @unrdf/oxigraph - SPARQL engine
â”‚   â”œâ”€â”€ hooks/                # @unrdf/hooks - Policy framework
â”‚   â”œâ”€â”€ streaming/            # @unrdf/streaming - Change feeds
â”‚   â”œâ”€â”€ federation/           # @unrdf/federation - Distributed queries
â”‚   â”œâ”€â”€ cli/                  # @unrdf/cli - Command-line tools
â”‚   â”œâ”€â”€ kgc-4d/               # @unrdf/kgc-4d - Time-travel engine
â”‚   â”œâ”€â”€ kgc-runtime/          # @unrdf/kgc-runtime - Governance runtime
â”‚   â”œâ”€â”€ v6-core/              # @unrdf/v6-core - v6 control plane
â”‚   â”œâ”€â”€ v6-compat/            # @unrdf/v6-compat - Migration bridge
â”‚   â”œâ”€â”€ yawl/                 # @unrdf/yawl - Workflow engine
â”‚   â”œâ”€â”€ knowledge-engine/     # @unrdf/knowledge-engine - Inference
â”‚   â””â”€â”€ ...                   # 44 more packages
â”œâ”€â”€ test/                      # 137 test files
â”œâ”€â”€ benchmarks/                # Performance benchmarks (core/, advanced/, integration/, v6/, regression/)
â”œâ”€â”€ docs/                      # 1,269 documentation files (Diataxis)
â”œâ”€â”€ examples/                  # 125 example files
â”œâ”€â”€ scripts/                   # 100+ development workflow scripts
â”œâ”€â”€ .claude/                   # Claude Code configuration
â”‚   â”œâ”€â”€ hooks/                # Git hooks and automation
â”‚   â”œâ”€â”€ rules/                # Code quality rules
â”‚   â””â”€â”€ helpers/              # Validation helpers
â”œâ”€â”€ .github/                   # GitHub Actions (25 workflows)
â””â”€â”€ apps/                      # Application projects (docs-site)
```

### Key Dependencies
```json
{
  "oxigraph": "^0.5.2",           // Rust-based SPARQL engine
  "zod": "^3.25.76",              // Runtime validation
  "@opentelemetry/api": "^1.9.0", // Observability
  "vitest": "^4.0.16",            // Test framework
  "pnpm": ">=7.0.0"               // Package manager (REQUIRED)
}
```

---

## ğŸ§ª Testing Infrastructure

### Configuration
- **Framework**: Vitest 4.0.16
- **Default Timeout**: 5 seconds (Andon principle)
- **Coverage Requirement**: 80% (lines, functions, branches, statements)
- **Parallel Execution**: 10 max forks

### Test Commands
```bash
# Core test commands
pnpm test                     # Run all tests
pnpm test:fast                # Fast pre-push suite (<30s)
pnpm test:coverage            # With coverage reports

# Package-specific
pnpm test:core               # Test @unrdf/core
pnpm test:hooks              # Test @unrdf/hooks
pnpm test:cli                # Test @unrdf/cli

# Watch mode
pnpm test:watch              # Watch all tests
pnpm test:watch:pkg core     # Watch specific package
```

### Test Organization
```
test/
â”œâ”€â”€ diff.test.mjs             # 685 lines - Core diff engine
â”œâ”€â”€ project-engine.test.mjs   # 487 lines - Domain inference
â”œâ”€â”€ dark-matter-80-20.test.mjs # 362 lines - Optimization
â”œâ”€â”€ e2e-integration.test.mjs  # 112 lines - End-to-end
â”œâ”€â”€ knowledge-engine/         # Knowledge engine tests
â”‚   â””â”€â”€ test-infrastructure/  # Test utilities
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ otel-validator.mjs    # OTEL validation
â””â”€â”€ vitest-helpers.mjs        # Shared helpers
```

### 80/20 Fast Suite
10 critical tests, ~900 lines, <30 seconds execution:
- `test/diff.test.mjs`
- `test/project-engine.test.mjs`
- `test/dark-matter-80-20.test.mjs`
- `test/e2e-integration.test.mjs`
- `test/knowledge-engine/utils/*.test.mjs`

---

## ğŸ”§ Development Workflow

### Essential Commands
```bash
# Development
pnpm dev                      # Start all dev servers
pnpm build                    # Build all packages
pnpm clean                    # Clean build artifacts

# Quality
pnpm lint                     # ESLint (0 violations target)
pnpm lint:fix                 # Auto-fix lint issues
pnpm format                   # Prettier formatting
pnpm quality                  # Generate quality report

# Validation
./scripts/dev-workflow.sh validate     # Full validation
./scripts/dev-workflow.sh fix          # Auto-fix issues
./scripts/dev-workflow.sh validate:commit # Pre-commit

# Benchmarks
pnpm benchmark                # Run all benchmarks
pnpm benchmark:core           # Core benchmarks
pnpm benchmark:regression     # Regression detection
pnpm profile:cpu              # CPU profiling
pnpm profile:mem              # Memory profiling
```

### CI/CD Workflows (25 Total)
**Core Workflows**:
- **ci.yml** - Main CI (TypeScript gate, lint, test matrix Node 18/20/22)
- **quality.yml** / **quality-gates.yml** - Quality gates (80% coverage, 70+ quality score)
- **release.yml** / **v6-release.yml** - Release automation (npm, Docker, GitHub Release)
- **code-quality.yml** - ESLint, Prettier, JSDoc validation
- **security.yml** - Security scanning and vulnerability detection

**Testing & Validation**:
- **v6-tests.yml** - V6 determinism and performance tests
- **v6-validate.yml** - V6 validation suite
- **v6-regression.yml** - V6 regression detection
- **performance-tracking.yml** / **perf.yml** - Performance regression detection
- **checks.yml** - Pre-commit and validation checks
- **thesis-validation.yml** - Academic thesis validation
- **otel-weaver-validate.yml** - OpenTelemetry validation

**Deployment & Automation**:
- **deploy-production.yml** / **deploy-staging.yml** - Environment deployments
- **deploy-benchmark-dashboard.yml** - Performance dashboard
- **deploy-book.yml** - Documentation book deployment
- **nextjs.yml** - Next.js application deployment
- **autofix.yml** - Automated code fixes
- **dependency-update.yml** - Automated dependency updates
- **cache-optimization.yml** - Build cache optimization
- **ggen-integration.yml** - Generated code integration
- **capability-docs.yml** - Capability documentation generation

### Quality Thresholds
| Metric | Threshold | Gate |
|--------|-----------|------|
| Test Coverage | 80% | Block |
| Lint Violations | 0 | Block |
| Quality Score | 70/100 | Block |
| Regression (Latency) | +20% | Block |
| Regression (Memory) | +30% | Block |

---

## ğŸ“Š Performance Benchmarks

### Benchmark Suites
```
benchmarks/
â”œâ”€â”€ core/                    # 5 core benchmarks (80/20)
â”‚   â”œâ”€â”€ 01-hook-registration.bench.mjs
â”‚   â”œâ”€â”€ 02-hook-execution-latency.bench.mjs
â”‚   â”œâ”€â”€ 03-concurrent-execution.bench.mjs
â”‚   â”œâ”€â”€ 04-memory-footprint.bench.mjs
â”‚   â””â”€â”€ 05-condition-evaluation.bench.mjs
â”œâ”€â”€ integration/             # Federation, streaming, knowledge-engine
â”œâ”€â”€ v6/                      # v6-specific performance
â”œâ”€â”€ regression/              # Baseline comparison
â””â”€â”€ baselines/baseline.json  # Performance baselines
```

### Performance Targets (v6)
| Operation | P95 Target | Actual | Status |
|-----------|------------|--------|--------|
| Receipt Creation | <1ms | 0.017ms | PASS |
| Delta Validation | <5ms | 0.005ms | PASS |
| Receipt Verification | <0.5ms | 0.000ms | PASS |
| Receipt Chain (10) | <50ms | 0.347ms | PASS |
| SPARQL Query (simple) | <10ms | - | - |

---

## ğŸ”’ Security & Validation

### Zod Schema Validation
- **555 Zod imports** across codebase
- **77 schema definition files** (`.schema.mjs`)
- All public APIs validated with Zod
- Runtime validation at system boundaries

### Validation Patterns
```javascript
// Strict validation (throws on invalid)
const validated = MySchema.parse(data);

// Safe validation (returns result object)
const result = MySchema.safeParse(data);
if (!result.success) { /* handle error */ }

// Custom refinements for security
z.string().refine(
  path => !path.includes('..'),
  'Path traversal not allowed'
);
```

### Security Checks (605 lines)
- Secret detection (API keys, AWS credentials, JWT tokens)
- Injection vulnerability detection (SQL, command, XSS)
- Path traversal prevention
- Error sanitization (removes sensitive info from errors)

### Production Validation Gates
1. Code Quality (20%) - JSDoc, linting, complexity
2. Testing (25%) - Coverage, pass rate, benchmarks
3. Security (20%) - Credential detection, injection patterns
4. Dependencies (15%) - Circular deps, vulnerabilities
5. Documentation (5%) - README, examples, API docs
6. Performance (10%) - Execution benchmarks, memory

---

## ğŸ’» Code Style Essentials

### RDF/Triple Store (MANDATORY)
```javascript
// âœ… CORRECT - Use Oxigraph
import { createStore, dataFactory } from '@unrdf/oxigraph';
const store = createStore();

// âŒ WRONG - Never import N3 directly
import { Store } from 'n3';  // FORBIDDEN in app code

// âœ… CORRECT - Streaming only via justified module
import { Parser } from '@unrdf/core/rdf/n3-justified-only';
```

### File Conventions
- **Extension**: `.mjs` (100% ESM)
- **Naming**: kebab-case (`query-cache.mjs`)
- **Schemas**: Co-located (`.schema.mjs` suffix)
- **Max Lines**: 500 per file

### JSDoc Pattern
```javascript
/**
 * @file [Purpose/Component Name]
 * @module [module-name]
 * @description [Detailed description]
 */

/**
 * Brief description
 * @param {Type} paramName - Description
 * @param {Object} [options] - Optional config
 * @returns {ReturnType} Description
 * @throws {Error} Error conditions
 * @example
 * const result = functionName(arg1);
 */
export function functionName(param, options = {}) { }
```

### Export Pattern
```javascript
// Named exports (primary)
export function createStore() { }
export function executeQuery() { }

// Re-export aggregation in index.mjs
export { createStore, executeQuery } from './store.mjs';
export * from './schemas.mjs';
```

---

## ğŸ“ Counter-Practice Lessons

### ğŸš« DON'T DO (Will fail)
1. Add OTEL to implementation modules (observability â‰  business logic)
2. Add defensive code (guards hide real bugs)
3. Try to improve working patterns (copy exactly)
4. Create complex test suites (5 essential > 95 complex)
5. Trust claims without evidence
6. Import from `'n3'` directly (use `@unrdf/oxigraph`)

### âœ… WHAT WORKS (Evidence-Based)
- Pure functions with NO OTEL in implementation
- Zod validation + simple try-catch
- Batch refactoring in phases
- Centralize library migrations (2 modules first)
- 5 focused tests (100% pass) > 95 flaky (60% pass)
- **MEASURE before claiming success**

**Golden Rule**: Centralize old API in 2 modules first â†’ Result: 100% migration, 40% faster, 60% less memory, 330/330 tests passing.

---

## ğŸ§  Working With Claude: Internal Patterns

### Token Generation
- Sequential generation - early tokens lock direction
- Hard to backtrack mid-response
- **Action**: Clarify intent upfront

### Context Window
- Sweet spot: 50K-100K tokens
- Degradation: >150K tokens
- **Action**: Summarize periodically

### Uncertainty Calibration
- "Not sure" = 70% confident
- "Let me check" = 20-30% confident
- Confident â‰  95%+ (often 60-80%)
- **Action**: Use OTEL for claims

### Failure Modes
- **Weak**: Exact counts, nested logic >3 levels
- **Strong**: Pattern matching, refactoring, planning
- **Check**: File counts (`ls | wc -l`), exact numbers

### Quality Degradation
- First response = best thought-through
- Rapid iteration = patch-over-patch
- After ~15 messages = coherence drops
- **Action**: Restart for complex problems

---

## ğŸ“š Documentation (Diataxis Framework)

### Structure
```
docs/
â”œâ”€â”€ diataxis/
â”‚   â”œâ”€â”€ tutorials/       # Learning-oriented (10-30 min)
â”‚   â”œâ”€â”€ how-to/          # Task-oriented (3-10 min)
â”‚   â”œâ”€â”€ reference/       # API documentation
â”‚   â””â”€â”€ explanation/     # Conceptual deep-dives
â”œâ”€â”€ substrate/           # Package-specific docs
â””â”€â”€ README.md            # Master navigation
```

### Examples
```
examples/
â”œâ”€â”€ 01-minimal-parse-query.mjs  # Beginner (3-5 min)
â”œâ”€â”€ basic-knowledge-hook.mjs    # Intermediate (15-20 min)
â”œâ”€â”€ dark-matter-80-20.mjs       # Advanced (25-30 min)
â””â”€â”€ README.md                   # Selection guide
```

---

## ğŸ¤” Session Quality Checklist

**Before declaring complete**:

### Claims vs Reality
- [ ] Did I RUN code or just read it?
- [ ] Did I read FULL output or stop at first pass?
- [ ] What BREAKS if claim is wrong?
- [ ] Can I REPRODUCE from scratch?

### Evidence Quality
- [ ] Test output showing success? (Not "tests pass")
- [ ] File counts with `ls | wc -l`? (Not "~X files")
- [ ] OTEL spans/logs? (Not "should work")
- [ ] Before/after metrics? (Not "faster")

### Process Quality
- [ ] Batched operations in ONE message?
- [ ] Timeout all commands?
- [ ] Verified cross-references?
- [ ] Measured performance?

### Red Flags (Stop if ANY apply)
- âŒ "I think..." / "should be..." â†’ No evidence
- âŒ "Mostly works" / "almost done" â†’ Not acceptable
- âŒ "Code looks good" â†’ Didn't run it
- âŒ Agent says "done" â†’ Didn't verify

---

## Key Rules (Enforcement)

1. **MJS + JSDoc + Zod** - NO TypeScript in source
2. **Pnpm only** - No `package-lock.json` / `yarn.lock`
3. **No analyst agent** - Use `code-analyzer` instead
4. **OTEL is truth** - Agent claims need OTEL â‰¥80/100
5. **Batch operations** - All in one message
6. **Hyper-advanced first** - Review all 54 agents, pick best match
7. **Pure functions** - No OTEL in business logic
8. **Oxigraph primary** - Never import from `'n3'` in app code

---

## ğŸ† Final Truth

**Core Principle**: Claude Flow coordinates, Claude Code creates. **OTEL spans + test output = ONLY validation.**

**The Adversarial PM Question**: *If someone challenged EVERY claim today, which would survive scrutiny?*

Answer honestly. That's your real quality level.

---

## Quick Reference

### Verify Commands
```bash
# Test (5s default timeout)
timeout 5s pnpm test:fast

# Lint (0 violations)
timeout 30s pnpm lint

# Build (verify artifacts)
timeout 60s pnpm build

# OTEL Validation (â‰¥80/100)
node validation/run-all.mjs comprehensive

# Check N3 imports (MUST be 0)
grep -r "from 'n3'" packages/*/src --include="*.mjs" | grep -v n3-justified | wc -l

# File sizes (<500 lines)
find packages/*/src -name "*.mjs" -exec wc -l {} + | awk '$1 > 500'
```

### Package Manager (pnpm ONLY)
```bash
pnpm install                    # Install dependencies
pnpm -r test                    # Run all package tests
pnpm -C packages/core test      # Test specific package
pnpm add -D <pkg>               # Add dev dependency (workspace root)
pnpm --filter @unrdf/core add <pkg>  # Add to specific package

# Common workspace commands
pnpm list:packages              # List all packages
pnpm check:health               # Health check all packages
pnpm update:deps                # Update dependencies
pnpm clean:all                  # Clean all build artifacts
```

### Git Workflow
```bash
git status            # Check changes
git add -A && git commit -m "feat: description"
git push -u origin <branch>

# Branch naming convention
# Format: claude/<description>-<sessionId>
# Example: claude/add-feature-S3gJi
# CRITICAL: Branch must start with 'claude/' and end with session ID or push fails with 403
```

### YAWL Workflow Engine (8 Extension Packages)

UNRDF includes comprehensive YAWL (Yet Another Workflow Language) support:

**Core YAWL**:
- `@unrdf/yawl` - Core YAWL workflow engine

**Extensions**:
- `@unrdf/yawl-ai` - AI/LLM integration for workflows
- `@unrdf/yawl-api` - REST API for workflow management
- `@unrdf/yawl-durable` - Durable execution patterns
- `@unrdf/yawl-kafka` - Kafka integration for event-driven workflows
- `@unrdf/yawl-langchain` - LangChain integration
- `@unrdf/yawl-observability` - OTEL instrumentation for workflows
- `@unrdf/yawl-queue` - Queue-based task management
- `@unrdf/yawl-realtime` - Real-time workflow execution
- `@unrdf/yawl-viz` - Workflow visualization

### KGC (Knowledge Graph Governance) Suite (9 Packages)

Advanced governance and tooling for knowledge graphs:

- `@unrdf/kgc-4d` - 4D temporal event sourcing engine
- `@unrdf/kgc-runtime` - Governance runtime with Zod schemas
- `@unrdf/kgc-substrate` - Deterministic KnowledgeStore
- `@unrdf/kgc-claude` - Claude AI integration
- `@unrdf/kgc-cli` - Command-line tools
- `@unrdf/kgc-docs` - Documentation generator
- `@unrdf/kgc-multiverse` - Multi-universe management
- `@unrdf/kgc-probe` - Runtime probing and inspection
- `@unrdf/kgc-swarm` - Swarm coordination
- `@unrdf/kgc-tools` - Utility tools

### Claude Code Integration

The `.claude/` directory contains Claude Code specific configuration:

```
.claude/
â”œâ”€â”€ hooks/                    # Git hooks and automation
â”‚   â”œâ”€â”€ hooks-shared.mjs     # Shared hook utilities
â”‚   â”œâ”€â”€ on-bounds-exceeded-hook.mjs
â”‚   â”œâ”€â”€ on-edit-docs-hook.mjs
â”‚   â”œâ”€â”€ on-non-determinism-hook.mjs
â”‚   â””â”€â”€ on-write-docs-hook.mjs
â”œâ”€â”€ rules/                    # Code quality enforcement
â”‚   â”œâ”€â”€ agent-quality.md     # Agent output standards
â”‚   â”œâ”€â”€ code-quality.md      # Code quality rules
â”‚   â””â”€â”€ testing-standards.md # Testing requirements
â””â”€â”€ helpers/                  # Validation utilities
    â”œâ”€â”€ count-stats.mjs
    â””â”€â”€ validate-file.mjs
```

**Hooks trigger automatically** on:
- File edits in docs/ directory
- Documentation writes
- Non-deterministic code patterns
- Context bounds exceeded
