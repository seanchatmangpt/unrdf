# YAWL Performance Benchmark Suite

**Purpose**: Empirical validation of theoretical performance claims
**Date**: 2025-12-25
**Status**: Specification (Implementation Needed)

---

## Overview

This document specifies a comprehensive benchmark suite to measure **actual performance** of the YAWL workflow engine and validate/refine the 7 major performance claims.

**Benchmark Philosophy**:
1. **Measure reality, not assumptions**
2. **Isolate variables** (policy vs engine overhead)
3. **Test realistic scenarios** (not just best-case)
4. **Report distributions** (p50, p95, p99 - not just averages)
5. **Reproducible** (fixed seeds, documented hardware)

---

## Benchmark 1: Task Activation Latency

### File: `packages/yawl/test/benchmarks/task-activation.bench.mjs`

### Objective

Measure end-to-end latency of `engine.enableTask()` under various policy configurations.

### Test Matrix

| Scenario | Policy Hooks | SPARQL Query | Expected p50 | Expected p99 |
|----------|-------------|--------------|--------------|--------------|
| No policy | 0 | None | <100Î¼s | <200Î¼s |
| Simple validation | 1 | None | <500Î¼s | <1ms |
| SPARQL enablement | 1 | Simple ASK | <2ms | <5ms |
| 3-hook chain | 3 | Simple ASK | <2ms | <10ms |
| Complex governance | 5 | Complex pattern | <10ms | <20ms |

### Implementation

```javascript
import { describe, it, expect } from 'vitest';
import { performance } from 'node:perf_hooks';
import { createWorkflowEngine } from '@unrdf/yawl/engine';
import { YawlWorkflow } from '@unrdf/yawl/workflow';
import { createYAWLPolicyPack } from '@unrdf/yawl/hooks/yawl-hooks';

describe('Task Activation Latency Benchmarks', () => {
  /**
   * Test configuration
   */
  const SAMPLE_SIZE = 10000;
  const WARMUP_SIZE = 1000;

  /**
   * Helper: Run benchmark with statistical analysis
   */
  function runBenchmark(name, fn, samples = SAMPLE_SIZE) {
    const durations = [];

    // Warmup phase
    for (let i = 0; i < WARMUP_SIZE; i++) {
      fn();
    }

    // Measurement phase
    for (let i = 0; i < samples; i++) {
      const start = performance.now();
      fn();
      durations.push(performance.now() - start);
    }

    // Calculate percentiles
    durations.sort((a, b) => a - b);
    const p50 = durations[Math.floor(samples * 0.50)];
    const p75 = durations[Math.floor(samples * 0.75)];
    const p90 = durations[Math.floor(samples * 0.90)];
    const p95 = durations[Math.floor(samples * 0.95)];
    const p99 = durations[Math.floor(samples * 0.99)];
    const mean = durations.reduce((a, b) => a + b, 0) / samples;
    const max = durations[samples - 1];

    console.log(`\nðŸ“Š ${name} (n=${samples})`);
    console.log(`  Mean: ${(mean * 1000).toFixed(2)}Î¼s`);
    console.log(`  p50:  ${(p50 * 1000).toFixed(2)}Î¼s`);
    console.log(`  p75:  ${(p75 * 1000).toFixed(2)}Î¼s`);
    console.log(`  p90:  ${(p90 * 1000).toFixed(2)}Î¼s`);
    console.log(`  p95:  ${(p95 * 1000).toFixed(2)}Î¼s`);
    console.log(`  p99:  ${(p99 * 1000).toFixed(2)}Î¼s`);
    console.log(`  max:  ${(max * 1000).toFixed(2)}Î¼s`);

    return { mean, p50, p75, p90, p95, p99, max, durations };
  }

  /**
   * Benchmark 1.1: No Policy
   */
  it('No policy - baseline overhead', async () => {
    const engine = createWorkflowEngine({ enableEventLog: false });
    const workflow = new YawlWorkflow({
      id: 'test-workflow',
      name: 'Test Workflow',
    });
    workflow.addTask({ id: 'task-1', name: 'Task 1' });
    workflow.setStart('task-1');
    workflow.setEnd(['task-1']);
    engine.registerWorkflow(workflow);

    const { case: testCase } = await engine.createCase(workflow.id);

    const stats = runBenchmark(
      'Task activation - No policy',
      async () => {
        await engine.enableTask(testCase.id, 'task-1', 'test-actor');
      }
    );

    // Assertion: p99 should be <200Î¼s (0.2ms)
    expect(stats.p99).toBeLessThan(0.2);
  });

  /**
   * Benchmark 1.2: Simple Validation Hook
   */
  it('Simple validation hook (1 hook, no SPARQL)', async () => {
    const engine = createWorkflowEngine({ enableEventLog: false });
    const workflow = new YawlWorkflow({
      id: 'test-workflow',
      name: 'Test Workflow',
    });
    workflow.addTask({ id: 'task-1', name: 'Task 1' });
    workflow.setStart('task-1');
    workflow.setEnd(['task-1']);

    // Add simple validation policy
    const policyPack = createYAWLPolicyPack({
      ...workflow.toJSON(),
      tasks: [
        {
          id: 'task-1',
          kind: 'AtomicTask',
          inputConditions: [], // No SPARQL query
        },
      ],
    });
    engine.registerPolicyPack(workflow.id, policyPack);
    engine.registerWorkflow(workflow);

    const { case: testCase } = await engine.createCase(workflow.id);

    const stats = runBenchmark(
      'Task activation - Simple validation',
      async () => {
        await engine.enableTask(testCase.id, 'task-1', 'test-actor');
      }
    );

    // Assertion: p99 should be <1ms
    expect(stats.p99).toBeLessThan(1.0);
  });

  /**
   * Benchmark 1.3: SPARQL Enablement Query
   */
  it('SPARQL enablement query (1 hook, simple ASK)', async () => {
    const engine = createWorkflowEngine({ enableEventLog: false });
    const store = engine.store;

    // Pre-populate store with test data
    for (let i = 0; i < 1000; i++) {
      store.add(quad(
        namedNode(`http://example.org/condition-${i}`),
        namedNode('http://yawl.io/conditionId'),
        literal(`cond-${i}`)
      ));
    }

    const workflow = new YawlWorkflow({
      id: 'test-workflow',
      name: 'Test Workflow',
    });
    workflow.addTask({ id: 'task-1', name: 'Task 1' });
    workflow.setStart('task-1');
    workflow.setEnd(['task-1']);

    // Add SPARQL policy
    const conditionEvaluator = {
      evaluate: async (condition, store, env) => {
        // Mock SPARQL evaluation
        const result = await store.query(condition.query);
        return result.length > 0;
      },
    };

    const policyPack = createYAWLPolicyPack(
      {
        ...workflow.toJSON(),
        tasks: [
          {
            id: 'task-1',
            kind: 'AtomicTask',
            inputConditions: ['cond-1', 'cond-2'], // SPARQL ASK query
          },
        ],
      },
      { conditionEvaluator }
    );
    engine.registerPolicyPack(workflow.id, policyPack);
    engine.registerWorkflow(workflow);

    const { case: testCase } = await engine.createCase(workflow.id);

    const stats = runBenchmark(
      'Task activation - SPARQL enablement',
      async () => {
        await engine.enableTask(testCase.id, 'task-1', 'test-actor');
      }
    );

    // Assertion: p99 should be <10ms
    expect(stats.p99).toBeLessThan(10.0);
  });

  /**
   * Benchmark 1.4: 3-Hook Chain
   */
  it('3-hook validation chain', async () => {
    const engine = createWorkflowEngine({ enableEventLog: false });
    const workflow = new YawlWorkflow({
      id: 'test-workflow',
      name: 'Test Workflow',
    });
    workflow.addTask({ id: 'task-1', name: 'Task 1' });
    workflow.setStart('task-1');
    workflow.setEnd(['task-1']);

    // Create 3-hook policy pack
    const policyPack = createYAWLPolicyPack({
      ...workflow.toJSON(),
      tasks: [
        {
          id: 'task-1',
          kind: 'AtomicTask',
          inputConditions: ['cond-1', 'cond-2', 'cond-3'], // 3 SPARQL queries
        },
      ],
    });
    engine.registerPolicyPack(workflow.id, policyPack);
    engine.registerWorkflow(workflow);

    const { case: testCase } = await engine.createCase(workflow.id);

    const stats = runBenchmark(
      'Task activation - 3-hook chain',
      async () => {
        await engine.enableTask(testCase.id, 'task-1', 'test-actor');
      }
    );

    // Assertion: p99 should be <10ms
    expect(stats.p99).toBeLessThan(10.0);
    console.log('\nâš ï¸  Expected <1ms claim may not hold for realistic policies');
  });

  /**
   * Benchmark 1.5: Latency Distribution Visualization
   */
  it('Generate latency histogram', async () => {
    const engine = createWorkflowEngine({ enableEventLog: false });
    const workflow = new YawlWorkflow({
      id: 'test-workflow',
      name: 'Test Workflow',
    });
    workflow.addTask({ id: 'task-1', name: 'Task 1' });
    workflow.setStart('task-1');
    workflow.setEnd(['task-1']);
    engine.registerWorkflow(workflow);

    const { case: testCase } = await engine.createCase(workflow.id);

    const durations = [];
    for (let i = 0; i < 10000; i++) {
      const start = performance.now();
      await engine.enableTask(testCase.id, 'task-1', 'test-actor');
      durations.push(performance.now() - start);
    }

    // Build histogram
    const buckets = [0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50];
    const histogram = buckets.reduce((acc, bucket) => {
      acc[bucket] = 0;
      return acc;
    }, {});
    histogram['inf'] = 0;

    durations.forEach(d => {
      let placed = false;
      for (const bucket of buckets) {
        if (d < bucket) {
          histogram[bucket]++;
          placed = true;
          break;
        }
      }
      if (!placed) histogram['inf']++;
    });

    console.log('\nðŸ“Š Latency Histogram (ms):');
    for (const [bucket, count] of Object.entries(histogram)) {
      const bar = 'â–ˆ'.repeat(Math.floor(count / 100));
      console.log(`  <${bucket}ms: ${count.toString().padStart(5)} ${bar}`);
    }
  });
});
```

### Expected Output

```
ðŸ“Š Task activation - No policy (n=10000)
  Mean: 45.23Î¼s
  p50:  42.10Î¼s
  p75:  48.30Î¼s
  p90:  55.20Î¼s
  p95:  62.40Î¼s
  p99:  85.30Î¼s    â† Should be <200Î¼s âœ…
  max:  120.50Î¼s

ðŸ“Š Task activation - Simple validation (n=10000)
  Mean: 185.45Î¼s
  p50:  175.30Î¼s
  p75:  192.10Î¼s
  p90:  210.50Î¼s
  p95:  230.20Î¼s
  p99:  280.60Î¼s   â† Should be <1ms âœ…
  max:  350.10Î¼s

ðŸ“Š Task activation - SPARQL enablement (n=10000)
  Mean: 1,523.20Î¼s (1.52ms)
  p50:  1,420.10Î¼s (1.42ms)
  p75:  1,680.30Î¼s (1.68ms)
  p90:  2,120.50Î¼s (2.12ms)
  p95:  2,450.20Î¼s (2.45ms)
  p99:  3,280.60Î¼s (3.28ms) â† Should be <10ms âœ…
  max:  4,150.10Î¼s (4.15ms)

âš ï¸  <1ms claim does NOT hold for policies with SPARQL queries
```

---

## Benchmark 2: Receipt Generation Throughput

### File: `packages/yawl/test/benchmarks/receipt-throughput.bench.mjs`

### Objective

Measure actual `generateReceipt()` throughput and validate >100K/sec claim.

### Test Scenarios

1. **Sequential chained receipts** (realistic workflow)
2. **Parallel independent receipts** (multi-core)
3. **Batched receipt generation** (optimization)

### Implementation

```javascript
import { describe, it, expect } from 'vitest';
import { performance } from 'node:perf_hooks';
import { generateReceipt } from '@unrdf/yawl/receipt';

describe('Receipt Generation Throughput Benchmarks', () => {
  const RECEIPT_COUNT = 100000;

  /**
   * Benchmark 2.1: Sequential Chained Receipts
   */
  it('Sequential chained receipt generation', async () => {
    const receipts = [];
    const event = {
      eventType: 'TASK_COMPLETED',
      caseId: 'case-123',
      taskId: 'task-456',
      payload: {
        decision: 'APPROVE',
        justification: {
          reasoning: 'All conditions satisfied',
        },
      },
    };

    console.log(`\nðŸ“Š Generating ${RECEIPT_COUNT} chained receipts...`);
    const start = performance.now();

    for (let i = 0; i < RECEIPT_COUNT; i++) {
      const receipt = await generateReceipt(event, receipts[i - 1] || null);
      receipts.push(receipt);

      if (i % 10000 === 0) {
        const elapsed = performance.now() - start;
        const current_tps = (i / elapsed) * 1000;
        console.log(`  ${i.toLocaleString()}: ${current_tps.toFixed(0)} receipts/sec`);
      }
    }

    const duration = performance.now() - start;
    const throughput = (RECEIPT_COUNT / duration) * 1000;

    console.log(`\nâœ… Completed ${RECEIPT_COUNT} receipts in ${duration.toFixed(2)}ms`);
    console.log(`   Throughput: ${throughput.toFixed(0)} receipts/sec`);
    console.log(`   Per-receipt: ${(duration * 1000 / RECEIPT_COUNT).toFixed(2)}Î¼s`);

    // Assertion: Should be 40,000-50,000 receipts/sec
    expect(throughput).toBeGreaterThan(35000);
    expect(throughput).toBeLessThan(60000);
    console.log(`\nâš ï¸  Claim of >100K receipts/sec NOT met (actual: ${throughput.toFixed(0)})`);
  });

  /**
   * Benchmark 2.2: Parallel Independent Receipts (Multi-Core)
   */
  it('Parallel independent receipt generation (4 workers)', async () => {
    const event = {
      eventType: 'TASK_COMPLETED',
      caseId: 'case-123',
      taskId: 'task-456',
      payload: { decision: 'APPROVE' },
    };

    const WORKERS = 4;
    const PER_WORKER = RECEIPT_COUNT / WORKERS;

    console.log(`\nðŸ“Š Generating ${RECEIPT_COUNT} receipts across ${WORKERS} workers...`);
    const start = performance.now();

    const workers = Array(WORKERS)
      .fill(0)
      .map(async () => {
        const receipts = [];
        for (let i = 0; i < PER_WORKER; i++) {
          const receipt = await generateReceipt(event, null); // Independent chains
          receipts.push(receipt);
        }
        return receipts;
      });

    await Promise.all(workers);

    const duration = performance.now() - start;
    const throughput = (RECEIPT_COUNT / duration) * 1000;

    console.log(`\nâœ… Completed ${RECEIPT_COUNT} receipts in ${duration.toFixed(2)}ms`);
    console.log(`   Throughput: ${throughput.toFixed(0)} receipts/sec`);
    console.log(`   Speedup: ${(throughput / 45000).toFixed(2)}x vs sequential`);

    // Assertion: Should be ~180,000 receipts/sec on 4 cores
    expect(throughput).toBeGreaterThan(140000);
    console.log(`\nâœ… Claim of >100K receipts/sec MET with parallelization`);
  });

  /**
   * Benchmark 2.3: Verify Chain Integrity
   */
  it('Verify receipt chain integrity', async () => {
    const receipts = [];
    const event = {
      eventType: 'TASK_COMPLETED',
      caseId: 'case-123',
      taskId: 'task-456',
      payload: { decision: 'APPROVE' },
    };

    // Generate 1000 chained receipts
    for (let i = 0; i < 1000; i++) {
      const receipt = await generateReceipt(event, receipts[i - 1] || null);
      receipts.push(receipt);
    }

    // Verify chain integrity
    console.log('\nðŸ“Š Verifying chain integrity...');
    const verifyStart = performance.now();

    for (let i = 1; i < receipts.length; i++) {
      const current = receipts[i];
      const previous = receipts[i - 1];

      expect(current.previousReceiptHash).toBe(previous.receiptHash);

      // Verify hash computation
      const result = await verifyReceipt(current);
      expect(result.valid).toBe(true);
    }

    const verifyDuration = performance.now() - verifyStart;
    const verifyThroughput = (1000 / verifyDuration) * 1000;

    console.log(`   Verified 1000 receipts in ${verifyDuration.toFixed(2)}ms`);
    console.log(`   Throughput: ${verifyThroughput.toFixed(0)} verifications/sec`);
  });
});
```

### Expected Output

```
ðŸ“Š Generating 100,000 chained receipts...
  10000: 48,234 receipts/sec
  20000: 46,891 receipts/sec
  30000: 45,678 receipts/sec
  ...
  100000: 44,932 receipts/sec

âœ… Completed 100,000 receipts in 2,226.30ms
   Throughput: 44,932 receipts/sec
   Per-receipt: 22.26Î¼s

âš ï¸  Claim of >100K receipts/sec NOT met (actual: 44,932)

ðŸ“Š Generating 100,000 receipts across 4 workers...

âœ… Completed 100,000 receipts in 556.58ms
   Throughput: 179,727 receipts/sec
   Speedup: 4.0x vs sequential

âœ… Claim of >100K receipts/sec MET with parallelization
```

---

## Benchmark 3: SPARQL Query Performance

### File: `packages/yawl/test/benchmarks/sparql-queries.bench.mjs`

### Implementation

```javascript
import { describe, it, expect } from 'vitest';
import { performance } from 'node:perf_hooks';
import { KGCStore } from '@unrdf/kgc-4d';
import { quad, namedNode, literal } from '@unrdf/oxigraph';
import {
  generateEnablementQuery,
  generatePredicateQuery,
  generateResourceCapacityQuery,
} from '@unrdf/yawl/hooks/yawl-hooks';

describe('SPARQL Query Performance Benchmarks', () => {
  /**
   * Benchmark 3.1: Simple Predicate Query
   */
  it('Simple predicate query (indexed)', async () => {
    const store = new KGCStore();

    // Populate store with 10K quads
    for (let i = 0; i < 10000; i++) {
      store.add(
        quad(
          namedNode(`http://example.org/var-${i}`),
          namedNode('http://yawl.io/name'),
          literal(`var-${i}`)
        )
      );
      store.add(
        quad(
          namedNode(`http://example.org/var-${i}`),
          namedNode('http://yawl.io/value'),
          literal(i % 2 === 0 ? 'true' : 'false')
        )
      );
    }

    const query = generatePredicateQuery('approved');

    // Warmup
    for (let i = 0; i < 100; i++) {
      await store.query(query);
    }

    // Measure
    const durations = [];
    for (let i = 0; i < 1000; i++) {
      const start = performance.now();
      await store.query(query);
      durations.push(performance.now() - start);
    }

    durations.sort((a, b) => a - b);
    const p50 = durations[Math.floor(durations.length * 0.50)];
    const p99 = durations[Math.floor(durations.length * 0.99)];

    console.log(`\nðŸ“Š Simple predicate query (10K quads)`);
    console.log(`   p50: ${(p50 * 1000).toFixed(2)}Î¼s`);
    console.log(`   p99: ${(p99 * 1000).toFixed(2)}Î¼s`);

    // Assertion: p99 should be <5ms
    expect(p99).toBeLessThan(5.0);
  });

  /**
   * Benchmark 3.2: Scalability Test (varying store size)
   */
  it('Query scalability test', async () => {
    const storeSizes = [1000, 10000, 100000];

    console.log('\nðŸ“Š Query Scalability Test');
    console.log('Store Size | p50 (Î¼s) | p99 (Î¼s) | Throughput (qps)');
    console.log('-----------|----------|----------|------------------');

    for (const size of storeSizes) {
      const store = new KGCStore();

      // Populate
      for (let i = 0; i < size; i++) {
        store.add(
          quad(
            namedNode(`http://example.org/task-${i}`),
            namedNode('http://yawl.io/status'),
            literal('enabled')
          )
        );
      }

      const query = generateEnablementQuery('task-5000', []);

      // Measure
      const durations = [];
      for (let i = 0; i < 1000; i++) {
        const start = performance.now();
        await store.query(query);
        durations.push(performance.now() - start);
      }

      durations.sort((a, b) => a - b);
      const p50 = durations[Math.floor(durations.length * 0.50)];
      const p99 = durations[Math.floor(durations.length * 0.99)];
      const throughput = (1000 / (durations.reduce((a, b) => a + b) / 1000)).toFixed(0);

      console.log(
        `${size.toString().padStart(10)} | ${(p50 * 1000).toFixed(2).padStart(8)} | ${(p99 * 1000).toFixed(2).padStart(8)} | ${throughput.padStart(16)}`
      );
    }

    console.log('\nâœ… Expected: O(log n) scaling for indexed queries');
  });
});
```

---

## Benchmark 4: Time-Travel Performance

### File: `packages/yawl/test/benchmarks/time-travel.bench.mjs`

### Implementation

```javascript
import { describe, it, expect } from 'vitest';
import { performance } from 'node:perf_hooks';
import { createWorkflowEngine } from '@unrdf/yawl/engine';
import { YawlWorkflow } from '@unrdf/yawl/workflow';

describe('Time-Travel Performance Benchmarks', () => {
  /**
   * Benchmark 4.1: Replay with varying checkpoint counts
   */
  it('Time-travel scaling test', async () => {
    const checkpointCounts = [10, 100, 1000, 10000];

    console.log('\nðŸ“Š Time-Travel Scaling Test');
    console.log('Checkpoints | Search (Î¼s) | Verify (Î¼s) | Total (ms) | Complexity');
    console.log('------------|-------------|-------------|------------|------------');

    for (const count of checkpointCounts) {
      const engine = createWorkflowEngine({
        gitPath: '/tmp/yawl-bench-git',
        enableSnapshots: true,
      });

      const workflow = new YawlWorkflow({
        id: 'test-workflow',
        name: 'Test Workflow',
      });
      workflow.addTask({ id: 'task-1', name: 'Task 1' });
      workflow.setStart('task-1');
      workflow.setEnd(['task-1']);
      engine.registerWorkflow(workflow);

      const { case: testCase } = await engine.createCase(workflow.id);

      // Generate checkpoints
      for (let i = 0; i < count; i++) {
        await engine.checkpoint(`checkpoint-${i}`);
      }

      // Replay to middle checkpoint
      const targetCheckpoint = Math.floor(count / 2);
      const targetTime = engine.checkpoints.get(
        [...engine.checkpoints.keys()][targetCheckpoint]
      );

      const start = performance.now();
      const state = await engine.replayCase(testCase.id, targetTime);
      const duration = performance.now() - start;

      const searchTime = 0; // Would need instrumentation
      const verifyTime = 0; // Would need instrumentation
      const complexity = `O(log ${count})`;

      console.log(
        `${count.toString().padStart(11)} | ${searchTime.toString().padStart(11)} | ${verifyTime.toString().padStart(11)} | ${duration.toFixed(2).padStart(10)} | ${complexity.padStart(10)}`
      );
    }

    console.log('\nâœ… Expected: O(log n) scaling with binary search');
  });
});
```

---

## Running the Benchmark Suite

### Prerequisites

```bash
cd /home/user/unrdf
pnpm install
```

### Run All Benchmarks

```bash
# Run complete suite
timeout 300s pnpm --filter @unrdf/yawl test test/benchmarks/*.bench.mjs

# Run individual benchmark
timeout 60s pnpm --filter @unrdf/yawl test test/benchmarks/task-activation.bench.mjs
```

### Environment Configuration

**Hardware Requirements**:
- CPU: Modern multi-core (4+ cores recommended)
- RAM: 8GB+ available
- SSD: For Git checkpoints

**Software Requirements**:
- Node.js: v20+
- OS: Linux/macOS (for high-resolution timers)

### Report Generation

```bash
# Generate HTML report
pnpm --filter @unrdf/yawl test test/benchmarks/*.bench.mjs --reporter=html > benchmark-report.html

# Generate JSON for analysis
pnpm --filter @unrdf/yawl test test/benchmarks/*.bench.mjs --reporter=json > benchmark-results.json
```

---

## Acceptance Criteria

### Pass/Fail Thresholds

| Benchmark | Metric | Target | Pass Criteria |
|-----------|--------|--------|---------------|
| **Task activation - no policy** | p99 | <200Î¼s | âœ… Must pass |
| **Task activation - simple** | p99 | <1ms | âœ… Must pass |
| **Task activation - SPARQL** | p99 | <10ms | âš ï¸ Advisory (claim needs adjustment) |
| **Receipt throughput - sequential** | TPS | >35K/sec | âœ… Must pass |
| **Receipt throughput - parallel** | TPS | >100K/sec | âœ… Must pass |
| **SPARQL query - simple** | p99 | <5ms | âœ… Must pass |
| **Time-travel - 1K checkpoints** | Total | <10ms | âœ… Must pass |

### Performance Regression Detection

**CI/CD Integration**:
```yaml
# .github/workflows/performance.yml
name: Performance Regression Tests

on: [push, pull_request]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
      - run: pnpm install
      - run: timeout 300s pnpm --filter @unrdf/yawl test test/benchmarks/*.bench.mjs
      - name: Check for regressions
        run: |
          # Compare to baseline
          node scripts/compare-benchmarks.mjs baseline.json benchmark-results.json
```

---

## Next Steps

1. **Implement benchmarks** (create 4 files above)
2. **Run baseline measurements** on reference hardware
3. **Update thesis claims** based on empirical data
4. **Optimize bottlenecks** (if needed)
5. **Re-benchmark** after optimizations
6. **Document final results** in PERFORMANCE-MEASURED.md

---

**Benchmark Suite Specification Completed**: 2025-12-25
**Status**: Ready for implementation
**Estimated Effort**: 8-16 hours to implement + run + analyze
