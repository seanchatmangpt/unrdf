% =============================================================================
% Agent 9 Package Documentation
% Packages 42-48: YAWL Workflow Ecosystem
% Generated: 2025-12-27
% =============================================================================

\label{pkg:unrdf-yawl}
\section{\pkg{unrdf-yawl} --- YAWL Workflow Engine}

\begin{pkgmeta}
Path & \texttt{packages/yawl} \\
Kind & js \\
Entrypoints & 12 files \\
Dependencies & 14 \\
Blurb & YAWL (Yet Another Workflow Language) engine with KGC-4D time-travel and receipt verification \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\textbf{Observable \(\Oobs\)}:
\begin{itemize}
\item \texttt{WorkflowCase} instances tracking case lifecycle (created, active, completed, cancelled, failed)
\item \texttt{WorkItem} status transitions (enabled, fired, allocated, started, completed, suspended, cancelled, failed)
\item SPARQL query results from Oxigraph store containing workflow state
\item Cryptographic receipt chains linking state transitions with hash pointers
\item Event sourcing log capturing all workflow operations with KGC-4D nanosecond timestamps
\end{itemize}

\textbf{Artifact \(\Aout\)}:
\begin{itemize}
\item RDF triples in YAWL ontology namespace (\texttt{http://yawlfoundation.org/yawlschema})
\item Workflow specifications with task definitions, flows, split/join behaviors
\item Receipt objects containing \texttt{receiptHash}, \texttt{previousReceiptHash}, event type, timestamp, actor
\item Control flow evaluation results (XOR/AND/OR split behaviors)
\item Resource allocation records with participant/role/capability mappings
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

\begin{verbatim}
// Core types
WorkflowEngine :: {
  registerWorkflow: (WorkflowSpec) -> WorkflowId,
  createCase: (WorkflowId, Data) -> { case: YawlCase, receipt: Receipt },
  startTask: (CaseId, WorkItemId, Options) -> { task: WorkItem, receipt: Receipt },
  completeTask: (CaseId, WorkItemId, Output, Actor) ->
    { task: WorkItem, receipt: Receipt, downstreamEnabled: WorkItem[] }
}

// Zod validation schemas
WorkflowSpecSchema = z.object({
  id: z.string(),
  name: z.string().optional(),
  version: z.string().default('1.0.0'),
  tasks: z.array(TaskDefSchema),
  flows: z.array(FlowDefSchema)
})

TaskDefSchema = z.object({
  id: z.string(),
  name: z.string(),
  kind: z.enum(['atomic', 'composite', 'multiInstance', 'automated', 'manual']),
  splitType: z.enum(['XOR', 'AND', 'OR']).optional(),
  joinType: z.enum(['XOR', 'AND', 'OR']).optional()
})

ReceiptSchema = z.object({
  receiptHash: z.string(),
  previousReceiptHash: z.string().nullable(),
  eventType: z.string(),
  timestamp: z.number(),
  actor: z.string().optional(),
  data: z.record(z.any())
})
\end{verbatim}

\subsection*{Reconciler \(\muRecon\)}

Reconciliation operates through SPARQL query evaluation and receipt chain verification:

\begin{enumerate}
\item \textbf{State reconstruction}: Given receipt chain \(R_0, R_1, \ldots, R_n\), replay events to reconstruct case state
\item \textbf{Control flow evaluation}: Query enabled tasks via SPARQL:
\begin{verbatim}
SELECT ?taskId WHERE {
  ?wi yawl:taskRef ?taskId ;
      yawl:status yawl:enabled ;
      yawl:caseRef ?caseId .
}
\end{verbatim}
\item \textbf{Join satisfaction}: For AND-join tasks, verify all incoming flows completed
\item \textbf{Receipt verification}: Check hash chain integrity: \texttt{R[i].previousReceiptHash === R[i-1].receiptHash}
\item \textbf{Pattern validation}: Ensure split/join behaviors match workflow specification (no orphan tasks, cycles detected)
\end{enumerate}

Guard condition: Transitions allowed only if \texttt{VALID\_TRANSITIONS[currentStatus].includes(newStatus)}.

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

\textbf{Horizontal composition} (\(\PiMerge\)):
\begin{itemize}
\item Van der Aalst workflow patterns (sequence, parallel split, synchronization, exclusive choice, simple merge)
\item Composite workflows via task decomposition (parent case spawns child cases)
\item Multiple instance tasks with dynamic cardinality
\end{itemize}

\textbf{Vertical composition} (\(\oplusMerge\)):
\begin{itemize}
\item YAWL-Hooks integration: Control flow guards evaluated via SPARQL predicates
\item Resource allocation policies: Participant eligibility based on role membership queries
\item Cancellation regions: Task completion triggers cancellation sets
\item Event sourcing layer: KGC-4D append-only log tracks all state changes
\end{itemize}

Pattern builders provide DSL for workflow construction:
\begin{verbatim}
const wf = sequence(
  parallelSplit('fork', ['task1', 'task2']),
  synchronization('join', ['task1', 'task2'])
);
\end{verbatim}

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

\textbf{Guards}:
\begin{itemize}
\item \texttt{isCaseStatus(status)}: Type guard for valid case status values
\item \texttt{isValidCaseTransition(from, to)}: Transition legality check
\item \texttt{validatePattern(pattern)}: Structural correctness (balanced splits/joins)
\item \texttt{detectCycles(flows)}: No cycles except in arbitraryCycle pattern
\item \texttt{validateResourceEligibility(participant, workItem)}: Capability matching
\end{itemize}

\textbf{Invariants}:
\begin{itemize}
\item Receipt chain monotonicity: Timestamps strictly increasing
\item Hash chain integrity: No broken links in receipt sequence
\item Control flow soundness: Every case reaches completion or explicit cancellation
\item Work item uniqueness: One active instance per task definition in case
\item Split-join balance: Every split has matching join (except OR-join with implicit merge)
\end{itemize}

\subsection*{Provenance and Receipts}

Receipt structure:
\begin{verbatim}
{
  receiptHash: sha256(eventType + timestamp + actor + data + prevHash),
  previousReceiptHash: "abc123...",
  eventType: "CASE_CREATED" | "TASK_ENABLED" | "TASK_STARTED" | ...,
  timestamp: 1640000000000000000,  // KGC-4D nanoseconds
  actor: "user@example.com",
  data: { caseId: "case-1", workflowId: "wf-1", ... }
}
\end{verbatim}

Provenance chain allows:
\begin{itemize}
\item Time-travel debugging: Reconstruct case state at any point in history
\item Audit trails: Cryptographic proof of who did what when
\item Deterministic replay: Re-execute workflow from receipts without side effects
\item Compliance: Evidence for regulatory requirements (SOC2, GDPR)
\end{itemize}

\subsection*{Minimal Example}

\begin{verbatim}
import { createWorkflowEngine } from '@unrdf/yawl';

const engine = createWorkflowEngine();

// Define workflow
engine.registerWorkflow({
  id: 'purchase-order',
  name: 'Purchase Order Approval',
  tasks: [
    { id: 'submit', name: 'Submit Order' },
    { id: 'approve', name: 'Approve Order' },
    { id: 'fulfill', name: 'Fulfill Order' }
  ],
  flows: [
    { from: 'submit', to: 'approve' },
    { from: 'approve', to: 'fulfill' }
  ]
});

// Create case
const { case: caseObj, receipt: r1 } = await engine.createCase(
  'purchase-order',
  { items: ['Widget A'], total: 100 }
);

// Get enabled tasks
const enabled = caseObj.getEnabledWorkItems();
console.log(enabled); // [{ id: 'wi-1', taskId: 'submit', status: 'ENABLED' }]

// Start and complete tasks
const { receipt: r2 } = await caseObj.startTask(enabled[0].id);
const { receipt: r3, downstreamEnabled } = await caseObj.completeTask(
  enabled[0].id,
  { approved: true }
);

// Verify receipt chain
console.log(r3.previousReceiptHash === r2.receiptHash); // true
\end{verbatim}

\subsection*{Open Questions}

\begin{enumerate}
\item How to optimize SPARQL queries for large-scale workflow executions (1M+ cases)?
\item What is the performance impact of RDF persistence vs in-memory case state?
\item Can workflow patterns be expressed as SHACL shapes for static validation?
\item How to handle cross-workflow dependencies (composite workflows referencing external cases)?
\item What is the storage overhead of maintaining full receipt chains vs event snapshots?
\end{enumerate}

% =============================================================================

\label{pkg:unrdf-yawl-ai}
\section{\pkg{unrdf-yawl-ai} --- AI-Powered Workflow Optimization}

\begin{pkgmeta}
Path & \texttt{packages/yawl-ai} \\
Kind & js \\
Entrypoints & 4 files \\
Dependencies & 3 \\
Blurb & AI-powered workflow optimization using TensorFlow.js \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\textbf{Observable \(\Oobs\)}:
\begin{itemize}
\item Workflow execution traces (task sequences, durations, outcomes)
\item Performance metrics (throughput, latency, resource utilization)
\item Anomaly scores for execution patterns
\item Neural network prediction confidence scores
\end{itemize}

\textbf{Artifact \(\Aout\)}:
\begin{itemize}
\item TensorFlow.js model files (\texttt{model.json}, weight binaries)
\item Path prediction vectors (next task probabilities)
\item Performance optimization reports identifying bottlenecks
\item Anomaly detection alerts with outlier scores
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

\begin{verbatim}
WorkflowPathPredictor :: {
  train: (ExecutionHistory[]) -> Promise<void>,
  predict: (TaskSequence) -> Promise<{
    nextTask: TaskId,
    probability: number,
    alternatives: Array<{ taskId: TaskId, probability: number }>
  }>
}

PerformanceOptimizer :: {
  analyze: (ExecutionMetrics[]) -> Promise<{
    bottlenecks: Array<{ taskId: TaskId, avgDuration: number, recommendation: string }>,
    parallelizationOpportunities: TaskId[][],
    resourceConstraints: string[]
  }>,
  generateReport: () -> Promise<OptimizationReport>
}

AnomalyDetector :: {
  train: (NormalExecutions[]) -> Promise<void>,
  detect: (Execution) -> Promise<{
    isAnomaly: boolean,
    score: number,
    reasons: string[]
  }>
}
\end{verbatim}

\subsection*{Reconciler \(\muRecon\)}

ML model reconciliation:
\begin{enumerate}
\item \textbf{Training data collection}: Extract features from YAWL receipt chains (task order, durations, outcomes)
\item \textbf{Model training}: Fit TensorFlow.js models (LSTM for path prediction, autoencoder for anomaly detection)
\item \textbf{Prediction integration}: Inject predictions into YAWL task enablement hooks
\item \textbf{Feedback loop}: Update models based on actual vs predicted outcomes
\end{enumerate}

Guard: Models only provide recommendations; workflow engine enforces control flow constraints.

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Horizontal: Predictor, optimizer, and detector modules composed via adapter pattern.

Vertical: Integrates with \texttt{@unrdf/yawl} engine via \texttt{YAWLMLAdapter} hooks.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards: Model predictions never override workflow specification (advisory only).

Invariants: Training data must be sanitized (no PII), models versioned with KGC-4D timestamps.

\subsection*{Provenance and Receipts}

Model training runs recorded with KGC-4D receipts containing:
\begin{itemize}
\item Training dataset hash
\item Model architecture hash
\item Hyperparameters (learning rate, epochs, batch size)
\item Accuracy/loss metrics
\end{itemize}

\subsection*{Minimal Example}

\begin{verbatim}
import { createAdapter, createPredictor } from '@unrdf/yawl-ai';
import { createWorkflowEngine } from '@unrdf/yawl';

const engine = createWorkflowEngine();
const predictor = createPredictor();

const adapter = createAdapter(engine, { predictor });

// Train on historical executions
const history = engine.getExecutionHistory('purchase-order');
await adapter.trainModels();

// Get predictions
const prediction = await predictor.predict(['submit', 'approve']);
console.log(`Next task: ${prediction.nextTask} (${prediction.probability * 100}%)`);
\end{verbatim}

\subsection*{Open Questions}

\begin{enumerate}
\item How to handle concept drift in workflow patterns over time?
\item What is the minimum training data size for reliable predictions?
\item Can transfer learning accelerate model training for new workflows?
\end{enumerate}

% =============================================================================

\label{pkg:unrdf-yawl-api}
\section{\pkg{unrdf-yawl-api} --- RESTful Workflow API}

\begin{pkgmeta}
Path & \texttt{packages/yawl-api} \\
Kind & js \\
Entrypoints & 1 file \\
Dependencies & 7 \\
Blurb & High-performance REST API framework that exposes YAWL workflows as RESTful APIs with OpenAPI documentation \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\textbf{Observable \(\Oobs\)}:
\begin{itemize}
\item HTTP requests (POST /api/workflows, GET /api/cases/:id, etc.)
\item Response status codes (201 Created, 404 Not Found, etc.)
\item HATEOAS hypermedia links in response bodies
\item Swagger UI interactions at /docs endpoint
\end{itemize}

\textbf{Artifact \(\Aout\)}:
\begin{itemize}
\item OpenAPI 3.1 specification generated from Zod schemas
\item JSON API responses with case state and enabled task links
\item Fastify server logs
\item Prometheus metrics (if enabled)
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

\begin{verbatim}
YAWLAPIServer :: {
  listen: (Options) -> Promise<string>,  // Server address
  close: () -> Promise<void>,
  getEngine: () -> WorkflowEngine,
  getServer: () -> FastifyInstance
}

// API routes
POST   /api/workflows                      -> { workflowId, message }
GET    /api/workflows                      -> { workflows: Workflow[] }
GET    /api/workflows/:workflowId          -> WorkflowSpec
POST   /api/workflows/:workflowId/cases    -> { case, receipt }
GET    /api/cases                          -> { cases: Case[] }
GET    /api/cases/:caseId                  -> CaseDetails + _links
POST   /api/cases/:caseId/tasks/:workItemId/start    -> { task, receipt }
POST   /api/cases/:caseId/tasks/:workItemId/complete -> { task, receipt }
POST   /api/cases/:caseId/tasks/:workItemId/cancel   -> { task, receipt }

// HATEOAS links
_links: {
  self: { href, method, description },
  enabledTasks: [{ taskId, actions: { start, cancel } }],
  runningTasks: [{ taskId, actions: { complete, cancel } }]
}
\end{verbatim}

\subsection*{Reconciler \(\muRecon\)}

HTTP layer reconciles with workflow engine state:
\begin{enumerate}
\item Request validation via Zod schemas
\item Route handler invokes \texttt{WorkflowEngine} methods
\item Response serialization includes HATEOAS links based on case state
\item OpenAPI spec auto-generated from \texttt{zodToJsonSchema}
\end{enumerate}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Horizontal: Fastify plugins (CORS, Swagger, OpenAPI) composed via middleware chain.

Vertical: REST API wraps \texttt{@unrdf/yawl} engine, exposing workflow operations via HTTP.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards: Request validation via Zod, authentication middleware (if configured).

Invariants:
\begin{itemize}
\item All responses include \texttt{\_links} for hypermedia navigation
\item Case state changes produce receipts returned in response
\item Error responses conform to RFC 7807 Problem Details
\end{itemize}

\subsection*{Provenance and Receipts}

API operations return receipt objects:
\begin{verbatim}
POST /api/workflows/purchase-order/cases
Response:
{
  "case": { "id": "case-1", "status": "ACTIVE", ... },
  "receipt": {
    "receiptHash": "abc123...",
    "eventType": "CASE_CREATED",
    "timestamp": 1640000000000000000
  }
}
\end{verbatim}

\subsection*{Minimal Example}

\begin{verbatim}
import { createYAWLAPIServer } from '@unrdf/yawl-api';

const server = await createYAWLAPIServer({
  baseUrl: 'http://localhost:3000',
  enableSwagger: true
});

await server.listen({ port: 3000 });
console.log('API docs at http://localhost:3000/docs');

// Register workflow via API
await fetch('http://localhost:3000/api/workflows', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    id: 'my-workflow',
    tasks: [{ id: 'task1', name: 'First Task' }],
    flows: []
  })
});
\end{verbatim}

\subsection*{Open Questions}

\begin{enumerate}
\item How to version API endpoints when workflow schemas evolve?
\item What authentication/authorization strategy for production deployments?
\item Can GraphQL replace REST for more flexible querying?
\end{enumerate}

% =============================================================================

\label{pkg:unrdf-yawl-durable}
\section{\pkg{unrdf-yawl-durable} --- Durable Execution Framework}

\begin{pkgmeta}
Path & \texttt{packages/yawl-durable} \\
Kind & js \\
Entrypoints & 4 files \\
Dependencies & 5 \\
Blurb & Durable execution framework inspired by Temporal.io \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\textbf{Observable \(\Oobs\)}:
\begin{itemize}
\item Activity execution attempts (including retries)
\item Workflow replay events reconstructed from receipts
\item Saga compensation triggers on failure
\item Receipt chain verification results
\end{itemize}

\textbf{Artifact \(\Aout\)}:
\begin{itemize}
\item Workflow execution handle (executionId, status, startedAt)
\item Receipt chain stored in \texttt{receiptStore} Map
\item Activity retry logs with backoff intervals
\item Saga compensation execution records
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

\begin{verbatim}
DurableWorkflowEngine :: {
  defineWorkflow: (WorkflowConfig) -> Promise<WorkflowId>,
  startWorkflow: (WorkflowId, Input, Options) -> Promise<ExecutionHandle>,
  executeActivity: (ExecutionId, ActivityId, Input) -> Promise<ActivityResult>,
  replay: (ExecutionId) -> Promise<ReplayedState>,
  verifyReceiptChain: (ExecutionId) -> Promise<{ valid: boolean, error?: string }>
}

ActivityConfigSchema = z.object({
  id: z.string(),
  name: z.string(),
  handler: z.function(),
  timeout: z.number().default(30000),
  retryPolicy: z.object({
    maxAttempts: z.number().default(3),
    initialInterval: z.number().default(1000),
    backoffCoefficient: z.number().default(2)
  }),
  compensate: z.function().optional()  // Saga compensation handler
})
\end{verbatim}

\subsection*{Reconciler \(\muRecon\)}

Durable execution reconciliation via deterministic replay:
\begin{enumerate}
\item \textbf{Receipt-based replay}: Reconstruct workflow state from cryptographic receipt chain
\item \textbf{Activity idempotency}: Re-execution produces same result (no side effects during replay)
\item \textbf{Retry logic}: Exponential backoff with jitter, max attempts enforcement
\item \textbf{Saga compensation}: On failure, execute compensate handlers in reverse order
\end{enumerate}

Guard: Activities never re-execute during replay (results fetched from receipts).

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Horizontal: Activities composed into workflows via flow definitions.

Vertical: Wraps \texttt{@unrdf/yawl} engine with durable execution composition rules (retry, timeout, saga).

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item Activity timeout enforcement (kill after configured duration)
\item Retry attempt counting (fail after maxAttempts)
\item Receipt chain integrity checks before replay
\end{itemize}

Invariants:
\begin{itemize}
\item Receipt chain is append-only (no mutations)
\item Activity handlers are pure functions during replay
\item Compensation handlers restore system to consistent state
\end{itemize}

\subsection*{Provenance and Receipts}

Receipt chain enables deterministic replay:
\begin{verbatim}
Receipts: [
  { eventType: "WORKFLOW_STARTED", data: { workflowId, input } },
  { eventType: "TASK_ENABLED", data: { taskId: "bookFlight" } },
  { eventType: "TASK_STARTED", data: { taskId: "bookFlight", attempt: 1 } },
  { eventType: "TASK_COMPLETED", data: { taskId: "bookFlight", result: {...} } },
  { eventType: "TASK_ENABLED", data: { taskId: "bookHotel" } },
  ...
]

// Replay: Step through receipts without re-executing activities
const state = await engine.replay(executionId);
\end{verbatim}

\subsection*{Minimal Example}

\begin{verbatim}
import { DurableWorkflowEngine } from '@unrdf/yawl-durable';

const engine = new DurableWorkflowEngine();

await engine.defineWorkflow({
  id: 'booking-saga',
  name: 'Travel Booking',
  activities: [
    {
      id: 'bookFlight',
      handler: async (input) => {
        const res = await fetch('https://api.flights/book', { body: input });
        return res.json();
      },
      compensate: async (result) => {
        await fetch(`https://api.flights/cancel/${result.bookingId}`, { method: 'DELETE' });
      }
    },
    {
      id: 'bookHotel',
      handler: async (input) => {
        const res = await fetch('https://api.hotels/book', { body: input });
        return res.json();
      },
      compensate: async (result) => {
        await fetch(`https://api.hotels/cancel/${result.bookingId}`, { method: 'DELETE' });
      }
    }
  ],
  flow: [{ from: 'bookFlight', to: 'bookHotel' }]
});

const execution = await engine.startWorkflow('booking-saga', { userId: '123' });
const flightResult = await engine.executeActivity(execution.executionId, 'bookFlight', {});

// Replay from receipts
const replayed = await engine.replay(execution.executionId);
console.log(replayed.state); // Reconstructed state from receipt chain
\end{verbatim}

\subsection*{Open Questions}

\begin{enumerate}
\item How to handle non-deterministic activity results during replay?
\item What is the storage overhead of maintaining full receipt chains for long-running workflows?
\item Can side-effecting memoization improve replay performance?
\end{enumerate}

% =============================================================================

\label{pkg:unrdf-yawl-kafka}
\section{\pkg{unrdf-yawl-kafka} --- Event Streaming Integration}

\begin{pkgmeta}
Path & \texttt{packages/yawl-kafka} \\
Kind & js \\
Entrypoints & 4 files \\
Dependencies & 4 \\
Blurb & Apache Kafka event streaming integration for YAWL workflows with Avro serialization \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\textbf{Observable \(\Oobs\)}:
\begin{itemize}
\item Kafka producer send confirmations (partition, offset, timestamp)
\item Consumer group rebalances
\item Avro serialization/deserialization events
\item Topic partition lag metrics
\end{itemize}

\textbf{Artifact \(\Aout\)}:
\begin{itemize}
\item Kafka messages with Avro-encoded YAWL receipts
\item Schema registry entries for event types
\item Consumer offset commits
\item Dead letter queue messages (failed deserialization)
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

\begin{verbatim}
YAWLKafkaProducer :: {
  connect: () -> Promise<void>,
  sendReceipt: (Receipt) -> Promise<{ partition: number, offset: string }>,
  sendEvent: (EventType, Data) -> Promise<RecordMetadata>,
  disconnect: () -> Promise<void>
}

YAWLKafkaConsumer :: {
  connect: () -> Promise<void>,
  subscribe: (Topics) -> void,
  run: (Handler) -> Promise<void>,
  disconnect: () -> Promise<void>
}

// Avro schemas for event types
ReceiptAvroSchema = {
  type: 'record',
  name: 'YAWLReceipt',
  fields: [
    { name: 'receiptHash', type: 'string' },
    { name: 'previousReceiptHash', type: ['null', 'string'] },
    { name: 'eventType', type: 'string' },
    { name: 'timestamp', type: 'long' },
    { name: 'data', type: { type: 'map', values: 'string' } }
  ]
}
\end{verbatim}

\subsection*{Reconciler \(\muRecon\)}

Event streaming reconciliation:
\begin{enumerate}
\item \textbf{Producer}: YAWL engine emits receipts to Kafka topic on state changes
\item \textbf{Serialization}: Convert receipt objects to Avro binary format
\item \textbf{Consumer}: External systems (analytics, monitoring) consume events
\item \textbf{Deserialization}: Avro binary to receipt objects using schema registry
\end{enumerate}

At-least-once delivery composition rules (Kafka guarantees).

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Horizontal: Producer and consumer as independent processes, composed via Kafka topic.

Vertical: Integrates with \texttt{@unrdf/yawl} via event emission hooks.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item Schema compatibility checks before producing
\item Consumer group rebalance handling
\item Message retry with exponential backoff
\end{itemize}

Invariants:
\begin{itemize}
\item Message order preserved within partition
\item No message loss (durability via replication factor)
\item Schema evolution backward compatible
\end{itemize}

\subsection*{Provenance and Receipts}

Kafka messages contain full receipt provenance:
\begin{verbatim}
Topic: yawl-workflow-events
Key: case-123
Value (Avro):
{
  "receiptHash": "abc123...",
  "previousReceiptHash": "def456...",
  "eventType": "TASK_COMPLETED",
  "timestamp": 1640000000000000000,
  "data": { "caseId": "case-123", "taskId": "approve", "output": {...} }
}
\end{verbatim}

\subsection*{Minimal Example}

\begin{verbatim}
import { createYAWLKafkaProducer, createYAWLKafkaConsumer } from '@unrdf/yawl-kafka';

// Producer
const producer = createYAWLKafkaProducer({
  brokers: ['localhost:9092'],
  clientId: 'yawl-producer'
});

await producer.connect();
await producer.sendReceipt({
  receiptHash: 'abc123...',
  eventType: 'CASE_CREATED',
  timestamp: Date.now() * 1e6,
  data: { caseId: 'case-1' }
});

// Consumer
const consumer = createYAWLKafkaConsumer({
  brokers: ['localhost:9092'],
  groupId: 'analytics-group'
});

await consumer.connect();
consumer.subscribe(['yawl-workflow-events']);

await consumer.run(async ({ message }) => {
  const receipt = JSON.parse(message.value);
  console.log(`Received: ${receipt.eventType}`);
});
\end{verbatim}

\subsection*{Open Questions}

\begin{enumerate}
\item How to handle schema evolution when adding new receipt fields?
\item What partition strategy optimizes for both throughput and ordering guarantees?
\item Can Kafka Streams enable real-time workflow analytics?
\end{enumerate}

% =============================================================================

\label{pkg:unrdf-yawl-langchain}
\section{\pkg{unrdf-yawl-langchain} --- LangChain Integration}

\begin{pkgmeta}
Path & \texttt{packages/yawl-langchain} \\
Kind & js \\
Entrypoints & 3 files \\
Dependencies & 6 \\
Blurb & LangChain integration for YAWL workflow engine - AI-powered workflow orchestration with RDF context \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\textbf{Observable \(\Oobs\)}:
\begin{itemize}
\item LLM API calls (OpenAI, Anthropic) with prompts and responses
\item Chain execution traces (tool calls, reasoning steps)
\item RDF context retrieval from Oxigraph store
\item Agent decision logs
\end{itemize}

\textbf{Artifact \(\Aout\)}:
\begin{itemize}
\item LangChain chain definitions (sequential, routing, map-reduce)
\item Task execution hooks with prompt templates
\item RDF triples injected as context for LLM reasoning
\item Agent tool call results
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

\begin{verbatim}
YAWLLangChainAdapter :: {
  createTaskExecutor: (TaskDef, LLMConfig) -> TaskExecutor,
  executeWithContext: (TaskId, Input, RDFContext) -> Promise<Output>,
  createPromptHook: (Template) -> Hook
}

createLangChainTaskExecutor :: (config: {
  taskId: string,
  llm: ChatOpenAI | ChatAnthropic,
  promptTemplate: PromptTemplate,
  tools?: Tool[],
  rdfContextQuery?: string  // SPARQL query for RDF context
}) -> AsyncFunction

LangChainTaskConfigSchema = z.object({
  taskId: z.string(),
  model: z.enum(['gpt-4', 'claude-3-opus', ...]),
  temperature: z.number().min(0).max(2).default(0.7),
  promptTemplate: z.string(),
  outputParser: z.enum(['json', 'text', 'structured']).default('text'),
  maxTokens: z.number().optional()
})
\end{verbatim}

\subsection*{Reconciler \(\muRecon\)}

AI-workflow reconciliation:
\begin{enumerate}
\item \textbf{RDF context injection}: SPARQL query extracts relevant triples, serialized as Turtle for LLM prompt
\item \textbf{Task execution}: LangChain chain invoked with workflow data + RDF context
\item \textbf{Result parsing}: LLM output structured via Zod schema, stored in YAWL case data
\item \textbf{Receipt generation}: LLM invocation logged in receipt with model name, token count
\end{enumerate}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Horizontal: LangChain chains (sequential, parallel, routing) composed with YAWL control flow patterns.

Vertical: LLM tasks integrated via \texttt{createLangChainTaskExecutor} adapter, RDF context from Oxigraph.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item API rate limit handling (exponential backoff)
\item Token budget enforcement (max\_tokens)
\item Output validation via Zod schema
\end{itemize}

Invariants:
\begin{itemize}
\item LLM calls are non-deterministic (receipts record actual output, not re-generated)
\item RDF context query deterministic (same result for same case state)
\end{itemize}

\subsection*{Provenance and Receipts}

LLM task receipts include full context:
\begin{verbatim}
{
  receiptHash: "abc123...",
  eventType: "LLM_TASK_COMPLETED",
  timestamp: 1640000000000000000,
  data: {
    taskId: "code-review",
    model: "gpt-4",
    prompt: "Review this code:\n...",
    completion: "This code has the following issues:\n...",
    tokensUsed: 1234,
    rdfContext: "<http://ex.org/pr/1> ex:diffSize 500 .",
    durationMs: 2345
  }
}
\end{verbatim}

\subsection*{Minimal Example}

\begin{verbatim}
import { createLangChainTaskExecutor } from '@unrdf/yawl-langchain';
import { createWorkflowEngine } from '@unrdf/yawl';
import { ChatOpenAI } from 'langchain/chat_models/openai';

const engine = createWorkflowEngine();

// Define workflow with AI task
engine.registerWorkflow({
  id: 'code-review-wf',
  tasks: [
    { id: 'review', name: 'AI Code Review' },
    { id: 'approve', name: 'Human Approval' }
  ],
  flows: [{ from: 'review', to: 'approve' }]
});

// Create LangChain executor
const reviewExecutor = createLangChainTaskExecutor({
  taskId: 'review',
  llm: new ChatOpenAI({ modelName: 'gpt-4', temperature: 0.2 }),
  promptTemplate: `
    Review the following code diff:
    {diff}

    RDF context:
    {rdfContext}

    Provide: 1) Issues found, 2) Severity, 3) Recommendations
  `,
  rdfContextQuery: `
    SELECT ?author ?fileCount WHERE {
      ?pr ex:author ?author ; ex:fileCount ?fileCount .
    }
  `
});

// Execute
const { case: caseObj } = await engine.createCase('code-review-wf', {
  diff: 'diff --git a/foo.js ...'
});

const result = await reviewExecutor(caseObj, { diff: caseObj.data.diff });
console.log(result); // { issues: [...], severity: 'medium', recommendations: [...] }
\end{verbatim}

\subsection*{Open Questions}

\begin{enumerate}
\item How to ensure reproducibility when LLM outputs are non-deterministic?
\item What is the cost/benefit tradeoff of RDF context injection vs RAG retrieval?
\item Can YAWL patterns guide LangChain agent tool selection?
\end{enumerate}

% =============================================================================

\label{pkg:unrdf-yawl-observability}
\section{\pkg{unrdf-yawl-observability} --- Workflow Observability Framework}

\begin{pkgmeta}
Path & \texttt{packages/yawl-observability} \\
Kind & js \\
Entrypoints & 4 files \\
Dependencies & 6 \\
Blurb & Workflow observability framework with Prometheus metrics and OpenTelemetry tracing for YAWL \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\textbf{Observable \(\Oobs\)}:
\begin{itemize}
\item Prometheus metrics (case\_created\_total, task\_duration\_seconds, etc.)
\item OpenTelemetry traces with spans for workflow operations
\item Service Level Indicators (SLI): availability, latency, error rate
\item Alert conditions (SLO violations, anomaly detection)
\end{itemize}

\textbf{Artifact \(\Aout\)}:
\begin{itemize}
\item Prometheus exposition format (/metrics endpoint)
\item OTLP trace exports to Jaeger/Zipkin
\item SLI snapshot JSON reports
\item Grafana dashboard JSON configurations
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

\begin{verbatim}
YAWLMetricsCollector :: {
  recordCaseCreated: (CaseId, WorkflowId) -> void,
  recordTaskStarted: (CaseId, TaskId) -> void,
  recordTaskCompleted: (CaseId, TaskId, DurationMs) -> void,
  getMetrics: () -> Promise<string>,  // Prometheus format
  contentType: string
}

YAWLTracer :: {
  startCaseSpan: (CaseId, WorkflowId) -> Span,
  startTaskSpan: (CaseId, TaskId, ParentSpan) -> Span,
  recordReceiptHash: (Span, ReceiptHash) -> void,
  endSpan: (Span) -> void
}

YAWLSLICalculator :: {
  recordRequest: (Success: boolean, LatencyMs: number) -> void,
  getSnapshot: () -> {
    availability: number,    // % successful requests
    latency: { p50, p95, p99 },
    errorRate: number,
    sloCompliance: { score, violations: string[] }
  }
}
\end{verbatim}

\subsection*{Reconciler \(\muRecon\)}

Observability reconciliation:
\begin{enumerate}
\item \textbf{Metric collection}: Engine emits events, metrics collector increments counters/histograms
\item \textbf{Trace propagation}: Span context attached to YAWL case, child spans for each task
\item \textbf{SLI calculation}: Rolling window aggregation (1m, 5m, 1h) for availability/latency
\item \textbf{Receipt correlation}: Trace spans annotated with receipt hashes for audit trail linkage
\end{enumerate}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Horizontal: Metrics, tracing, and SLI modules composed via event hooks.

Vertical: Wraps \texttt{@unrdf/yawl} engine with observability instrumentation.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item Metric cardinality limits (prevent label explosion)
\item Trace sampling (configurable rate to manage overhead)
\item SLI time window bounds (prevent unbounded memory growth)
\end{itemize}

Invariants:
\begin{itemize}
\item Metrics monotonically increasing (counters never decrease)
\item Trace spans properly closed (no leaks)
\item SLI snapshots immutable once generated
\end{itemize}

\subsection*{Provenance and Receipts}

Trace spans linked to receipts:
\begin{verbatim}
Span {
  name: "yawl.task.execute",
  attributes: {
    "yawl.case.id": "case-123",
    "yawl.task.id": "approve",
    "yawl.receipt.hash": "abc123...",
    "yawl.workflow.id": "purchase-order"
  },
  duration: 234ms
}

Receipt {
  receiptHash: "abc123...",
  traceId: "def456...",  // Link to OTLP trace
  spanId: "ghi789...",
  ...
}
\end{verbatim}

\subsection*{Minimal Example}

\begin{verbatim}
import { createWorkflowEngine } from '@unrdf/yawl';
import { YAWLMetricsCollector, YAWLTracer } from '@unrdf/yawl-observability';
import express from 'express';

const engine = createWorkflowEngine();
const metrics = new YAWLMetricsCollector(engine);
const tracer = new YAWLTracer(engine);

// Expose metrics endpoint
const app = express();
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', metrics.contentType);
  res.end(await metrics.getMetrics());
});
app.listen(9090);

// Execute workflow (auto-instrumented)
const { case: caseObj } = await engine.createCase('purchase-order', {});
// Metrics: yawl_case_created_total{workflow_id="purchase-order"} 1

const span = tracer.startCaseSpan(caseObj.id, 'purchase-order');
const { receipt } = await caseObj.startTask('task-1');
tracer.recordReceiptHash(span, receipt.receiptHash);
tracer.endSpan(span);
// Trace exported to Jaeger with receipt hash attribute
\end{verbatim}

\subsection*{Open Questions}

\begin{enumerate}
\item How to correlate traces across distributed workflow executions?
\item What is the performance overhead of full trace instrumentation?
\item Can SLO violations trigger automatic workflow compensation (circuit breaker pattern)?
\end{enumerate}

% =============================================================================
% End of Agent 9 Package Documentation
% =============================================================================
