% agent_6_packages.tex
% Agent 6 Documentation: Packages 19-24
% Validation Layer - Core validation, SHACL, SPARQL validation, Schema validation, Test utilities, OpenTelemetry

\label{pkg:unrdf-validation}
\section{\pkg{unrdf-validation} --- OTEL Validation Framework}

\begin{pkgmeta}
Path & \texttt{packages/validation} \\
Kind & js \\
Entrypoints & 1 file \\
Dependencies & 1 (@unrdf/knowledge-engine) \\
Blurb & OTEL validation framework for UNRDF development with receipts-backed quality scoring \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\begin{align*}
\Oobs &= \texttt{OTELSpans} \times \texttt{ValidationRules} \times \texttt{QualityGates} \\
\Aout &= \texttt{ValidationReport} \times \texttt{QualityScore} \times \texttt{Receipt}
\end{align*}

The observable is a collection of OpenTelemetry spans representing system execution traces, validation rule definitions, and quality gate thresholds. The artifact is a comprehensive validation report with numerical quality score (0-100) and cryptographic receipt proving validation execution.

Core components:
\begin{enumerate}
\item \textbf{OTELValidator}: Analyzes OTEL spans for code quality, testing coverage, security patterns
\item \textbf{ValidationRunner}: Orchestrates validation suite execution with timeout guards
\item \textbf{MetricsCollector}: Aggregates metrics from OTEL traces (latency, error rates, throughput)
\item \textbf{ValidationReceipts}: Generates tamper-proof receipts for validation results
\end{enumerate}

\subsection*{Type Signature \(\SigmaType\)}

Validation report schema:
\begin{lstlisting}
const ValidationReportSchema = z.object({
  timestamp: z.string().datetime(),
  score: z.number().min(0).max(100),
  categories: z.object({
    codeQuality: z.number().min(0).max(20),
    testing: z.number().min(0).max(25),
    security: z.number().min(0).max(20),
    dependencies: z.number().min(0).max(15),
    documentation: z.number().min(0).max(5),
    performance: z.number().min(0).max(10),
    otelCoverage: z.number().min(0).max(5)
  }),
  violations: z.array(z.object({
    category: z.string(),
    severity: z.enum(['error', 'warning', 'info']),
    message: z.string(),
    location: z.string().optional()
  })),
  metrics: z.object({
    totalSpans: z.number().int().nonnegative(),
    avgDuration: z.number().nonnegative(),
    errorRate: z.number().min(0).max(1)
  })
});
\end{lstlisting}

\subsection*{Reconciler \(\muRecon\)}

\[\muRecon_{\text{validate}}: \texttt{OTELSpans} \to (\texttt{QualityScore}, \texttt{Violations[]})\]

Validation reconciliation pipeline:
\begin{enumerate}
\item \textbf{Span Collection}: Gather OTEL spans from instrumented code execution
\item \textbf{Metric Extraction}: Parse span attributes for quality indicators
\item \textbf{Rule Evaluation}: Apply validation rules to collected metrics
\item \textbf{Score Computation}: Weighted sum across 7 categories (total 100 points)
\item \textbf{Receipt Generation}: Create cryptographic proof of validation state
\end{enumerate}

Quality gate enforcement:
\begin{lstlisting}
async function validateQualityGate(report, threshold = 80) {
  if (report.score < threshold) {
    return {
      passed: false,
      reason: `Quality score ${report.score} below threshold ${threshold}`,
      violations: report.violations.filter(v => v.severity === 'error')
    };
  }
  return { passed: true };
}
\end{lstlisting}

Validation runner executes comprehensive checks:
\begin{itemize}
\item Code Quality (20\%): JSDoc coverage, cyclomatic complexity, naming conventions
\item Testing (25\%): Test pass rate, coverage metrics (lines, branches, functions)
\item Security (20\%): Credential detection, injection patterns, path traversal
\item Dependencies (15\%): Circular dependencies, version vulnerabilities
\item Documentation (5\%): README presence, API documentation completeness
\item Performance (10\%): P95 latency targets, memory footprint, throughput
\item OTEL Coverage (5\%): Span density, trace completeness
\end{itemize}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Sequential validation composition:
\[\PiMerge(\texttt{validator}_1, \texttt{validator}_2) = \texttt{validator}_2 \compose \texttt{validator}_1\]

Validators execute in dependency order. Each validator receives OTEL spans and produces partial scores. Final score is weighted sum of category scores.

Parallel validation for independent checks:
\[\texttt{check}_1 \oplusMerge \texttt{check}_2 \iff \neg\texttt{dependent}(\texttt{check}_1, \texttt{check}_2)\]

Independent checks (e.g., JSDoc coverage and test coverage) run concurrently, merging results via score aggregation.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item \(\GuardH_{\text{timeout}}\): Abort validation exceeding 30-second budget
\item \(\GuardH_{\text{threshold}}\): Block deployment if score \textless 80/100
\item \(\GuardH_{\text{error}}\): Fail fast on critical security violations
\end{itemize}

Invariants:
\begin{itemize}
\item \(\InvQ_{\text{score}}\): \(0 \leq \texttt{score} \leq 100\) (bounded quality metric)
\item \(\InvQ_{\text{deterministic}}\): Same OTEL spans produce identical score
\item \(\InvQ_{\text{receipt}}\): Every validation produces cryptographic receipt
\item \(\InvQ_{\text{comprehensive}}\): All 7 categories evaluated (no partial reports)
\end{itemize}

\subsection*{Provenance and Receipts}

Validation receipt structure:
\begin{lstlisting}
{
  "id": "validation-550e8400-e29b-41d4-a716",
  "timestamp": "2024-12-04T15:16:00.123Z",
  "score": 87.3,
  "spans_hash": "blake3:a3f8d...",
  "rules_version": "5.0.1",
  "violations_count": 3,
  "categories": {
    "codeQuality": 18.5,
    "testing": 24.0,
    "security": 20.0,
    "dependencies": 13.5,
    "documentation": 4.0,
    "performance": 9.8,
    "otelCoverage": 4.5
  }
}
\end{lstlisting}

Receipt chain links validation runs:
\[\texttt{receipt}_n.\texttt{previous} = \ProvHash(\texttt{receipt}_{n-1})\]

\subsection*{Minimal Example}

\begin{lstlisting}
import { createValidationRunner, createOTELValidator }
  from '@unrdf/validation';

const runner = createValidationRunner({
  threshold: 80,
  timeout: 30000
});

const validator = createOTELValidator();

// Run comprehensive validation
const report = await runner.validate({
  spans: otelSpans,
  rules: defaultValidationRules
});

console.log(`Quality Score: ${report.score}/100`);
console.log(`Violations: ${report.violations.length}`);

if (report.score >= 80) {
  console.log('Quality gate passed');
} else {
  console.error('Quality gate failed');
  process.exit(1);
}
\end{lstlisting}

\subsection*{Open Questions}

\begin{enumerate}
\item How to extend validation rules dynamically without recompiling validator?
\item Can validation receipts be anchored to blockchain for external auditability?
\item Optimal category weights for different project types (library vs application)?
\item Real-time validation streaming for continuous quality monitoring?
\end{enumerate}

% ============================================================================

\label{pkg:unrdf-shacl}
\section{\pkg{unrdf-shacl} --- SHACL Shape Validation}

\begin{pkgmeta}
Path & \texttt{packages/core/src/validation + v6-core/grammar/parser} \\
Kind & js \\
Entrypoints & 2 files \\
Dependencies & 3 (rdf-validate-shacl, @unrdf/oxigraph, zod) \\
Blurb & SHACL (Shapes Constraint Language) validation engine with complexity estimation \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\begin{align*}
\Oobs &= \texttt{DataGraph} \times \texttt{ShapesGraph} \\
\Aout &= \texttt{ValidationReport} \times \texttt{Violations[]} \times \texttt{Conforms}
\end{align*}

The observable consists of an RDF data graph to be validated and a SHACL shapes graph defining constraints. The artifact is a W3C SHACL validation report indicating conformance, with detailed violation traces for non-conforming nodes.

SHACL validation enforces structural constraints:
\begin{itemize}
\item \textbf{Node Shapes}: Constraints on RDF resources (e.g., \texttt{sh:class}, \texttt{sh:datatype})
\item \textbf{Property Shapes}: Constraints on property values (e.g., \texttt{sh:minCount}, \texttt{sh:pattern})
\item \textbf{Target Selection}: \texttt{sh:targetClass}, \texttt{sh:targetNode}, \texttt{sh:targetSubjectsOf}
\item \textbf{Severity Levels}: \texttt{sh:Violation}, \texttt{sh:Warning}, \texttt{sh:Info}
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

SHACL validation report schema:
\begin{lstlisting}
const SHACLReportSchema = z.object({
  conforms: z.boolean(),
  results: z.array(z.object({
    sourceConstraintComponent: z.string(),
    focusNode: z.string(),
    resultPath: z.string().optional(),
    resultSeverity: z.enum(['Violation', 'Warning', 'Info']),
    resultMessage: z.string(),
    sourceShape: z.string()
  })),
  metadata: z.object({
    shapesCount: z.number().int().nonnegative(),
    targetsCount: z.number().int().nonnegative(),
    validationTimeMs: z.number().nonnegative()
  })
});
\end{lstlisting}

SHACL shape AST (from v6-core parser):
\begin{lstlisting}
const SHACLASTSchema = z.object({
  type: z.literal('shacl'),
  shapes: z.array(z.object({
    id: z.string(),
    targetClass: z.string().optional(),
    properties: z.array(z.object({
      path: z.string(),
      constraints: z.record(z.any())
    }))
  })),
  complexity: z.object({
    estimatedTimeMs: z.number(),
    shapesDepth: z.number(),
    astNodeCount: z.number()
  })
});
\end{lstlisting}

\subsection*{Reconciler \(\muRecon\)}

\[\muRecon_{\text{shacl}}: (\texttt{DataGraph}, \texttt{ShapesGraph}) \to \texttt{ValidationReport}\]

SHACL validation algorithm:
\begin{enumerate}
\item \textbf{Parse Shapes}: Extract node shapes and property shapes from shapes graph
\item \textbf{Target Selection}: Identify focus nodes using target declarations
\item \textbf{Constraint Evaluation}: For each focus node, evaluate all applicable constraints
\item \textbf{Report Generation}: Aggregate violations into W3C SHACL validation report
\end{enumerate}

Complexity estimation (from v6-core parser):
\begin{lstlisting}
function estimateSHACLComplexity(ast) {
  const shapes = ast.shapes?.length || 0;
  const properties = ast.properties?.length || 0;

  // SHACL validation is O(nodes * shapes)
  const estimatedTimeMs = Math.min(
    200 + (shapes * 20) + (properties * 30),
    10000 // 10s timeout
  );

  return {
    estimatedTimeMs,
    shapesDepth: shapes,
    astNodeCount: countASTNodes(ast)
  };
}
\end{lstlisting}

Integration with \texttt{rdf-validate-shacl} library:
\begin{lstlisting}
import factory from 'rdf-ext';
import SHACLValidator from 'rdf-validate-shacl';

async function validateSHACL(dataGraph, shapesGraph) {
  const validator = new SHACLValidator(shapesGraph, { factory });
  const report = await validator.validate(dataGraph);

  return {
    conforms: report.conforms,
    results: report.results.map(r => ({
      focusNode: r.focusNode.value,
      resultPath: r.path?.value,
      resultMessage: r.message?.[0]?.value,
      sourceShape: r.sourceShape.value
    }))
  };
}
\end{lstlisting}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Sequential shape validation:
\[\PiMerge(\texttt{shape}_1, \texttt{shape}_2) = \texttt{validateAll}([\texttt{shape}_1, \texttt{shape}_2])\]

All shapes in shapes graph are evaluated. Order does not affect conformance (shapes are declarative constraints).

Data graph composition:
\[\texttt{data}_1 \oplusMerge \texttt{data}_2 = \texttt{validate}(\texttt{data}_1 \cup \texttt{data}_2, \texttt{shapes})\]

Validation over union of data graphs. Adding triples can introduce new violations (non-monotonic).

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item \(\GuardH_{\text{timeout}}\): Abort validation exceeding 10-second estimate
\item \(\GuardH_{\text{cycle}}\): Detect cyclic shape references (prevent infinite loops)
\item \(\GuardH_{\text{wellformed}}\): Reject malformed SHACL shapes (invalid constraint components)
\end{itemize}

Invariants:
\begin{itemize}
\item \(\InvQ_{\text{w3c}}\): Validation report conforms to W3C SHACL spec
\item \(\InvQ_{\text{deterministic}}\): Same data and shapes produce identical report
\item \(\InvQ_{\text{complete}}\): All focus nodes evaluated for all applicable shapes
\item \(\InvQ_{\text{severity}}\): Violations have higher severity than warnings
\end{itemize}

\subsection*{Provenance and Receipts}

SHACL validation produces W3C-compliant reports with full provenance:
\begin{lstlisting}
{
  "conforms": false,
  "results": [
    {
      "focusNode": "http://example.org/Alice",
      "resultPath": "http://schema.org/age",
      "resultSeverity": "Violation",
      "resultMessage": "Value must be >= 0",
      "sourceConstraintComponent": "sh:MinInclusiveConstraintComponent",
      "sourceShape": "http://example.org/PersonShape"
    }
  ],
  "metadata": {
    "shapesCount": 5,
    "targetsCount": 12,
    "validationTimeMs": 234
  }
}
\end{lstlisting}

Shapes graph hash provides deterministic versioning:
\[\texttt{shapes\_version} = \ProvHash(\texttt{sorted}(\texttt{shapesGraph}))\]

\subsection*{Minimal Example}

\begin{lstlisting}
import { createStore } from '@unrdf/oxigraph';
import SHACLValidator from 'rdf-validate-shacl';
import factory from 'rdf-ext';

const dataGraph = createStore();
dataGraph.add(/* RDF quads for Person data */);

const shapesGraph = createStore();
shapesGraph.add(/* SHACL shapes for Person validation */);

const validator = new SHACLValidator(shapesGraph, { factory });
const report = await validator.validate(dataGraph);

if (report.conforms) {
  console.log('Data conforms to shapes');
} else {
  console.log('Validation failures:');
  for (const result of report.results) {
    console.log(`- ${result.message}`);
  }
}
\end{lstlisting}

\subsection*{Open Questions}

\begin{enumerate}
\item How to optimize SHACL validation for large graphs (>1M triples)?
\item Incremental validation on delta updates (avoid full re-validation)?
\item SHACL-to-Zod compilation for static type checking?
\item Distributed SHACL validation across federated stores?
\end{enumerate}

% ============================================================================

\label{pkg:unrdf-sparql-validator}
\section{\pkg{unrdf-sparql-validator} --- SPARQL Query Validation}

\begin{pkgmeta}
Path & \texttt{packages/v6-core/src/grammar/parser + packages/core/src/sparql} \\
Kind & js \\
Entrypoints & 2 files \\
Dependencies & 2 (@unrdf/oxigraph, zod) \\
Blurb & SPARQL query parser, validator, and complexity estimator for query optimization \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\begin{align*}
\Oobs &= \texttt{SPARQLQuery} \times \texttt{ValidationRules} \\
\Aout &= \texttt{QueryAST} \times \texttt{Complexity} \times \texttt{SafetyReport}
\end{align*}

The observable is a SPARQL query string with validation rules (e.g., timeout limits, complexity bounds). The artifact is a parsed Abstract Syntax Tree (AST), complexity estimate, and safety analysis report flagging potential performance issues or security vulnerabilities.

SPARQL validation checks:
\begin{itemize}
\item \textbf{Syntax Validation}: Parse query to AST, reject malformed queries
\item \textbf{Complexity Estimation}: Estimate execution time based on join depth and filters
\item \textbf{Safety Analysis}: Detect unbounded queries, Cartesian products, regex DoS
\item \textbf{Security Checks}: Prevent injection attacks, enforce query size limits
\end{itemize}

\subsection*{Type Signature \(\SigmaType\)}

SPARQL AST schema:
\begin{lstlisting}
const SPARQLASTSchema = z.object({
  type: z.literal('sparql'),
  queryType: z.enum(['SELECT', 'CONSTRUCT', 'ASK', 'DESCRIBE']),
  prefixes: z.record(z.string()),
  patterns: z.array(z.object({
    pattern: z.string(),
    variables: z.array(z.string()).optional()
  })),
  filters: z.array(z.string()),
  modifiers: z.object({
    limit: z.number().int().positive().optional(),
    offset: z.number().int().nonnegative().optional(),
    hasOrderBy: z.boolean(),
    hasGroupBy: z.boolean()
  }),
  complexity: z.object({
    estimatedTimeMs: z.number(),
    triplePatterns: z.number(),
    joinDepth: z.number(),
    filterComplexity: z.number()
  })
});
\end{lstlisting}

Safety report schema:
\begin{lstlisting}
const SafetyReportSchema = z.object({
  safe: z.boolean(),
  warnings: z.array(z.object({
    severity: z.enum(['high', 'medium', 'low']),
    category: z.enum(['complexity', 'security', 'performance']),
    message: z.string(),
    suggestion: z.string().optional()
  })),
  metrics: z.object({
    estimatedTimeMs: z.number(),
    unboundedPatterns: z.number(),
    cartesianProducts: z.number()
  })
});
\end{lstlisting}

\subsection*{Reconciler \(\muRecon\)}

\[\muRecon_{\text{parse}}: \texttt{SPARQLQuery} \to (\texttt{AST}, \texttt{Complexity})\]

SPARQL parsing and validation pipeline:
\begin{enumerate}
\item \textbf{Lexical Analysis}: Tokenize SPARQL query string
\item \textbf{Syntax Parsing}: Build AST from tokens (query type, patterns, filters)
\item \textbf{Complexity Estimation}: Analyze AST to estimate execution cost
\item \textbf{Safety Checks}: Detect performance hazards and security issues
\end{enumerate}

Complexity estimation algorithm:
\begin{lstlisting}
function estimateSPARQLComplexity(ast) {
  const triplePatterns = ast.patterns?.length || 0;
  const filters = ast.filters?.length || 0;
  const joinDepth = estimateJoinDepth(ast.patterns);

  // O(n^joinDepth) worst case for joins
  const estimatedTimeMs = Math.min(
    100 + (triplePatterns * 10) + (filters * 50) + (joinDepth * 100),
    5000 // 5s default timeout from CLAUDE.md
  );

  return {
    estimatedTimeMs,
    triplePatterns,
    joinDepth,
    filterComplexity: filters,
    astNodeCount: countASTNodes(ast)
  };
}
\end{lstlisting}

Safety analysis:
\begin{lstlisting}
function analyzeSafety(ast) {
  const warnings = [];

  // Check for unbounded queries (no LIMIT)
  if (!ast.modifiers.limit && ast.queryType === 'SELECT') {
    warnings.push({
      severity: 'medium',
      category: 'performance',
      message: 'Query has no LIMIT clause',
      suggestion: 'Add LIMIT to prevent unbounded result sets'
    });
  }

  // Check for Cartesian products (unconnected patterns)
  const cartesian = detectCartesianProducts(ast.patterns);
  if (cartesian > 0) {
    warnings.push({
      severity: 'high',
      category: 'performance',
      message: `Detected ${cartesian} Cartesian product(s)`,
      suggestion: 'Add join conditions between patterns'
    });
  }

  // Check for regex DoS (complex FILTER regex)
  for (const filter of ast.filters) {
    if (/regex.*\(.*\*.*\*.*\)/i.test(filter)) {
      warnings.push({
        severity: 'high',
        category: 'security',
        message: 'Complex regex in FILTER may cause DoS',
        suggestion: 'Simplify regex pattern or use string functions'
      });
    }
  }

  return {
    safe: warnings.filter(w => w.severity === 'high').length === 0,
    warnings
  };
}
\end{lstlisting}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Query federation (sequential execution):
\[\PiMerge(q_1, q_2) = q_2(q_1(\texttt{store}))\]

Federated queries execute sequentially, with second query operating on results of first.

Complexity composition (additive for independent queries):
\[\texttt{complexity}(q_1 \oplusMerge q_2) = \texttt{complexity}(q_1) + \texttt{complexity}(q_2)\]

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item \(\GuardH_{\text{timeout}}\): Reject queries exceeding 5-second estimate
\item \(\GuardH_{\text{syntax}}\): Block malformed queries at parse stage
\item \(\GuardH_{\text{injection}}\): Prevent SPARQL injection via parameterized queries
\item \(\GuardH_{\text{size}}\): Enforce maximum query size (10KB default)
\end{itemize}

Invariants:
\begin{itemize}
\item \(\InvQ_{\text{deterministic}}\): Same query produces identical AST
\item \(\InvQ_{\text{reversible}}\): AST can be serialized back to valid SPARQL
\item \(\InvQ_{\text{complete}}\): All SPARQL 1.1 features supported in parser
\item \(\InvQ_{\text{bounded}}\): Complexity estimate never exceeds configured max
\end{itemize}

\subsection*{Provenance and Receipts}

SPARQL validation receipt:
\begin{lstlisting}
{
  "queryHash": "blake3:a3f8d...",
  "parseTimestamp": "2024-12-04T15:16:00.123Z",
  "queryType": "SELECT",
  "complexity": {
    "estimatedTimeMs": 234,
    "triplePatterns": 5,
    "joinDepth": 3,
    "filterComplexity": 2
  },
  "safety": {
    "safe": true,
    "warnings": []
  },
  "validated": true
}
\end{lstlisting}

Query fingerprint enables caching:
\[\texttt{queryFingerprint} = \ProvHash(\texttt{normalizedQuery})\]

\subsection*{Minimal Example}

\begin{lstlisting}
import { parseSPARQL, estimateSPARQLComplexity }
  from '@unrdf/v6-core/grammar/parser/sparql';

const query = `
  PREFIX ex: <http://example.org/>
  SELECT ?person ?name WHERE {
    ?person a ex:Person .
    ?person ex:name ?name .
    FILTER(?name != "")
  }
  LIMIT 100
`;

const { ast, complexity } = parseSPARQL(query);

console.log(`Query type: ${ast.queryType}`);
console.log(`Estimated time: ${complexity.estimatedTimeMs}ms`);
console.log(`Join depth: ${complexity.joinDepth}`);

if (complexity.estimatedTimeMs > 5000) {
  console.warn('Query exceeds 5s timeout');
}
\end{lstlisting}

\subsection*{Open Questions}

\begin{enumerate}
\item How to accurately estimate complexity for property paths and aggregations?
\item Can ML models predict query execution time better than rule-based estimators?
\item Automatic query rewriting to optimize detected anti-patterns?
\item Integration with query planner for adaptive timeout adjustment?
\end{enumerate}

% ============================================================================

\label{pkg:unrdf-schema-validator}
\section{\pkg{unrdf-schema-validator} --- Zod Schema Validation}

\begin{pkgmeta}
Path & \texttt{packages/core/src/validation} \\
Kind & js \\
Entrypoints & 1 file \\
Dependencies & 2 (@unrdf/oxigraph, zod) \\
Blurb & Runtime schema validation for RDF quads, stores, and query options using Zod \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\begin{align*}
\Oobs &= \texttt{Value} \times \texttt{ZodSchema} \\
\Aout &= \texttt{ValidatedValue} \lor \texttt{ValidationError}
\end{align*}

The observable is an arbitrary JavaScript value paired with a Zod schema defining its expected structure. The artifact is either the validated value (with type refinements applied) or a structured validation error with detailed issue descriptions.

Core schemas:
\begin{enumerate}
\item \textbf{QuadSchema}: Validates RDF quad structure (subject, predicate, object, graph)
\item \textbf{StoreSchema}: Validates triple store interface (getQuads, addQuad, etc.)
\item \textbf{QueryOptionsSchema}: Validates SPARQL query options (limit, offset, signal)
\end{enumerate}

\subsection*{Type Signature \(\SigmaType\)}

Quad validation schema:
\begin{lstlisting}
const QuadSchema = z.object({
  subject: z.object({
    value: z.string(),
    termType: z.enum(['NamedNode', 'BlankNode', 'Variable'])
  }),
  predicate: z.object({
    value: z.string(),
    termType: z.literal('NamedNode')
  }),
  object: z.object({
    value: z.string(),
    termType: z.enum(['NamedNode', 'BlankNode', 'Literal', 'Variable'])
  }),
  graph: z.object({
    value: z.string(),
    termType: z.enum(['NamedNode', 'DefaultGraph'])
  }).optional()
});
\end{lstlisting}

Store interface schema:
\begin{lstlisting}
const StoreSchema = z.object({
  getQuads: z.function()
    .args(
      z.any().optional(), // subject
      z.any().optional(), // predicate
      z.any().optional(), // object
      z.any().optional()  // graph
    )
    .returns(z.any()),
  addQuad: z.function().optional(),
  removeQuad: z.function().optional(),
  countQuads: z.function().optional()
});
\end{lstlisting}

Query options schema:
\begin{lstlisting}
const QueryOptionsSchema = z.object({
  limit: z.number().int().positive().optional(),
  offset: z.number().int().nonnegative().optional(),
  signal: z.instanceof(AbortSignal).optional()
}).optional();
\end{lstlisting}

\subsection*{Reconciler \(\muRecon\)}

\[\muRecon_{\text{validate}}: (\texttt{Value}, \texttt{Schema}) \to \texttt{ValidatedValue} \lor \texttt{Error}\]

Validation reconciliation strategies:

\textbf{Strict Validation} (throws on invalid):
\begin{lstlisting}
function validateQuad(quad) {
  return QuadSchema.parse(quad); // Throws ZodError if invalid
}
\end{lstlisting}

\textbf{Safe Validation} (returns result object):
\begin{lstlisting}
function safeValidateQuad(quad) {
  const result = QuadSchema.safeParse(quad);
  if (result.success) {
    return { valid: true, data: result.data };
  } else {
    return { valid: false, errors: result.error.issues };
  }
}
\end{lstlisting}

Custom refinements for RDF-specific constraints:
\begin{lstlisting}
const IRIQuadSchema = QuadSchema.refine(
  quad => quad.subject.termType !== 'BlankNode',
  { message: 'Subject must be a named node (no blank nodes)' }
);

const ValidLiteralSchema = z.object({
  value: z.string(),
  termType: z.literal('Literal'),
  datatype: z.object({ value: z.string() }).optional(),
  language: z.string().regex(/^[a-z]{2,3}(-[A-Z]{2})?$/).optional()
});
\end{lstlisting}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Schema composition (intersection):
\[\texttt{schema}_1 \oplusMerge \texttt{schema}_2 = \texttt{schema}_1.\texttt{and}(\texttt{schema}_2)\]

Zod intersection combines constraints:
\begin{lstlisting}
const ExtendedQuadSchema = QuadSchema.and(z.object({
  metadata: z.object({
    source: z.string(),
    timestamp: z.number()
  }).optional()
}));
\end{lstlisting}

Schema refinement (sequential):
\[\PiMerge(\texttt{schema}_1, \texttt{refine}) = \texttt{schema}_1.\texttt{refine}(\texttt{predicate})\]

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item \(\GuardH_{\text{type}}\): Reject values with incorrect TypeScript types
\item \(\GuardH_{\text{format}}\): Validate IRI format, language tags (BCP47)
\item \(\GuardH_{\text{range}}\): Enforce numeric bounds (limit \textgreater 0, offset \(\geq\) 0)
\end{itemize}

Invariants:
\begin{itemize}
\item \(\InvQ_{\text{parse-safe}}\): Validated values are type-safe (no runtime errors)
\item \(\InvQ_{\text{deterministic}}\): Same value + schema produces same result
\item \(\InvQ_{\text{informative}}\): Errors include path and expected type
\item \(\InvQ_{\text{reversible}}\): Validated values serialize back to original structure
\end{itemize}

\subsection*{Provenance and Receipts}

Validation error structure provides full provenance:
\begin{lstlisting}
{
  "issues": [
    {
      "code": "invalid_type",
      "expected": "string",
      "received": "number",
      "path": ["subject", "value"],
      "message": "Expected string, received number"
    }
  ],
  "name": "ZodError"
}
\end{lstlisting}

Schema fingerprint for versioning:
\[\texttt{schemaVersion} = \ProvHash(\texttt{schema.shape})\]

\subsection*{Minimal Example}

\begin{lstlisting}
import { QuadSchema, validateQuad, validateStore }
  from '@unrdf/core/validation';
import { dataFactory, createStore } from '@unrdf/oxigraph';

// Validate quad
const quad = dataFactory.quad(
  dataFactory.namedNode('http://example.org/Alice'),
  dataFactory.namedNode('http://schema.org/name'),
  dataFactory.literal('Alice')
);

const isValid = validateQuad(quad); // Returns true

// Validate store
const store = createStore();
const storeValid = validateStore(store); // Returns true

// Safe validation with error handling
const result = QuadSchema.safeParse(quad);
if (result.success) {
  console.log('Valid quad:', result.data);
} else {
  console.error('Validation errors:', result.error.issues);
}
\end{lstlisting}

\subsection*{Open Questions}

\begin{enumerate}
\item How to generate Zod schemas from SHACL shapes automatically?
\item Runtime schema migration when API contracts evolve?
\item Performance optimization for high-throughput validation (schema caching)?
\item Integration with TypeScript for compile-time + runtime validation?
\end{enumerate}

% ============================================================================

\label{pkg:unrdf-test-utils}
\section{\pkg{unrdf-test-utils} --- Testing Utilities and Helpers}

\begin{pkgmeta}
Path & \texttt{packages/test-utils} \\
Kind & js \\
Entrypoints & 3 files \\
Dependencies & 3 (@unrdf/oxigraph, @opentelemetry/api, zod) \\
Blurb & Comprehensive testing utilities with scenario DSL, fluent assertions, and helper functions \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\begin{align*}
\Oobs &= \texttt{TestScenario} \times \texttt{TestContext} \\
\Aout &= \texttt{TestResult} \times \texttt{Assertions[]} \times \texttt{Duration}
\end{align*}

The observable is a test scenario definition with setup/teardown functions, action steps, and assertion predicates, paired with a test context containing RDF store and knowledge hook manager. The artifact is a structured test result with pass/fail status, assertion outcomes, and execution duration.

Core components:
\begin{enumerate}
\item \textbf{TestScenario}: Builder pattern for defining multi-step test scenarios
\item \textbf{FluentAssertions}: Chainable assertions for RDF stores and receipts
\item \textbf{TestContextBuilder}: Fluent API for constructing test contexts
\item \textbf{TestHelpers}: Utility functions for creating quads, deltas, hooks
\end{enumerate}

\subsection*{Type Signature \(\SigmaType\)}

Test scenario schema:
\begin{lstlisting}
const TestScenarioSchema = z.object({
  name: z.string().min(1),
  description: z.string().optional(),
  setup: z.function().optional().nullable(),
  teardown: z.function().optional().nullable(),
  steps: z.array(z.object({
    name: z.string().min(1),
    action: z.function(),
    assertions: z.array(z.function()).optional()
  })).min(1)
});
\end{lstlisting}

Test result schema:
\begin{lstlisting}
const TestResultSchema = z.object({
  name: z.string(),
  description: z.string(),
  success: z.boolean(),
  steps: z.array(z.object({
    name: z.string(),
    success: z.boolean(),
    error: z.string().nullable(),
    duration: z.number()
  })),
  errors: z.array(z.string()),
  duration: z.number()
});
\end{lstlisting}

Test context schema:
\begin{lstlisting}
const TestContextSchema = z.object({
  store: z.any(), // RDF Store instance
  manager: z.any(), // KnowledgeHookManager
  policyPackManager: z.any().optional(),
  lockchainWriter: z.any().optional(),
  sandbox: z.any().optional(),
  metadata: z.record(z.any()).optional()
});
\end{lstlisting}

\subsection*{Reconciler \(\muRecon\)}

\[\muRecon_{\text{scenario}}: \texttt{TestScenario} \to \texttt{TestResult}\]

Test scenario execution pipeline:
\begin{enumerate}
\item \textbf{Validation}: Validate scenario against TestScenarioSchema
\item \textbf{Setup}: Execute setup function, create test context
\item \textbf{Step Execution}: Run each step's action and assertions sequentially
\item \textbf{Teardown}: Clean up resources, restore pre-test state
\item \textbf{Reporting}: Aggregate results, compute total duration
\end{enumerate}

Scenario builder pattern:
\begin{lstlisting}
const scenario = new TestScenario('Hook Execution')
  .setupScenario(async () => {
    const store = createStore();
    const manager = new KnowledgeHookManager();
    return { store, manager };
  })
  .step('Register hook', async (ctx) => {
    ctx.manager.register(myHook);
  })
  .step('Execute hook', async (ctx) => {
    return await ctx.manager.execute(delta);
  }, [
    async (ctx, result) => {
      if (!result.receipt.committed) {
        throw new Error('Expected receipt to be committed');
      }
    }
  ])
  .teardownScenario(async (ctx) => {
    ctx.store.clear();
  });

const result = await scenario.execute();
\end{lstlisting}

Fluent assertions API:
\begin{lstlisting}
class FluentAssertions {
  toBeCommitted() { /* Assert receipt committed */ }
  toNotBeCommitted() { /* Assert receipt not committed */ }
  expectHook(name, vetoed) { /* Assert hook status */ }
  toContainQuads(quads) { /* Assert store contains quads */ }
  toNotContainQuads(quads) { /* Assert store missing quads */ }
  toHaveSize(count) { /* Assert store size */ }
  toHaveProperty(prop, value) { /* Assert result property */ }
  toMatchSchema(schema) { /* Assert Zod schema match */ }
  toCompleteWithin(ms) { /* Assert duration bound */ }
}
\end{lstlisting}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Sequential test execution:
\[\PiMerge(\texttt{test}_1, \texttt{test}_2) = \texttt{test}_2(\texttt{context}_{\texttt{test}_1})\]

Tests execute sequentially. Later tests can depend on state from earlier tests (if same context).

Parallel test execution:
\[\texttt{test}_1 \oplusMerge \texttt{test}_2 \iff \neg\texttt{sharedState}(\texttt{test}_1, \texttt{test}_2)\]

Independent tests (disjoint contexts) can execute in parallel via Vitest's concurrency.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item \(\GuardH_{\text{timeout}}\): Abort tests exceeding 5-second default timeout
\item \(\GuardH_{\text{isolation}}\): Prevent test state leakage between scenarios
\item \(\GuardH_{\text{cleanup}}\): Ensure teardown executes even on assertion failure
\end{itemize}

Invariants:
\begin{itemize}
\item \(\InvQ_{\text{idempotent}}\): Tests produce same result on repeated execution
\item \(\InvQ_{\text{hermetic}}\): Tests have no external dependencies (filesystem, network)
\item \(\InvQ_{\text{deterministic}}\): No flaky tests (100\% pass rate required)
\item \(\InvQ_{\text{reversible}}\): Teardown fully restores pre-test state
\end{itemize}

\subsection*{Provenance and Receipts}

Test execution receipt:
\begin{lstlisting}
{
  "scenario": "Hook Execution",
  "timestamp": "2024-12-04T15:16:00.123Z",
  "success": true,
  "steps": [
    { "name": "Register hook", "success": true, "duration": 12 },
    { "name": "Execute hook", "success": true, "duration": 45 }
  ],
  "totalDuration": 67,
  "assertions": 5,
  "errors": []
}
\end{lstlisting}

Vitest integration for CI/CD:
\begin{lstlisting}
import { describe, it, expect } from 'vitest';
import { scenario, createTestContext } from '@unrdf/test-utils';

describe('Knowledge Hooks', () => {
  it('should execute hook on delta', async () => {
    const result = await scenario('Hook Execution')
      .setupScenario(() => createTestContext().build())
      .step('Execute hook', async (ctx) => {
        return await ctx.manager.execute(delta);
      })
      .execute();

    expect(result.success).toBe(true);
  });
});
\end{lstlisting}

\subsection*{Minimal Example}

\begin{lstlisting}
import { scenario, createTestContext, TestHelpers }
  from '@unrdf/test-utils';
import { KnowledgeHookManager } from '@unrdf/knowledge-engine';

const result = await scenario('Basic Hook Test')
  .setupScenario(() => {
    return createTestContext()
      .withManager(new KnowledgeHookManager())
      .build();
  })
  .step('Add quad', async (ctx) => {
    const quad = TestHelpers.createQuad(
      'http://example.org/Alice',
      'http://www.w3.org/1999/02/22-rdf-syntax-ns#type',
      'http://example.org/Person'
    );
    ctx.store.add(quad);
    return quad;
  }, [
    async (ctx, quad) => {
      // Assert quad was added
      const found = Array.from(ctx.store.match(quad.subject));
      if (found.length === 0) throw new Error('Quad not found');
    }
  ])
  .execute();

console.log(`Test ${result.success ? 'passed' : 'failed'}`);
console.log(`Duration: ${result.duration}ms`);
\end{lstlisting}

\subsection*{Open Questions}

\begin{enumerate}
\item How to generate test scenarios from SHACL shapes automatically?
\item Property-based testing for RDF store invariants (QuickCheck-style)?
\item Snapshot testing for SPARQL query results?
\item Integration with mutation testing frameworks (Stryker)?
\end{enumerate}

% ============================================================================

\label{pkg:unrdf-otel}
\section{\pkg{unrdf-observability} --- OpenTelemetry Integration}

\begin{pkgmeta}
Path & \texttt{packages/observability} \\
Kind & js \\
Entrypoints & 3 files \\
Dependencies & 5 (prom-client, @opentelemetry/api, @opentelemetry/exporter-prometheus, zod) \\
Blurb & Prometheus/Grafana observability dashboard with OpenTelemetry integration for distributed workflows \\
\end{pkgmeta}

\subsection*{Observable \(\Oobs\) and Artifact \(\Aout\)}

\begin{align*}
\Oobs &= \texttt{WorkflowExecution} \times \texttt{Metrics} \times \texttt{Traces} \\
\Aout &= \texttt{Dashboard} \times \texttt{Alerts} \times \texttt{MetricsExport}
\end{align*}

The observable consists of distributed workflow executions instrumented with OpenTelemetry spans and Prometheus metrics. The artifact is a Grafana-compatible dashboard, alert rules for anomaly detection, and Prometheus metrics endpoint for scraping.

Core components:
\begin{enumerate}
\item \textbf{WorkflowMetrics}: Prometheus metric collectors (counters, histograms, gauges)
\item \textbf{GrafanaExporter}: Dashboard JSON generator with pre-configured panels
\item \textbf{AlertManager}: Rule-based alerting for SLA violations
\item \textbf{OTEL Integration}: Trace and metric export to OpenTelemetry collector
\end{enumerate}

\subsection*{Type Signature \(\SigmaType\)}

Workflow metrics schema:
\begin{lstlisting}
const WorkflowMetricsSchema = z.object({
  workflowStarts: z.object({
    name: z.literal('workflow_starts_total'),
    type: z.literal('counter'),
    help: z.string(),
    labels: z.array(z.string())
  }),
  workflowDuration: z.object({
    name: z.literal('workflow_duration_seconds'),
    type: z.literal('histogram'),
    buckets: z.array(z.number())
  }),
  activeWorkflows: z.object({
    name: z.literal('workflow_active'),
    type: z.literal('gauge')
  }),
  errors: z.object({
    name: z.literal('workflow_errors_total'),
    type: z.literal('counter'),
    labels: z.array(z.string())
  })
});
\end{lstlisting}

Alert rule schema:
\begin{lstlisting}
const AlertRuleSchema = z.object({
  name: z.string(),
  metric: z.string(),
  condition: z.enum(['>', '<', '>=', '<=', '==']),
  threshold: z.number(),
  duration: z.number().optional(),
  severity: z.enum(['critical', 'warning', 'info']),
  annotations: z.record(z.string()).optional()
});
\end{lstlisting}

Grafana dashboard schema:
\begin{lstlisting}
const GrafanaDashboardSchema = z.object({
  title: z.string(),
  panels: z.array(z.object({
    id: z.number(),
    title: z.string(),
    type: z.enum(['graph', 'stat', 'gauge', 'table']),
    targets: z.array(z.object({
      expr: z.string(), // PromQL query
      legendFormat: z.string().optional()
    }))
  })),
  refresh: z.string().default('10s'),
  time: z.object({
    from: z.string().default('now-1h'),
    to: z.string().default('now')
  })
});
\end{lstlisting}

\subsection*{Reconciler \(\muRecon\)}

\[\muRecon_{\text{observe}}: \texttt{WorkflowExecution} \to \texttt{MetricsUpdate}\]

Observability pipeline:
\begin{enumerate}
\item \textbf{Instrumentation}: Wrap workflow steps with OTEL spans
\item \textbf{Metric Collection}: Record counters, histograms, gauges in Prometheus
\item \textbf{Trace Export}: Send spans to OTEL collector (Jaeger, Zipkin)
\item \textbf{Alert Evaluation}: Check metrics against alert rule thresholds
\item \textbf{Dashboard Update}: Stream metrics to Grafana via Prometheus exporter
\end{enumerate}

Metric recording:
\begin{lstlisting}
class WorkflowMetrics {
  recordWorkflowStart(workflowId, pattern) {
    this.workflowStarts.inc({ workflow_id: workflowId, pattern });
    this.activeWorkflows.inc();
  }

  recordWorkflowComplete(workflowId, status, duration, pattern) {
    this.workflowDuration.observe(
      { workflow_id: workflowId, status, pattern },
      duration / 1000 // Convert to seconds
    );
    this.activeWorkflows.dec();

    if (status === 'failed') {
      this.errors.inc({ workflow_id: workflowId, error_type: 'failure' });
    }
  }

  recordError(errorType, workflowId, severity) {
    this.errors.inc({
      error_type: errorType,
      workflow_id: workflowId,
      severity
    });
  }
}
\end{lstlisting}

Alert evaluation:
\begin{lstlisting}
class AlertManager {
  evaluateMetric(metricName, value, labels) {
    for (const rule of this.rules) {
      if (rule.metric !== metricName) continue;

      const triggered = this.checkCondition(
        value,
        rule.condition,
        rule.threshold
      );

      if (triggered) {
        this.fireAlert({
          ruleName: rule.name,
          severity: rule.severity,
          metric: metricName,
          value,
          labels,
          timestamp: new Date()
        });
      }
    }
  }
}
\end{lstlisting}

\subsection*{Composition \(\PiMerge / \oplusMerge\)}

Metric aggregation (commutative for counters):
\[\texttt{metric}_1 \oplusMerge \texttt{metric}_2 = \texttt{sum}(\texttt{metric}_1, \texttt{metric}_2)\]

Prometheus counters and histograms aggregate across instances.

Dashboard composition (sequential panels):
\[\PiMerge(\texttt{panel}_1, \texttt{panel}_2) = [\texttt{panel}_1, \texttt{panel}_2]\]

Grafana panels display sequentially in dashboard layout.

\subsection*{Guard \(\GuardH\) and Invariant \(\InvQ\)}

Guards:
\begin{itemize}
\item \(\GuardH_{\text{scrape}}\): Ensure Prometheus scrape interval \(\leq\) 30s
\item \(\GuardH_{\text{cardinality}}\): Prevent metric label explosion (max 1000 unique labels)
\item \(\GuardH_{\text{latency}}\): Alert if P95 latency \textgreater 5 seconds
\end{itemize}

Invariants:
\begin{itemize}
\item \(\InvQ_{\text{monotonic}}\): Counters only increase (never reset)
\item \(\InvQ_{\text{consistent}}\): Metric timestamps align with wall clock
\item \(\InvQ_{\text{complete}}\): All workflow executions produce metrics
\item \(\InvQ_{\text{bounded}}\): Histogram buckets cover expected latency range
\end{itemize}

\subsection*{Provenance and Receipts}

OTEL trace context propagation:
\begin{lstlisting}
{
  "traceId": "a3f8d1e2c4b5a6d7e8f9",
  "spanId": "b2c3d4e5f6a7",
  "parentSpanId": "a1b2c3d4e5f6",
  "name": "workflow.execute",
  "startTime": 1733314560123456789n,
  "endTime": 1733314560345678901n,
  "attributes": {
    "workflow.id": "wf-123",
    "workflow.pattern": "sequential",
    "workflow.status": "completed"
  },
  "events": [
    {
      "name": "step.complete",
      "timestamp": 1733314560234567890n,
      "attributes": { "step": "data-ingestion" }
    }
  ]
}
\end{lstlisting}

Prometheus metrics endpoint provides scrape target:
\begin{lstlisting}
# HELP workflow_starts_total Total workflow starts
# TYPE workflow_starts_total counter
workflow_starts_total{workflow_id="wf-123",pattern="sequential"} 42

# HELP workflow_duration_seconds Workflow execution duration
# TYPE workflow_duration_seconds histogram
workflow_duration_seconds_bucket{le="0.1"} 10
workflow_duration_seconds_bucket{le="0.5"} 35
workflow_duration_seconds_bucket{le="1.0"} 40
workflow_duration_seconds_sum 123.45
workflow_duration_seconds_count 42
\end{lstlisting}

\subsection*{Minimal Example}

\begin{lstlisting}
import { createObservabilityStack } from '@unrdf/observability';

const { metrics, grafana, alerts } = await createObservabilityStack({
  metrics: {
    prefix: 'unrdf_',
    port: 9090
  },
  grafana: {
    dashboardDir: './dashboards'
  },
  alerts: {
    rules: [
      {
        name: 'HighErrorRate',
        metric: 'error_count',
        condition: '>',
        threshold: 10,
        severity: 'critical'
      }
    ]
  }
});

// Instrument workflow
metrics.recordWorkflowStart('wf-123', 'sequential');

const startTime = Date.now();
// Execute workflow...
const duration = Date.now() - startTime;

metrics.recordWorkflowComplete('wf-123', 'completed', duration, 'sequential');

// Export Grafana dashboard
const dashboard = await grafana.generateDashboard();
await grafana.exportDashboard(dashboard, './dashboard.json');

// Start Prometheus exporter
metrics.startServer();
console.log('Metrics available at http://localhost:9090/metrics');
\end{lstlisting}

\subsection*{Open Questions}

\begin{enumerate}
\item How to aggregate metrics across distributed UNRDF nodes?
\item Can ML models detect anomalies in workflow metrics automatically?
\item Integration with Grafana Loki for log correlation?
\item OpenTelemetry baggage for cross-service trace context propagation?
\end{enumerate}

% ============================================================================
% End of agent_6_packages.tex
% ============================================================================
